{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "84cc7f7fba3047cebabfba705cd0c951"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-06-27T06:51:42.221215Z",
          "iopub.status.busy": "2024-06-27T06:51:42.220379Z",
          "iopub.status.idle": "2024-06-27T06:51:42.651991Z",
          "shell.execute_reply": "2024-06-27T06:51:42.650967Z",
          "shell.execute_reply.started": "2024-06-27T06:51:42.221164Z"
        },
        "id": "zlp-lx7_ncca",
        "outputId": "fa1254c5-919e-41f6-d990-d4ef78327598",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84cc7f7fba3047cebabfba705cd0c951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:51:54.671548Z",
          "iopub.status.busy": "2024-06-27T06:51:54.671150Z",
          "iopub.status.idle": "2024-06-27T06:52:21.785435Z",
          "shell.execute_reply": "2024-06-27T06:52:21.784166Z",
          "shell.execute_reply.started": "2024-06-27T06:51:54.671522Z"
        },
        "id": "6YTdFvqYnccd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:52:21.787640Z",
          "iopub.status.busy": "2024-06-27T06:52:21.787302Z",
          "iopub.status.idle": "2024-06-27T06:54:33.776057Z",
          "shell.execute_reply": "2024-06-27T06:54:33.774836Z",
          "shell.execute_reply.started": "2024-06-27T06:52:21.787606Z"
        },
        "id": "DzqpdHr3nccd",
        "outputId": "4126b780-eb7e-49cc-e5d4-97b81ba80adc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
            "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
            "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
            "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.9.0\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m925.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers -qq\n",
        "!pip install accelerate\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install --upgrade transformers -qq\n",
        "!pip install accelerate\n",
        "!pip install groq\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:33.777829Z",
          "iopub.status.busy": "2024-06-27T06:54:33.777495Z",
          "iopub.status.idle": "2024-06-27T06:54:38.196302Z",
          "shell.execute_reply": "2024-06-27T06:54:38.195349Z",
          "shell.execute_reply.started": "2024-06-27T06:54:33.777796Z"
        },
        "id": "KdGa9HUCncce",
        "outputId": "87ef3965-72bf-4dd9-d30e-30f4ef44019c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-27 06:54:34--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 166184960 (158M) [application/x-tar]\n",
            "Saving to: 'data.tar'\n",
            "\n",
            "data.tar            100%[===================>] 158.49M  54.0MB/s    in 2.9s    \n",
            "\n",
            "2024-06-27 06:54:38 (54.0 MB/s) - 'data.tar' saved [166184960/166184960]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:38.199234Z",
          "iopub.status.busy": "2024-06-27T06:54:38.198857Z",
          "iopub.status.idle": "2024-06-27T06:54:38.203762Z",
          "shell.execute_reply": "2024-06-27T06:54:38.202675Z",
          "shell.execute_reply.started": "2024-06-27T06:54:38.199200Z"
        },
        "id": "MyoIpj8nncce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import tarfile\n",
        "# unzip_path = '.'\n",
        "# tar = tarfile.open('data.tar')\n",
        "# tar.extractall(path=unzip_path)\n",
        "# tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:38.205159Z",
          "iopub.status.busy": "2024-06-27T06:54:38.204855Z",
          "iopub.status.idle": "2024-06-27T06:54:44.358381Z",
          "shell.execute_reply": "2024-06-27T06:54:44.357526Z",
          "shell.execute_reply.started": "2024-06-27T06:54:38.205135Z"
        },
        "id": "ZClR6l57ncce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_reasoning = pd.read_parquet(\"hf://datasets/livebench/reasoning/data/test-00000-of-00001.parquet\")\n",
        "df_math = pd.read_parquet(\"hf://datasets/livebench/math/data/test-00000-of-00001.parquet\")\n",
        "df_coding = pd.read_parquet(\"hf://datasets/livebench/coding/data/test-00000-of-00001.parquet\")\n",
        "df_language = pd.read_parquet(\"hf://datasets/livebench/language/data/test-00000-of-00001.parquet\")\n",
        "df_data_analysis = pd.read_parquet(\"hf://datasets/livebench/data_analysis/data/test-00000-of-00001.parquet\")\n",
        "# df_instruction = pd.read_parquet(\"hf://datasets/livebench/instruction_following/data/test-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:44.360571Z",
          "iopub.status.busy": "2024-06-27T06:54:44.359802Z",
          "iopub.status.idle": "2024-06-27T06:54:52.503617Z",
          "shell.execute_reply": "2024-06-27T06:54:52.502554Z",
          "shell.execute_reply.started": "2024-06-27T06:54:44.360541Z"
        },
        "id": "JZApsJbtncce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_reasoning.to_csv('reasoning_dev.csv', index=False)\n",
        "df_math.to_csv('math_dev.csv', index=False)\n",
        "df_language.to_csv('language_dev.csv', index=False)\n",
        "df_data_analysis.to_csv('data_analysis_dev.csv', index=False)\n",
        "df_coding.to_csv('coding_dev.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:52.505834Z",
          "iopub.status.busy": "2024-06-27T06:54:52.505071Z",
          "iopub.status.idle": "2024-06-27T06:54:52.511882Z",
          "shell.execute_reply": "2024-06-27T06:54:52.510964Z",
          "shell.execute_reply.started": "2024-06-27T06:54:52.505797Z"
        },
        "id": "oB9HA04pnccf",
        "outputId": "91b920e0-fcb9-499c-8604-e29dff264d1b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 5)\n",
            "(232, 9)\n",
            "(88, 13)\n",
            "(140, 10)\n",
            "(150, 5)\n"
          ]
        }
      ],
      "source": [
        "print(df_reasoning.shape)\n",
        "print(df_math.shape)\n",
        "print(df_coding.shape)\n",
        "print(df_language.shape)\n",
        "print(df_data_analysis.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:52.513273Z",
          "iopub.status.busy": "2024-06-27T06:54:52.513026Z",
          "iopub.status.idle": "2024-06-27T06:54:52.522053Z",
          "shell.execute_reply": "2024-06-27T06:54:52.521329Z",
          "shell.execute_reply.started": "2024-06-27T06:54:52.513251Z"
        },
        "id": "Od-fNTrOnccf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dev_folder_path = '/kaggle/working/dev'\n",
        "os.makedirs(dev_folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:52.523557Z",
          "iopub.status.busy": "2024-06-27T06:54:52.523228Z",
          "iopub.status.idle": "2024-06-27T06:54:52.534116Z",
          "shell.execute_reply": "2024-06-27T06:54:52.533210Z",
          "shell.execute_reply.started": "2024-06-27T06:54:52.523526Z"
        },
        "id": "KQvEdrptnccf",
        "outputId": "b2f1b512-a2dc-4fc0-b01d-f93b56d15810",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved 'data_analysis_dev.csv' to '/kaggle/working/dev'\n",
            "Moved 'language_dev.csv' to '/kaggle/working/dev'\n",
            "Moved 'math_dev.csv' to '/kaggle/working/dev'\n",
            "Moved 'reasoning_dev.csv' to '/kaggle/working/dev'\n",
            "Moved 'coding_dev.csv' to '/kaggle/working/dev'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = '/kaggle/working/'\n",
        "destination_folder = '/kaggle/working/dev'\n",
        "\n",
        "files = os.listdir(source_folder)\n",
        "csv_files = [file for file in files if file.endswith('.csv')]\n",
        "\n",
        "for file in csv_files:\n",
        "    source_file_path = os.path.join(source_folder, file)\n",
        "    destination_file_path = os.path.join(destination_folder, file)\n",
        "    shutil.move(source_file_path, destination_file_path)\n",
        "    print(f\"Moved '{file}' to '{destination_folder}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re0E7ltmnccg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:54:52.537671Z",
          "iopub.status.busy": "2024-06-27T06:54:52.537168Z",
          "iopub.status.idle": "2024-06-27T06:55:20.729138Z",
          "shell.execute_reply": "2024-06-27T06:55:20.728292Z",
          "shell.execute_reply.started": "2024-06-27T06:54:52.537646Z"
        },
        "id": "vABo6TFTnccg",
        "outputId": "6790e6c5-eb55-4358-c53c-05407d046738",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-27 06:55:03.053453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-27 06:55:03.053590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-27 06:55:03.312739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch.nn.functional as F\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import google.generativeai as genai\n",
        "from bert_score import score\n",
        "import os\n",
        "from groq import Groq\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import json\n",
        "import re\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from yaml import safe_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:55:20.730975Z",
          "iopub.status.busy": "2024-06-27T06:55:20.730256Z",
          "iopub.status.idle": "2024-06-27T06:55:21.083890Z",
          "shell.execute_reply": "2024-06-27T06:55:21.082971Z",
          "shell.execute_reply.started": "2024-06-27T06:55:20.730946Z"
        },
        "id": "vQu07wo7nccg",
        "outputId": "3518f2c5-642d-4312-987d-7a1618f7b763",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "\n",
        "os.environ['GOOGLE_API_KEY']=\"\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:55:21.086943Z",
          "iopub.status.busy": "2024-06-27T06:55:21.086660Z",
          "iopub.status.idle": "2024-06-27T06:55:21.094813Z",
          "shell.execute_reply": "2024-06-27T06:55:21.093965Z",
          "shell.execute_reply.started": "2024-06-27T06:55:21.086906Z"
        },
        "id": "v9hdTBF-nccg",
        "outputId": "9b3b4e80-4c87-4010-af89-91d3e608a586",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:55:21.118982Z",
          "iopub.status.busy": "2024-06-27T06:55:21.118702Z",
          "iopub.status.idle": "2024-06-27T06:55:21.128511Z",
          "shell.execute_reply": "2024-06-27T06:55:21.127710Z",
          "shell.execute_reply.started": "2024-06-27T06:55:21.118959Z"
        },
        "id": "er91c300ncch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:55:21.144576Z",
          "iopub.status.busy": "2024-06-27T06:55:21.143989Z",
          "iopub.status.idle": "2024-06-27T06:55:21.836267Z",
          "shell.execute_reply": "2024-06-27T06:55:21.835395Z",
          "shell.execute_reply.started": "2024-06-27T06:55:21.144544Z"
        },
        "id": "XM_fm79tncch",
        "outputId": "57793b92-91ae-4216-f671-9f1d7df95ec1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What a profound and existential question!\n",
            "\n",
            "**What is the meaning of life?**\n",
            "\n",
            "The meaning of life is a philosophical question that has been debated and explored by scholars, thinkers, and individuals across various cultures and centuries.asements.<|start_header_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "While there's no one definitive answer, here are some insights:\n",
            "\n",
            "1. **Purpose and Fulfillment**: Many believe that the meaning of life is to find one's purpose and fulfill it.<|start_header_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "2. **Happiness and Well-being**: Others argue that the meaning of life is to attain happiness and well-being, often through personal growth, relationships, and contributing to the greater good.\n",
            "3. **Self-Discovery and Exploration**: Some propose that the meaning of life is to continuously learn, grow, and explore one's own potential and the world around them\n"
          ]
        }
      ],
      "source": [
        "client = Groq(api_key=\"\")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n",
        "        }\n",
        "\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T06:56:17.888664Z",
          "iopub.status.busy": "2024-06-27T06:56:17.887878Z",
          "iopub.status.idle": "2024-06-27T06:56:17.892838Z",
          "shell.execute_reply": "2024-06-27T06:56:17.891800Z",
          "shell.execute_reply.started": "2024-06-27T06:56:17.888632Z"
        },
        "id": "DU9NPyVjncch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "gemini_model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1JyRXp1nccq"
      },
      "source": [
        "## Zero shot prompting with livebench and mmlu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gD7oEq6n3lA"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def encode(self, text, max_length=1024):\n",
        "        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return self.tokenizer.decode(tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T07:25:50.855678Z",
          "iopub.status.busy": "2024-06-27T07:25:50.855301Z",
          "iopub.status.idle": "2024-06-27T07:25:50.914178Z",
          "shell.execute_reply": "2024-06-27T07:25:50.913434Z",
          "shell.execute_reply.started": "2024-06-27T07:25:50.855650Z"
        },
        "id": "83I2SS1Tnccr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EvaluationPipeline:\n",
        "    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n",
        "        self.api_key = api_key\n",
        "        self.judge_model_name = judge_model_name\n",
        "        self.smaller_model_name = smaller_model_name\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(smaller_model_name)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        self.main_model = AutoModelForCausalLM.from_pretrained(\n",
        "            smaller_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            quantization_config=bnb_config,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        self.mmlu_categories = {\n",
        "            \"abstract_algebra\": [\"math\"],\n",
        "            \"anatomy\": [\"health\"],\n",
        "            \"astronomy\": [\"physics\"],\n",
        "            \"business_ethics\": [\"business\"],\n",
        "            \"clinical_knowledge\": [\"health\"],\n",
        "            \"college_biology\": [\"biology\"],\n",
        "            \"college_chemistry\": [\"chemistry\"],\n",
        "            \"college_computer_science\": [\"computer science\"],\n",
        "            \"college_mathematics\": [\"math\"],\n",
        "            \"college_medicine\": [\"health\"],\n",
        "            \"college_physics\": [\"physics\"],\n",
        "            \"computer_security\": [\"computer science\"],\n",
        "            \"conceptual_physics\": [\"physics\"],\n",
        "            \"econometrics\": [\"economics\"],\n",
        "            \"electrical_engineering\": [\"engineering\"],\n",
        "            \"elementary_mathematics\": [\"math\"],\n",
        "            \"formal_logic\": [\"philosophy\"],\n",
        "            \"global_facts\": [\"other\"],\n",
        "            \"high_school_biology\": [\"biology\"],\n",
        "            \"high_school_chemistry\": [\"chemistry\"],\n",
        "            \"high_school_computer_science\": [\"computer science\"],\n",
        "            \"high_school_european_history\": [\"history\"],\n",
        "            \"high_school_geography\": [\"geography\"],\n",
        "            \"high_school_government_and_politics\": [\"politics\"],\n",
        "            \"high_school_macroeconomics\": [\"economics\"],\n",
        "            \"high_school_mathematics\": [\"math\"],\n",
        "            \"high_school_microeconomics\": [\"economics\"],\n",
        "            \"high_school_physics\": [\"physics\"],\n",
        "            \"high_school_psychology\": [\"psychology\"],\n",
        "            \"high_school_statistics\": [\"math\"],\n",
        "            \"high_school_us_history\": [\"history\"],\n",
        "            \"high_school_world_history\": [\"history\"],\n",
        "            \"human_aging\": [\"health\"],\n",
        "            \"human_sexuality\": [\"culture\"],\n",
        "            \"international_law\": [\"law\"],\n",
        "            \"jurisprudence\": [\"law\"],\n",
        "            \"logical_fallacies\": [\"philosophy\"],\n",
        "            \"machine_learning\": [\"computer science\"],\n",
        "            \"management\": [\"business\"],\n",
        "            \"marketing\": [\"business\"],\n",
        "            \"medical_genetics\": [\"health\"],\n",
        "            \"miscellaneous\": [\"other\"],\n",
        "            \"moral_disputes\": [\"philosophy\"],\n",
        "            \"moral_scenarios\": [\"philosophy\"],\n",
        "            \"nutrition\": [\"health\"],\n",
        "            \"philosophy\": [\"philosophy\"],\n",
        "            \"prehistory\": [\"history\"],\n",
        "            \"professional_accounting\": [\"other\"],\n",
        "            \"professional_law\": [\"law\"],\n",
        "            \"professional_medicine\": [\"health\"],\n",
        "            \"professional_psychology\": [\"psychology\"],\n",
        "            \"public_relations\": [\"politics\"],\n",
        "            \"security_studies\": [\"politics\"],\n",
        "            \"sociology\": [\"culture\"],\n",
        "            \"us_foreign_policy\": [\"politics\"],\n",
        "            \"virology\": [\"health\"],\n",
        "            \"world_religions\": [\"philosophy\"],\n",
        "        }\n",
        "\n",
        "        self.livebench_categories = {\n",
        "            \"data_analysis\": [\"data_analysis\"],\n",
        "            \"coding\": [\"coding\"],\n",
        "            \"language\": [\"language\"],\n",
        "            \"reasoning\": [\"reasoning\"],\n",
        "            \"math\": [\"math\"],\n",
        "        }\n",
        "\n",
        "    def get_model_answer(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.main_model.device)\n",
        "        outputs = self.main_model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=300,\n",
        "            num_return_sequences=1,\n",
        "            temperature=self.temperature,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "        response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        logits = torch.stack(outputs.scores, dim=1)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        token_ids = outputs.sequences[:, inputs.shape[1]:]\n",
        "        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n",
        "\n",
        "        avg_confidence = confidences[0]\n",
        "        return response, avg_confidence\n",
        "\n",
        "    def judge_answer(self, response, prompt):\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"\n",
        "                        Review the user’s question and the corresponding response using the binary scoring system described below.\n",
        "                        - 0 points: The response is incorrect or does not address the user’s question.\n",
        "                        - 1 point: The response is correct and addresses the user’s question.\n",
        "\n",
        "                        User: {prompt}\n",
        "                        Response: {response}\n",
        "                        \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.judge_model_name,\n",
        "        )\n",
        "\n",
        "        judge_response = chat_completion.choices[0].message.content.strip()\n",
        "        return judge_response\n",
        "\n",
        "    def parse_evaluation(self, evaluation):\n",
        "        return 1 if \"1 point\" in evaluation else 0\n",
        "\n",
        "    def generate_reference_text(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.main_model.device)\n",
        "        outputs = self.main_model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=self.max_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=self.temperature,\n",
        "            do_sample=True\n",
        "        )\n",
        "        reference_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return reference_text\n",
        "\n",
        "\n",
        "    def calculate_bertscore(self, references, candidates):\n",
        "        references = list(references)\n",
        "        candidates = list(candidates)\n",
        "        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "        return P.mean().item(), R.mean().item(), F1.mean().item()\n",
        "\n",
        "    def evaluate_from_csv(self, csv_file_path, dataset_type, num_questions=None):\n",
        "        self.prompts = []\n",
        "        self.responses = []\n",
        "        self.references = []\n",
        "        self.confidences = []\n",
        "        self.accuracies = []\n",
        "\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        if dataset_type == \"mmlu\":\n",
        "            self.prompts = df.iloc[:, 0].tolist()\n",
        "            if num_questions:\n",
        "                self.prompts = self.prompts[:num_questions]\n",
        "\n",
        "            for prompt in self.prompts:\n",
        "                response, confidence = self.get_model_answer(prompt)\n",
        "                self.responses.append(response)\n",
        "                self.confidences.append(confidence)\n",
        "                reference_text = self.generate_reference_text(prompt)\n",
        "                self.references.append(reference_text)\n",
        "                judgement = self.judge_answer(response, prompt)\n",
        "                accuracy = self.parse_evaluation(judgement)\n",
        "                self.accuracies.append(accuracy)\n",
        "\n",
        "        elif dataset_type == \"livebench\":\n",
        "            if \"coding\" in csv_file_path:\n",
        "                self.prompts = df[\"turns\"].tolist()\n",
        "                self.references = df[\"solution\"].tolist()\n",
        "            else:\n",
        "                self.prompts = df[\"turns\"].tolist()\n",
        "                self.references = df[\"ground_truth\"].tolist()\n",
        "\n",
        "            # Filter out rows where \"turns\" has a value but \"ground_truth\"/\"solution\" is empty\n",
        "            self.prompts, self.references = zip(*[\n",
        "                (prompt, reference) for prompt, reference in zip(self.prompts, self.references)\n",
        "                if pd.notna(prompt) and pd.notna(reference)\n",
        "            ])\n",
        "\n",
        "            if num_questions:\n",
        "                self.prompts = self.prompts[:num_questions]\n",
        "                self.references = self.references[:num_questions]\n",
        "\n",
        "            for prompt, reference in zip(self.prompts, self.references):\n",
        "                response, confidence = self.get_model_answer(prompt)\n",
        "                self.responses.append(response)\n",
        "                self.confidences.append(confidence)\n",
        "                accuracy = 1 if response.strip() == reference.strip() else 0\n",
        "                self.accuracies.append(accuracy)\n",
        "\n",
        "        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n",
        "        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
        "\n",
        "        data, bins, bin_accuracies = self.calculate_ece()\n",
        "        data.to_csv(\"results.csv\", index=False)\n",
        "\n",
        "        # Save BERTScore along with ECE results\n",
        "        data['precision'] = precision\n",
        "        data['recall'] = recall\n",
        "        data['f1'] = f1\n",
        "        data.to_csv(\"results.csv\", index=False)\n",
        "\n",
        "        return data, bins, bin_accuracies\n",
        "\n",
        "    def evaluate_folder(self, folder_path, dataset_type, num_questions=None):\n",
        "        results = {}\n",
        "        reliability_data = {}\n",
        "        category_results = {category: [] for category in set(cat for sublist in self.mmlu_categories.values() for cat in sublist)}\n",
        "\n",
        "        if dataset_type == \"livebench\":\n",
        "            category_results = {category: [] for category in set(cat for sublist in self.livebench_categories.values() for cat in sublist)}\n",
        "\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".csv\"):\n",
        "                filepath = os.path.join(folder_path, filename)\n",
        "#                 class_name = filename.split('.')[0]\n",
        "                class_name = filename[:-8]\n",
        "                data, bins, bin_accuracies = self.evaluate_from_csv(filepath, dataset_type, num_questions)\n",
        "                results[class_name] = data\n",
        "\n",
        "\n",
        "                # Store class-wise data for reliability diagram\n",
        "                reliability_data[class_name] = {\n",
        "                    'bin_confidences': (bins[:-1] + bins[1:]) / 2,\n",
        "                    'bin_accuracies': bin_accuracies\n",
        "                }\n",
        "                if dataset_type == \"mmlu\":\n",
        "                    for category_name in self.mmlu_categories.get(class_name, []):\n",
        "                        # Calculate ECE within the category loop\n",
        "                        bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n",
        "                        valid_bins = bin_accuracies.dropna().index  # Bins with data\n",
        "                        bin_accuracies = bin_accuracies[valid_bins]\n",
        "                        bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n",
        "                        bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n",
        "\n",
        "                        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "                        category_results[category_name].append(ece)\n",
        "                        print(f\"{class_name} ECE: {ece}\")\n",
        "\n",
        "                elif dataset_type == \"livebench\":\n",
        "                    for category_name in self.livebench_categories.get(class_name, []):\n",
        "                        # Calculate ECE within the category loop\n",
        "                        bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n",
        "                        valid_bins = bin_accuracies.dropna().index  # Bins with data\n",
        "                        bin_accuracies = bin_accuracies[valid_bins]\n",
        "                        bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n",
        "                        bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n",
        "\n",
        "                        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "                        category_results[category_name].append(ece)\n",
        "                        print(f\"{class_name} ECE: {ece}\")\n",
        "\n",
        "\n",
        "                print(f\"Processed {class_name}\")\n",
        "\n",
        "\n",
        "        # Calculate average ECE for each category\n",
        "        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n",
        "        # Save average ECE results to 'category_results.csv'\n",
        "        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n",
        "        average_ece_df.to_csv(\"category_results.csv\", index=False)\n",
        "\n",
        "\n",
        "        # Save BERTScore results for all classes\n",
        "        bertscore_results = {\n",
        "            class_name: {\n",
        "                'precision': results[class_name]['precision'].iloc[0],\n",
        "                'recall': results[class_name]['recall'].iloc[0],\n",
        "                'f1': results[class_name]['f1'].iloc[0]\n",
        "            }\n",
        "            for class_name in results\n",
        "        }\n",
        "        bertscore_df = pd.DataFrame.from_dict(bertscore_results, orient='index')\n",
        "        bertscore_df.to_csv(\"bertscore_results.csv\", index=True)\n",
        "\n",
        "        # Plot reliability diagram for all classes in one figure\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
        "\n",
        "        # Find the maximum number of bins across all classes\n",
        "        max_bins = max([len(data['bin_accuracies']) for data in reliability_data.values()])\n",
        "\n",
        "        # Pad bin_accuracies with NaN to ensure equal lengths for plotting\n",
        "        for class_name, data in reliability_data.items():\n",
        "            num_missing_bins = max_bins - len(data['bin_accuracies'])\n",
        "            data['bin_accuracies'] = np.pad(data['bin_accuracies'], (0, num_missing_bins), 'constant', constant_values=np.nan)\n",
        "\n",
        "            plt.scatter(data['bin_confidences'][:max_bins], data['bin_accuracies'], label=class_name, s=50)\n",
        "\n",
        "        plt.xlabel('Confidence')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Reliability Diagram for All Classes')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def calculate_ece(self):\n",
        "        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n",
        "            data = pd.DataFrame({\n",
        "                'prompt': self.prompts,\n",
        "                'response': self.responses,\n",
        "                'confidence': self.confidences,\n",
        "                'rating': self.accuracies\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(\"All arrays must be of the same length\")\n",
        "\n",
        "        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n",
        "\n",
        "        bins = np.linspace(0, 1, 11)\n",
        "        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n",
        "\n",
        "        bin_accuracies = data.groupby('bin')['rating'].mean()\n",
        "        bin_proportions = data['bin'].value_counts(normalize=True)\n",
        "\n",
        "        valid_bins = bin_accuracies.dropna().index\n",
        "        bin_accuracies = bin_accuracies[valid_bins]\n",
        "        bin_proportions = bin_proportions[valid_bins]\n",
        "        bin_confidences = (bins[:-1] + bins[1:]) / 2\n",
        "        bin_confidences = bin_confidences[valid_bins]\n",
        "\n",
        "        bin_confidences = bin_confidences[:len(bin_accuracies)]\n",
        "\n",
        "        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "        print(f\"Expected Calibration Error (ECE): {ece}\")\n",
        "\n",
        "        return data, bins, bin_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T07:25:51.742521Z",
          "iopub.status.busy": "2024-06-27T07:25:51.742191Z",
          "iopub.status.idle": "2024-06-27T07:25:51.747652Z",
          "shell.execute_reply": "2024-06-27T07:25:51.746500Z",
          "shell.execute_reply.started": "2024-06-27T07:25:51.742496Z"
        },
        "id": "AN7bwpKhnccr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## All Models\n",
        "models = [\n",
        "    {'name': 'Qwen/Qwen2-7B-Instruct'},\n",
        "    {'name': 'stabilityai/StableBeluga-7B'},\n",
        "    {'name': 'meta-llama/Meta-Llama-3-8B'},\n",
        "    {'name': 'teknium/OpenHermes-2.5-Mistral-7B'},\n",
        "    {'name': 'mistralai/Mistral-7B-Instruct-v0.2'},\n",
        "    {'name': 'HuggingFaceH4/zephyr-7b-beta'},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-27T07:32:41.296203Z",
          "iopub.status.busy": "2024-06-27T07:32:41.295814Z",
          "iopub.status.idle": "2024-06-27T07:32:41.601270Z",
          "shell.execute_reply": "2024-06-27T07:32:41.599548Z",
          "shell.execute_reply.started": "2024-06-27T07:32:41.296172Z"
        },
        "id": "SYRvMD6Xnccr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=models[4]['name'])\n",
        "# pipeline.evaluate_folder(\"/kaggle/working/dev\", dataset_type=\"livebench\", num_questions=3) # if u control question\n",
        "pipeline.evaluate_folder(\"/kaggle/working/dev\", dataset_type=\"livebench\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET4Nmyo-nccs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain of thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def encode(self, text, max_length=1024):\n",
        "        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return self.tokenizer.decode(tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvaluationPipeline:\n",
        "    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n",
        "        self.api_key = api_key\n",
        "        self.judge_model_name = judge_model_name\n",
        "        self.smaller_model_name = smaller_model_name\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(smaller_model_name)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        self.main_model = AutoModelForCausalLM.from_pretrained(\n",
        "            smaller_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            quantization_config=bnb_config,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        self.mmlu_categories = {\n",
        "            \"abstract_algebra\": [\"math\"],\n",
        "            \"anatomy\": [\"health\"],\n",
        "            \"astronomy\": [\"physics\"],\n",
        "            \"business_ethics\": [\"business\"],\n",
        "            \"clinical_knowledge\": [\"health\"],\n",
        "            \"college_biology\": [\"biology\"],\n",
        "            \"college_chemistry\": [\"chemistry\"],\n",
        "            \"college_computer_science\": [\"computer science\"],\n",
        "            \"college_mathematics\": [\"math\"],\n",
        "            \"college_medicine\": [\"health\"],\n",
        "            \"college_physics\": [\"physics\"],\n",
        "            \"computer_security\": [\"computer science\"],\n",
        "            \"conceptual_physics\": [\"physics\"],\n",
        "            \"econometrics\": [\"economics\"],\n",
        "            \"electrical_engineering\": [\"engineering\"],\n",
        "            \"elementary_mathematics\": [\"math\"],\n",
        "            \"formal_logic\": [\"philosophy\"],\n",
        "            \"global_facts\": [\"other\"],\n",
        "            \"high_school_biology\": [\"biology\"],\n",
        "            \"high_school_chemistry\": [\"chemistry\"],\n",
        "            \"high_school_computer_science\": [\"computer science\"],\n",
        "            \"high_school_european_history\": [\"history\"],\n",
        "            \"high_school_geography\": [\"geography\"],\n",
        "            \"high_school_government_and_politics\": [\"politics\"],\n",
        "            \"high_school_macroeconomics\": [\"economics\"],\n",
        "            \"high_school_mathematics\": [\"math\"],\n",
        "            \"high_school_microeconomics\": [\"economics\"],\n",
        "            \"high_school_physics\": [\"physics\"],\n",
        "            \"high_school_psychology\": [\"psychology\"],\n",
        "            \"high_school_statistics\": [\"math\"],\n",
        "            \"high_school_us_history\": [\"history\"],\n",
        "            \"high_school_world_history\": [\"history\"],\n",
        "            \"human_aging\": [\"health\"],\n",
        "            \"human_sexuality\": [\"culture\"],\n",
        "            \"international_law\": [\"law\"],\n",
        "            \"jurisprudence\": [\"law\"],\n",
        "            \"logical_fallacies\": [\"philosophy\"],\n",
        "            \"machine_learning\": [\"computer science\"],\n",
        "            \"management\": [\"business\"],\n",
        "            \"marketing\": [\"business\"],\n",
        "            \"medical_genetics\": [\"health\"],\n",
        "            \"miscellaneous\": [\"other\"],\n",
        "            \"moral_disputes\": [\"philosophy\"],\n",
        "            \"moral_scenarios\": [\"philosophy\"],\n",
        "            \"nutrition\": [\"health\"],\n",
        "            \"philosophy\": [\"philosophy\"],\n",
        "            \"prehistory\": [\"history\"],\n",
        "            \"professional_accounting\": [\"other\"],\n",
        "            \"professional_law\": [\"law\"],\n",
        "            \"professional_medicine\": [\"health\"],\n",
        "            \"professional_psychology\": [\"psychology\"],\n",
        "            \"public_relations\": [\"politics\"],\n",
        "            \"security_studies\": [\"politics\"],\n",
        "            \"sociology\": [\"culture\"],\n",
        "            \"us_foreign_policy\": [\"politics\"],\n",
        "            \"virology\": [\"health\"],\n",
        "            \"world_religions\": [\"philosophy\"],\n",
        "        }\n",
        "\n",
        "        self.livebench_categories = {\n",
        "            \"data_analysis\": [\"data_analysis\"],\n",
        "            \"coding\": [\"coding\"],\n",
        "            \"language\": [\"language\"],\n",
        "            \"reasoning\": [\"reasoning\"],\n",
        "            \"math\": [\"math\"],\n",
        "        }\n",
        "\n",
        "    def generate_cot_prompt(self, question):\n",
        "        # Chain of Thought prompt\n",
        "        cot_prompt = f\"Q: {question}\\nA: Let's think through this step by step.\\n\"\n",
        "        cot_prompt += \"First, consider the main aspects of the question. \"\n",
        "        cot_prompt += \"Next, break down the problem into smaller parts. \"\n",
        "        cot_prompt += \"Finally, synthesize the information to form a coherent answer.\"\n",
        "        return cot_prompt\n",
        "\n",
        "    def get_model_answer(self, question):\n",
        "        \"\"\"Generates an answer from the specified model using Chain of Thought prompting.\"\"\"\n",
        "        prompt = self.generate_cot_prompt(question)\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.main_model.device)\n",
        "        outputs = self.main_model.generate(\n",
        "            inputs['input_ids'], \n",
        "            max_new_tokens=300,  # Limit the number of generated tokens\n",
        "            num_return_sequences=1, \n",
        "            temperature=self.temperature, \n",
        "            output_scores=True, \n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "        response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "        # Get the confidence score\n",
        "        logits = torch.stack(outputs.scores, dim=1)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n",
        "        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n",
        "\n",
        "        avg_confidence = confidences[0]\n",
        "        return response, avg_confidence\n",
        "\n",
        "    def judge_answer(self, response, prompt):\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"\n",
        "                        Review the user’s question and the corresponding response using the binary scoring system described below.\n",
        "                        - 0 points: The response is incorrect or does not address the user’s question.\n",
        "                        - 1 point: The response is correct and addresses the user’s question.\n",
        "\n",
        "                        User: {prompt}\n",
        "                        Response: {response}\n",
        "                        \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.judge_model_name,\n",
        "        )\n",
        "\n",
        "        judge_response = chat_completion.choices[0].message.content.strip()\n",
        "        return judge_response\n",
        "\n",
        "    def parse_evaluation(self, evaluation):\n",
        "        return 1 if \"1 point\" in evaluation else 0\n",
        "\n",
        "    def generate_reference_text(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.main_model.device)\n",
        "        outputs = self.main_model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=self.max_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=self.temperature,\n",
        "            do_sample=True\n",
        "        )\n",
        "        reference_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return reference_text\n",
        "\n",
        "\n",
        "    def calculate_bertscore(self, references, candidates):\n",
        "        references = list(references)\n",
        "        candidates = list(candidates)\n",
        "        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "        return P.mean().item(), R.mean().item(), F1.mean().item()\n",
        "\n",
        "    def evaluate_from_csv(self, csv_file_path, dataset_type, num_questions=None):\n",
        "        self.prompts = []\n",
        "        self.responses = []\n",
        "        self.references = []\n",
        "        self.confidences = []\n",
        "        self.accuracies = []\n",
        "\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        if dataset_type == \"mmlu\":\n",
        "            self.prompts = df.iloc[:, 0].tolist()\n",
        "            if num_questions:\n",
        "                self.prompts = self.prompts[:num_questions]\n",
        "                \n",
        "            for prompt in self.prompts:\n",
        "                response, confidence = self.get_model_answer(prompt)\n",
        "                self.responses.append(response)\n",
        "                self.confidences.append(confidence)\n",
        "                reference_text = self.generate_reference_text(prompt)\n",
        "                self.references.append(reference_text)\n",
        "                judgement = self.judge_answer(response, prompt)\n",
        "                accuracy = self.parse_evaluation(judgement)\n",
        "                self.accuracies.append(accuracy)\n",
        "                \n",
        "        elif dataset_type == \"livebench\":\n",
        "            if \"coding\" in csv_file_path:\n",
        "                self.prompts = df[\"turns\"].tolist()\n",
        "                self.references = df[\"solution\"].tolist()\n",
        "            else:\n",
        "                self.prompts = df[\"turns\"].tolist()\n",
        "                self.references = df[\"ground_truth\"].tolist()\n",
        "            \n",
        "            # Filter out rows where \"turns\" has a value but \"ground_truth\"/\"solution\" is empty\n",
        "            self.prompts, self.references = zip(*[\n",
        "                (prompt, reference) for prompt, reference in zip(self.prompts, self.references) \n",
        "                if pd.notna(prompt) and pd.notna(reference)\n",
        "            ])\n",
        "\n",
        "            if num_questions:\n",
        "                self.prompts = self.prompts[:num_questions]\n",
        "                self.references = self.references[:num_questions]\n",
        "\n",
        "            for prompt, reference in zip(self.prompts, self.references):\n",
        "                response, confidence = self.get_model_answer(prompt)\n",
        "                self.responses.append(response)\n",
        "                self.confidences.append(confidence)\n",
        "                accuracy = 1 if response.strip() == reference.strip() else 0\n",
        "                self.accuracies.append(accuracy)\n",
        "\n",
        "        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n",
        "        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
        "\n",
        "        data, bins, bin_accuracies = self.calculate_ece()\n",
        "        data.to_csv(\"results.csv\", index=False)\n",
        "        \n",
        "        # Save BERTScore along with ECE results\n",
        "        data['precision'] = precision\n",
        "        data['recall'] = recall\n",
        "        data['f1'] = f1\n",
        "        data.to_csv(\"results.csv\", index=False)\n",
        "        \n",
        "        return data, bins, bin_accuracies\n",
        "\n",
        "    def evaluate_folder(self, folder_path, dataset_type, num_questions=None):\n",
        "        results = {}\n",
        "        reliability_data = {}\n",
        "        category_results = {category: [] for category in set(cat for sublist in self.mmlu_categories.values() for cat in sublist)}\n",
        "\n",
        "        if dataset_type == \"livebench\":\n",
        "            category_results = {category: [] for category in set(cat for sublist in self.livebench_categories.values() for cat in sublist)}\n",
        "\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".csv\"):\n",
        "                filepath = os.path.join(folder_path, filename)\n",
        "#                 class_name = filename.split('.')[0]\n",
        "                class_name = filename[:-8] \n",
        "                data, bins, bin_accuracies = self.evaluate_from_csv(filepath, dataset_type, num_questions)\n",
        "                results[class_name] = data\n",
        "        \n",
        "                \n",
        "                # Store class-wise data for reliability diagram\n",
        "                reliability_data[class_name] = {\n",
        "                    'bin_confidences': (bins[:-1] + bins[1:]) / 2,\n",
        "                    'bin_accuracies': bin_accuracies\n",
        "                }\n",
        "                \n",
        "                if dataset_type == \"mmlu\":\n",
        "                    for category_name in self.mmlu_categories.get(class_name, []):\n",
        "                        print(f\"Processed {class_name}\")\n",
        "                        # Calculate ECE within the category loop\n",
        "                        bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n",
        "                        valid_bins = bin_accuracies.dropna().index  # Bins with data\n",
        "                        bin_accuracies = bin_accuracies[valid_bins]\n",
        "                        bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n",
        "                        bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n",
        "\n",
        "                        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "                        category_results[category_name].append(ece)\n",
        "                        print(f\"{class_name} ECE: {ece}\")\n",
        "\n",
        "                elif dataset_type == \"livebench\":\n",
        "                    for category_name in self.livebench_categories.get(class_name, []):\n",
        "                        print(f\"Processed {class_name}\")\n",
        "                        # Calculate ECE within the category loop\n",
        "                        bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n",
        "                        valid_bins = bin_accuracies.dropna().index  # Bins with data\n",
        "                        bin_accuracies = bin_accuracies[valid_bins]\n",
        "                        bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n",
        "                        bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n",
        "\n",
        "                        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "                        category_results[category_name].append(ece)\n",
        "                        print(f\"{class_name} ECE: {ece}\")\n",
        "                        \n",
        "\n",
        "        # Calculate average ECE for each category\n",
        "        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n",
        "\n",
        "        # Save average ECE results to 'category_results.csv'\n",
        "        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n",
        "        average_ece_df.to_csv(\"category_results.csv\", index=False)\n",
        "\n",
        "        \n",
        "        # Save BERTScore results for all classes\n",
        "        bertscore_results = {\n",
        "            class_name: {\n",
        "                'precision': results[class_name]['precision'].iloc[0],\n",
        "                'recall': results[class_name]['recall'].iloc[0],\n",
        "                'f1': results[class_name]['f1'].iloc[0]\n",
        "            }\n",
        "            for class_name in results\n",
        "        }\n",
        "        bertscore_df = pd.DataFrame.from_dict(bertscore_results, orient='index')\n",
        "        bertscore_df.to_csv(\"bertscore_results.csv\", index=True)\n",
        "        \n",
        "        # Plot reliability diagram for all classes in one figure\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
        "\n",
        "        # Find the maximum number of bins across all classes\n",
        "        max_bins = max([len(data['bin_accuracies']) for data in reliability_data.values()])\n",
        "\n",
        "        # Pad bin_accuracies with NaN to ensure equal lengths for plotting\n",
        "        for class_name, data in reliability_data.items():\n",
        "            num_missing_bins = max_bins - len(data['bin_accuracies'])\n",
        "            data['bin_accuracies'] = np.pad(data['bin_accuracies'], (0, num_missing_bins), 'constant', constant_values=np.nan)\n",
        "\n",
        "            plt.scatter(data['bin_confidences'][:max_bins], data['bin_accuracies'], label=class_name, s=50)\n",
        "\n",
        "        plt.xlabel('Confidence')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Reliability Diagram for All Classes')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def calculate_ece(self):\n",
        "        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n",
        "            data = pd.DataFrame({\n",
        "                'prompt': self.prompts,\n",
        "                'response': self.responses,\n",
        "                'confidence': self.confidences,\n",
        "                'rating': self.accuracies\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(\"All arrays must be of the same length\")\n",
        "\n",
        "        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n",
        "\n",
        "        bins = np.linspace(0, 1, 11)\n",
        "        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n",
        "\n",
        "        bin_accuracies = data.groupby('bin')['rating'].mean()\n",
        "        bin_proportions = data['bin'].value_counts(normalize=True)\n",
        "\n",
        "        valid_bins = bin_accuracies.dropna().index\n",
        "        bin_accuracies = bin_accuracies[valid_bins]\n",
        "        bin_proportions = bin_proportions[valid_bins]\n",
        "        bin_confidences = (bins[:-1] + bins[1:]) / 2\n",
        "        bin_confidences = bin_confidences[valid_bins]\n",
        "\n",
        "        bin_confidences = bin_confidences[:len(bin_accuracies)]\n",
        "\n",
        "        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n",
        "        print(f\"Expected Calibration Error (ECE): {ece}\")\n",
        "\n",
        "        return data, bins, bin_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## All Models\n",
        "models = [\n",
        "    {'name': 'Qwen/Qwen2-7B-Instruct'},\n",
        "    {'name': 'stabilityai/StableBeluga-7B'},\n",
        "    {'name': 'meta-llama/Meta-Llama-3-8B'}, \n",
        "    {'name': 'teknium/OpenHermes-2.5-Mistral-7B'}, \n",
        "    {'name': 'mistralai/Mistral-7B-Instruct-v0.2'},  \n",
        "    {'name': 'HuggingFaceH4/zephyr-7b-beta'},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example usage\n",
        "pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=models[4]['name'])\n",
        "# pipeline.evaluate_folder(\"/kaggle/working/dev\", dataset_type=\"livebench\", num_questions=1) # if u control question\n",
        "pipeline.evaluate_folder(\"/kaggle/working/dev\", dataset_type=\"livebench\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
