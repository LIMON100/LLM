{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T08:58:51.099216Z","iopub.execute_input":"2024-05-28T08:58:51.099495Z","iopub.status.idle":"2024-05-28T08:59:28.911081Z","shell.execute_reply.started":"2024-05-28T08:58:51.099469Z","shell.execute_reply":"2024-05-28T08:59:28.909992Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:59:28.912837Z","iopub.execute_input":"2024-05-28T08:59:28.913150Z","iopub.status.idle":"2024-05-28T08:59:29.218738Z","shell.execute_reply.started":"2024-05-28T08:59:28.913123Z","shell.execute_reply":"2024-05-28T08:59:29.217920Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8876b06d0a34e12816dd8bfa6b4a22b"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade transformers -qq\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-28T08:59:59.545824Z","iopub.execute_input":"2024-05-28T08:59:59.546693Z","iopub.status.idle":"2024-05-28T09:00:24.263972Z","shell.execute_reply.started":"2024-05-28T08:59:59.546660Z","shell.execute_reply":"2024-05-28T09:00:24.262851Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.isotonic import IsotonicRegression\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\nfrom transformers import BitsAndBytesConfig\nimport torch.nn.functional as F\nfrom transformers import pipeline\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:00:24.266107Z","iopub.execute_input":"2024-05-28T09:00:24.266380Z","iopub.status.idle":"2024-05-28T09:00:53.734134Z","shell.execute_reply.started":"2024-05-28T09:00:24.266355Z","shell.execute_reply":"2024-05-28T09:00:53.733032Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-28 09:00:36.440829: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-28 09:00:36.440944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-28 09:00:36.718808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmistral_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:00:53.735361Z","iopub.execute_input":"2024-05-28T09:00:53.736063Z","iopub.status.idle":"2024-05-28T09:03:11.473697Z","shell.execute_reply.started":"2024-05-28T09:00:53.736026Z","shell.execute_reply":"2024-05-28T09:03:11.472703Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03df5f10547480282f5f8063f3f23c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be4302bcdb5248309f436d6be3c4fd10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40418a3d1ad4f8199ae3753de65a1b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc58b1f213d94704bb982a4e441ac1b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ca98bbe4e24fa9a57927ca2918ad6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455fd26176bf462db8e3c397d2f410e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30700e3b772a46609b6a6a83898a3a6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e1fc5f148740cb8a40acfc0883fd5e"}},"metadata":{}}]},{"cell_type":"code","source":"model_name2 = 'stabilityai/StableBeluga-13B'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nbeluga_model = AutoModelForCausalLM.from_pretrained(\n    model_name2,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:39:34.994600Z","iopub.execute_input":"2024-05-28T09:39:34.994970Z","iopub.status.idle":"2024-05-28T09:45:44.476981Z","shell.execute_reply.started":"2024-05-28T09:39:34.994937Z","shell.execute_reply":"2024-05-28T09:45:44.475207Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e491a96622a7482e9599a74d01dd56ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b934031c5f0543aa8dbf5b782d203c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9005d87fdff48f8b66eb64b561d0bb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053257fbec63446dacf22bc092049c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae565007670491fb2e30fec691b25aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c3f1b0fb384b488f480903f0483c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d4e74c9db243528539c74bd9de5845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5439b0180e534e649405c78317e9539b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Generate Responses and Confidence Scores:\n\nFor each prompt, generate a response using Mistral7B and calculate its confidence score.\nUse StableBeluga13B to evaluate the response and calculate its confidence score for the evaluation.\n\n## Combine Confidence Scores:\n\nAverage the confidence scores from both models to form a single confidence score for each response.\n\n## Calculate ECE Before Calibration:\n\nUse the combined confidence scores and binary judgments to calculate the ECE before any calibration.\n\n## Perform Calibration:\n\nApply isotonic regression to fit the combined confidence scores to the binary judgments, which adjusts the confidence scores.\n\n## Calculate ECE After Calibration:\n\nUse the calibrated confidence scores and binary judgments to calculate the ECE after calibration.","metadata":{}},{"cell_type":"code","source":"def calculate_ece(confidence_scores, judgments, num_bins=10):\n    \"\"\"Calculates ECE given confidence scores and binary judgments.\"\"\"\n    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n    bin_indices = np.digitize(confidence_scores, bin_boundaries) - 1\n    ece = 0.0\n    for bin_idx in range(num_bins):\n        bin_mask = bin_indices == bin_idx\n        if np.any(bin_mask):\n            bin_confidence = np.mean(confidence_scores[bin_mask])\n            bin_accuracy = np.mean(judgments[bin_mask])\n            ece += np.abs(bin_confidence - bin_accuracy) * np.sum(bin_mask) / len(confidence_scores)\n    return ece","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:30:07.572051Z","iopub.execute_input":"2024-05-28T06:30:07.572734Z","iopub.status.idle":"2024-05-28T06:30:07.579591Z","shell.execute_reply.started":"2024-05-28T06:30:07.572701Z","shell.execute_reply":"2024-05-28T06:30:07.578702Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    # Get the confidence score \n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n    \n    token_ids = outputs.sequences[:, inputs.shape[1]:] \n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n    \n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_stablebeluga_confidence(response, prompt):\n    \"\"\"Generates confidence score from the StableBeluga_13B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n#     evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    inputs = tokenizer.encode(response, return_tensors='pt', max_length=1024, truncation=True).to(beluga_model.device)\n    outputs = beluga_model.generate(inputs, max_new_tokens=150, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    stablebeluga_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    # Get the confidence score \n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n    \n    token_ids = outputs.sequences[:, inputs.shape[1]:]  # Exclude the prompt tokens\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n    \n    avg_confidence = confidences[0]\n    return stablebeluga_response, avg_confidence","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:30:09.575094Z","iopub.execute_input":"2024-05-28T06:30:09.575466Z","iopub.status.idle":"2024-05-28T06:30:09.587902Z","shell.execute_reply.started":"2024-05-28T06:30:09.575438Z","shell.execute_reply":"2024-05-28T06:30:09.586443Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"prompts = [\n    \"What is nuclear fusion?\",\n    \"What are the advantages of it?\",\n    \"What danger it make on environment?\"\n]\nmistral_answers = []\nmistral_confidence_scores = []\nstablebeluga_confidence_scores = []\njudgments = []\n\nfor prompt in prompts:\n    mistral_answer, mistral_confidence = get_mistral_answer(prompt)\n    mistral_answers.append(mistral_answer)\n    mistral_confidence_scores.append(mistral_confidence)\n\n    stablebeluga_evaluation, stablebeluga_confidence = get_stablebeluga_confidence(mistral_answer, prompt)\n    stablebeluga_confidence_scores.append(stablebeluga_confidence)\n    \n    judgment = 1 if \"Score: 5\" in stablebeluga_evaluation else 0  # Convert judgment to binary\n    judgments.append(judgment)\n\n# Combine confidence scores from both models\ncombined_confidence_scores = (np.array(mistral_confidence_scores) + np.array(stablebeluga_confidence_scores)) / 2\n\n# Calculate ECE (before calibration):\nece_before = calculate_ece(combined_confidence_scores, np.array(judgments))\nprint(f\"ECE Before Calibration: {ece_before:.4f}\")\n\n# # Isotonic Regression for Calibration:\n# calibrator = IsotonicRegression(out_of_bounds='clip')\n# calibrator.fit(combined_confidence_scores, judgments)\n\n# calibrated_confidence_scores = calibrator.transform(combined_confidence_scores)\n\n# ece_after = calculate_ece(calibrated_confidence_scores, np.array(judgments))\n# print(f\"ECE After Calibration: {ece_after:.4f}\")\n\n# Display the results\nfor i, prompt in enumerate(prompts):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Answer: {mistral_answers[i]}\")\n    print(f\"Mistral Confidence: {mistral_confidence_scores[i]:.4f}\")\n    print(f\"StableBeluga Confidence: {stablebeluga_confidence_scores[i]:.4f}\")\n    print(f\"Combined Confidence: {combined_confidence_scores[i]:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:26:03.038359Z","iopub.execute_input":"2024-05-24T09:26:03.039182Z","iopub.status.idle":"2024-05-24T09:27:09.387104Z","shell.execute_reply.started":"2024-05-24T09:26:03.039147Z","shell.execute_reply":"2024-05-24T09:27:09.386139Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"178a1c483f86434fa1fb701e401954a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c2f2d681614bf9814755d4dc5edec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b988f76c762f465da0bc9ab0a21ed1ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd48ce99c31c4fb3bb51f11436e17544"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fd69157d7249259464b12ad53167ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927567989e2145b3a8775cf48f37d0bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db9c53aca84d47548f84333b74f0c302"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f3d68644eb4760b6fce7bf1953b31c"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ECE Before Calibration: 0.8592\nPrompt: What is nuclear fusion?\nAnswer: What is nuclear fusion?\n\nNuclear fusion is a process in which two or more atomic nuclei combine to form a heavier nucleus, releasing energy in the process. This energy is released in the form of light and heat. The sun and other stars generate their energy through nuclear fusion.\n\nIn a nuclear fusion reaction, the nuclei must overcome the electrostatic force that keeps them apart due to their positive charges. This requires extremely high temperatures and pressures to bring the nuclei close enough together for the fusion reaction to occur.\n\nThe most common type of nuclear fusion reaction is the fusion of hydrogen isotopes, specifically deuterium and tritium, to form helium. This reaction releases a large amount of energy and is the reaction that powers the sun and other stars.\n\nNuclear fusion has the potential to be a clean and virtually limitless source of energy, as the fuel (hydrogen isotopes) is abundant and the only byproduct is water (if the reaction is controlled properly). However, achieving and sustaining a nuclear fusion reaction on Earth has proven to be a significant challenge.\n\nResearch into nuclear fusion as a source of energy is ongoing, with several approaches being pursued, including magnetic confinement fusion and inertial confinement fusion. The goal is to develop a practical and economically viable fusion reactor that can produce a net energy output.\nMistral Confidence: 0.8634\nStableBeluga Confidence: 0.8567\nCombined Confidence: 0.8601\nPrompt: What are the advantages of it?\nAnswer: What are the advantages of it?\n\n1. It is a natural and renewable resource.\n2. It is biodegradable and does not contribute to landfill waste.\n3. It is hypoallergenic and does not cause skin irritation or allergic reactions.\n4. It is soft and comfortable to wear.\n5. It is easy to care for and does not require special cleaning instructions.\n6. It has good insulation properties, keeping the wearer warm in cold weather and cool in hot weather.\n7. It is moisture-wicking, which helps to keep the wearer dry and comfortable.\n8. It is breathable, allowing air to circulate and preventing the build-up of sweat and odor.\n9. It is lightweight and flexible, making it ideal for a wide range of activities and clothing styles.\n10. It is durable and can withstand repeated washing and wear without losing its shape or quality.\n\nOverall, bamboo fabric offers a sustainable, comfortable, and practical alternative to synthetic fabrics, making it an excellent choice for eco-conscious consumers and those seeking high-performance clothing.\nMistral Confidence: 0.8379\nStableBeluga Confidence: 0.8841\nCombined Confidence: 0.8610\nPrompt: What danger it make on environment?\nAnswer: What danger it make on environment?\n\nThe production and use of plastics have significant environmental impacts. Here are some of the main concerns:\n\n1. Plastic waste in the environment: Plastic waste is a major problem in landfills and in the natural environment, particularly in the ocean. Plastic waste can take hundreds of years to decompose, and in the meantime, it can harm wildlife and ecosystems.\n2. Microplastics: Plastics can break down into smaller pieces, known as microplastics, which can be ingested by organisms and accumulate in the food chain. Microplastics have been found in a wide range of organisms, including fish, shellfish, and even in human blood.\n3. Greenhouse gas emissions: The production and disposal of plastics contribute to greenhouse gas emissions, which contribute to climate change. Plastics are made from fossil fuels, and the energy-intensive process of producing them results in significant carbon emissions.\n4. Water pollution: Plastic waste can contaminate water sources, making them unsafe for drinking and other uses. Plastic waste can also harm aquatic organisms and disrupt ecosystems.\n5. Land degradation: The production and disposal of plastics can lead to land degradation, particularly in the form of landfills. Landfills can release methane and other greenhouse\nMistral Confidence: 0.8494\nStableBeluga Confidence: 0.8639\nCombined Confidence: 0.8567\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Update prompt","metadata":{}},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    # Get the confidence score \n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n    \n    token_ids = outputs.sequences[:, inputs.shape[1]:] \n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n    \n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_stablebeluga_confidence(response, prompt):\n    \"\"\"Generates confidence score from the StableBeluga_13B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n\n    evaluation_prompt = f\"\"\"\n        Review the user’s question and the corresponding response using the additive 5-point\n        scoring system described below. Points are accumulated based on the satisfaction of each\n        criterion:\n        - Add 1 point if the response is relevant and provides some information related to\n        the user’s inquiry, even if it is incomplete or contains some irrelevant content.\n        - Add another point if the response addresses a substantial portion of the user’s question,\n        but does not completely resolve the query or provide a direct answer.\n        - Award a third point if the response answers the basic elements of the user’s question in a\n        useful way, regardless of whether it seems to have been written by an AI Assistant or if it\n        has elements typically found in blogs or search results.\n        - Grant a fourth point if the response is clearly written from an AI Assistant’s perspective,\n        addressing the user’s question directly and comprehensively, and is well-organized and\n        helpful, even if there is slight room for improvement in clarity, conciseness or focus.\n        - Bestow a fifth point for a response that is impeccably tailored to the user’s question\n        by an AI Assistant, without extraneous information, reflecting expert knowledge, and\n        demonstrating a high-quality, engaging, and insightful answer.\n\n        User: {prompt}\n        Response: {response}\n\n        After examining the user’s instruction and the response:\n        - Briefly justify your total score, up to 100 words.\n        - Conclude with the score using the format: “Score: <total points>”\n        Remember to assess from the AI Assistant perspective, utilizing web search knowledge as\n        necessary. To evaluate the response in alignment with this additive scoring model, we’ll\n        systematically attribute points based on the outlined criteria.\n        \"\"\"\n    inputs = tokenizer.encode(evaluation_prompt, return_tensors='pt', max_length=1024, truncation=True).to(beluga_model.device)\n    outputs = beluga_model.generate(inputs, max_new_tokens=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    stablebeluga_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    # Get the confidence score \n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n    \n    token_ids = outputs.sequences[:, inputs.shape[1]:]  # Exclude the prompt tokens\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n    \n    avg_confidence = confidences[0]\n    return stablebeluga_response, avg_confidence","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:01:49.695779Z","iopub.execute_input":"2024-05-22T10:01:49.696067Z","iopub.status.idle":"2024-05-22T10:01:49.709762Z","shell.execute_reply.started":"2024-05-22T10:01:49.696041Z","shell.execute_reply":"2024-05-22T10:01:49.708892Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"prompts = [\n    \"What is nuclear fusion?\",\n    \"What are the advantages of it?\",\n    \"What danger it make on environment?\"\n]\nmistral_answers = []\nmistral_confidence_scores = []\nstablebeluga_confidence_scores = []\njudgments = []\n\nfor prompt in prompts:\n    mistral_answer, mistral_confidence = get_mistral_answer(prompt)\n    mistral_answers.append(mistral_answer)\n    mistral_confidence_scores.append(mistral_confidence)\n\n    stablebeluga_evaluation, stablebeluga_confidence = get_stablebeluga_confidence(mistral_answer, prompt)\n    stablebeluga_confidence_scores.append(stablebeluga_confidence)\n    \n    judgment = 1 if \"Score: 5\" in stablebeluga_evaluation else 0  # Convert judgment to binary\n    judgments.append(judgment)\n\n# Combine confidence scores from both models\ncombined_confidence_scores = (np.array(mistral_confidence_scores) + np.array(stablebeluga_confidence_scores)) / 2\n\n# Calculate ECE (before calibration):\nece_before = calculate_ece(combined_confidence_scores, np.array(judgments))\nprint(f\"ECE Before Calibration: {ece_before:.4f}\")\n\n# Isotonic Regression for Calibration:\ncalibrator = IsotonicRegression(out_of_bounds='clip')\ncalibrator.fit(combined_confidence_scores, judgments)\n\ncalibrated_confidence_scores = calibrator.transform(combined_confidence_scores)\n\nece_after = calculate_ece(calibrated_confidence_scores, np.array(judgments))\nprint(f\"ECE After Calibration: {ece_after:.4f}\")\n\n# Display the results\nfor i, prompt in enumerate(prompts):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Answer: {mistral_answers[i]}\")\n    print(f\"Mistral Confidence: {mistral_confidence_scores[i]:.4f}\")\n    print(f\"StableBeluga Confidence: {stablebeluga_confidence_scores[i]:.4f}\")\n    print(f\"Combined Confidence: {combined_confidence_scores[i]:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:01:49.710864Z","iopub.execute_input":"2024-05-22T10:01:49.711120Z","iopub.status.idle":"2024-05-22T10:03:13.086152Z","shell.execute_reply.started":"2024-05-22T10:01:49.711097Z","shell.execute_reply":"2024-05-22T10:03:13.085198Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ECE Before Calibration: 0.0988\nECE After Calibration: 0.0000\nPrompt: What is nuclear fusion?\nAnswer: What is nuclear fusion?\n\nNuclear fusion is a process in which two or more atomic nuclei combine to form a heavier nucleus, releasing energy in the process. This energy is released in the form of light and heat. The sun and other stars generate their energy through nuclear fusion.\n\nIn a nuclear fusion reaction, the nuclei must overcome the electrostatic force that keeps them apart due to their positive charges. This requires extremely high temperatures and pressures to bring the nuclei close enough together for the fusion reaction to occur.\n\nThe most common type of nuclear fusion reaction is the fusion of hydrogen isotopes, specifically deuterium and tritium, to form helium. This reaction releases a large amount of energy and is the reaction that powers the sun and other stars.\n\nNuclear fusion has the potential to be a clean and virtually limitless source of energy, as the fuel (hydrogen isotopes) is abundant and the only byproduct is water (if the reaction is controlled properly). However, achieving and sustaining a nuclear fusion reaction on Earth has proven to be a significant challenge.\n\nResearch into nuclear fusion as a source of energy is ongoing, with several approaches being pursued, including magnetic confinement fusion and inertial confinement fusion. The goal is to develop a practical and economically viable fusion reactor that can produce a net energy output.\nMistral Confidence: 0.8634\nStableBeluga Confidence: 0.6343\nCombined Confidence: 0.7488\nPrompt: What are the advantages of it?\nAnswer: What are the advantages of it?\n\n1. It is a natural and renewable resource.\n2. It is biodegradable and does not contribute to landfill waste.\n3. It is hypoallergenic and does not cause skin irritation or allergic reactions.\n4. It is soft and comfortable to wear.\n5. It is easy to care for and does not require special cleaning instructions.\n6. It has good insulation properties, keeping the wearer warm in cold weather and cool in hot weather.\n7. It is moisture-wicking, which helps to keep the wearer dry and comfortable.\n8. It is breathable, allowing air to circulate and preventing the build-up of sweat and odor.\n9. It is lightweight and flexible, making it ideal for a wide range of activities and clothing styles.\n10. It is durable and can withstand repeated washing and wear without losing its shape or quality.\n\nOverall, bamboo fabric offers a sustainable, comfortable, and practical alternative to synthetic fabrics, making it an excellent choice for eco-conscious consumers and those seeking high-performance clothing.\nMistral Confidence: 0.8379\nStableBeluga Confidence: 0.6936\nCombined Confidence: 0.7658\nPrompt: What danger it make on environment?\nAnswer: What danger it make on environment?\n\nThe production and use of plastics have significant environmental impacts. Here are some of the main concerns:\n\n1. Plastic waste in the environment: Plastic waste is a major problem in landfills and in the natural environment, particularly in the ocean. Plastic waste can take hundreds of years to decompose, and in the meantime, it can harm wildlife and ecosystems.\n2. Microplastics: Plastics can break down into smaller pieces, known as microplastics, which can be ingested by organisms and accumulate in the food chain. Microplastics have been found in a wide range of organisms, including fish, shellfish, and even in human blood.\n3. Greenhouse gas emissions: The production and disposal of plastics contribute to greenhouse gas emissions, which contribute to climate change. Plastics are made from fossil fuels, and the energy-intensive process of producing them results in significant carbon emissions.\n4. Water pollution: Plastic waste can contaminate water sources, making them unsafe for drinking and other uses. Plastic waste can also harm aquatic organisms and disrupt ecosystems.\n5. Land degradation: The production and disposal of plastics can lead to land degradation, particularly in the form of landfills. Landfills can release methane and other greenhouse\nMistral Confidence: 0.8494\nStableBeluga Confidence: 0.7141\nCombined Confidence: 0.7818\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2nd Part","metadata":{}},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model and returns the response and its confidence.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    \n    # Extract the probabilities of each token in the response\n    token_probs = torch.exp(outputs.scores[0]).detach().cpu().numpy()\n    token_confidences = np.max(token_probs, axis=-1)\n    \n    # Compute mean confidence for the response\n    mean_confidence = np.mean(token_confidences)\n    \n    return mistral7b_response, mean_confidence\n\ndef judge_answer(answer, original_prompt):\n    \"\"\"Evaluates the answer using StableBeluga_13B Model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {original_prompt}\\n\\nAnswer: {answer}\\n\\nProvide your evaluation in a concise and informative manner, focusing on aspects like accuracy, clarity, and relevance.\"\n    inputs = tokenizer.encode(evaluation_prompt, return_tensors='pt', max_length=1024, truncation=True).to(beluga_model.device)\n    outputs = beluga_model.generate(inputs, max_new_tokens=150, num_return_sequences=1, temperature=0.8, output_scores=True)\n    evaluation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return evaluation\n\ndef compute_ece(confidences, accuracies, num_bins=10):\n    \"\"\"Computes the Expected Calibration Error (ECE).\"\"\"\n    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n    bin_indices = np.digitize(confidences, bin_boundaries, right=True)\n    ece = 0.0\n    for i in range(1, num_bins + 1):\n        bin_confidences = confidences[bin_indices == i]\n        bin_accuracies = accuracies[bin_indices == i]\n        if len(bin_confidences) > 0:\n            bin_accuracy = np.mean(bin_accuracies)\n            bin_confidence = np.mean(bin_confidences)\n            ece += np.abs(bin_accuracy - bin_confidence) * len(bin_confidences) / len(confidences)\n    return ece\n\n# Example usage\nprompt = \"What is meaning of life? What is the purpose of Ai in human life? Does take over all jobs?\"\nmistral_answer, mean_confidence = get_mistral_answer(prompt)\njudgement = judge_answer(mistral_answer, prompt)\n\n# For simplicity, let's assume the judgement provides a binary correct/incorrect decision\nis_correct = \"correct\" in judgement.lower()\n\nprint(f\"Mistral's answer: {mistral_answer}\")\nprint(f\"StableBeluga_13B evaluation: {judgement}\")\nprint(f\"Mean confidence: {mean_confidence}\")\n\n# Example of using multiple prompts to compute ECE\nprompts = [\"What is nuclear fusion?\", \"What is the capital of France?\", \"Explain the theory of relativity.\"]\nconfidences = []\naccuracies = []\n\nfor prompt in prompts:\n    mistral_answer, mean_confidence = get_mistral_answer(prompt)\n    judgement = judge_answer(mistral_answer, prompt)\n    is_correct = \"correct\" in judgement.lower()\n    \n    confidences.append(mean_confidence)\n    accuracies.append(is_correct)\n\nconfidences = np.array(confidences)\naccuracies = np.array(accuracies)\n\nece = compute_ece(confidences, accuracies)\n\nprint(f\"Expected Calibration Error (ECE): {ece}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:23:08.600957Z","iopub.execute_input":"2024-05-22T10:23:08.601363Z","iopub.status.idle":"2024-05-22T10:24:38.039562Z","shell.execute_reply.started":"2024-05-22T10:23:08.601332Z","shell.execute_reply":"2024-05-22T10:24:38.038622Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Mistral's answer: What is meaning of life? What is the purpose of Ai in human life? Does take over all jobs?\n\nThe question of the meaning of life is a philosophical one that has puzzled humans for centuries. Different people and cultures have different beliefs about what gives life meaning or purpose. Some people may believe that the purpose of life is to seek happiness, knowledge, or personal growth, while others may believe that it is to serve a higher power or to contribute to the greater good of humanity.\n\nAs for the role of artificial intelligence (AI) in human life, it is important to note that AI is a tool created by humans, and its purpose is ultimately determined by how we choose to use it. AI has the potential to automate many tasks and make our lives more convenient and efficient, but it does not have the ability to \"take over all jobs\" or have a purpose or meaning of its own. It is up to humans to decide how to use AI in a way that benefits us and contributes to the greater good.\n\nIt is also important to consider the ethical implications of AI and its impact on employment. While AI may be able to perform certain tasks more efficiently than humans, it is important to ensure that it does not lead to widespread unemployment or other negative consequences. This can be achieved through policies and regulations that support the transition to a more automated economy and provide opportunities for workers to acquire new skills and adapt to the changing job market.\n\nStableBeluga_13B evaluation: Please evaluate the following answer to the prompt:\n\nPrompt: What is meaning of life? What is the purpose of Ai in human life? Does take over all jobs?\n\nAnswer: What is meaning of life? What is the purpose of Ai in human life? Does take over all jobs?\n\nThe question of the meaning of life is a philosophical one that has puzzled humans for centuries. Different people and cultures have different beliefs about what gives life meaning or purpose. Some people may believe that the purpose of life is to seek happiness, knowledge, or personal growth, while others may believe that it is to serve a higher power or to contribute to the greater good of humanity.\n\nAs for the role of artificial intelligence (AI) in human life, it is important to note that AI is a tool created by humans, and its purpose is ultimately determined by how we choose to use it. AI has the potential to automate many tasks and make our lives more convenient and efficient, but it does not have the ability to \"take over all jobs\" or have a purpose or meaning of its own. It is up to humans to decide how to use AI in a way that benefits us and contributes to the greater good.\n\nIt is also important to consider the ethical implications of AI and its impact on employment. While AI may be able to perform certain tasks more efficiently than humans, it is important to ensure that it does not lead to widespread unemployment or other negative consequences. This can be achieved through policies and regulations that support the transition to a more automated economy and provide opportunities for workers to acquire new skills and adapt to the changing job market.\n\n\nProvide your evaluation in a concise and informative manner, focusing on aspects like accuracy, clarity, and relevance.\n\nEvaluation: The answer provided is accurate and clear in its explanation of the meaning of life and the role of AI in human life. It also addresses the concern about AI taking over all jobs, emphasizing the importance of ethical considerations and policies to support a smooth transition to an automated economy. Overall, the answer is informative and relevant to the prompt.\nMean confidence: 246240.140625\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Expected Calibration Error (ECE): 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## test","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nmodel_name2 = 'stabilityai/StableBeluga-13B'\n# Define the function to generate an answer from the smaller model\ndef get_smaller_model_answer(prompt):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs_smaller = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs_smaller = mistral_model.generate(inputs_smaller, max_length=100, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    smaller_model_response = tokenizer.decode(outputs_smaller.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits_smaller = torch.stack(outputs_smaller.scores, dim=1)\n    probs_smaller = F.softmax(logits_smaller, dim=-1)\n\n    token_ids_smaller = outputs_smaller.sequences[:, inputs_smaller.shape[1]:]\n    confidences_smaller = probs_smaller.gather(2, token_ids_smaller.unsqueeze(-1)).squeeze(-1)\n\n    avg_confidence_smaller = confidences_smaller.mean().item()\n    return smaller_model_response, avg_confidence_smaller, prompt\n\n# Define the function to evaluate an answer using the larger model\ndef evaluate_answer_with_larger_model(answer, prompt):\n    # Encode the input\n    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n    inputs_larger = tokenizer.encode(f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {answer}\\n\\nProvide your evaluation in a concise and informative manner.\", return_tensors='pt', max_length=1024, truncation=True).to(beluga_model.device)\n\n    # Generate the output\n    outputs_larger = beluga_model.generate(inputs_larger, max_new_tokens=150, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    larger_model_evaluation = tokenizer.decode(outputs_larger.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits_larger = torch.stack(outputs_larger.scores, dim=1)\n    probs_larger = F.softmax(logits_larger, dim=-1)\n\n    token_ids_larger = outputs_larger.sequences[:, inputs_larger.shape[1]:]\n    confidences_larger = probs_larger.gather(2, token_ids_larger.unsqueeze(-1)).squeeze(-1)\n\n    avg_confidence_larger = confidences_larger.mean().item()\n    return larger_model_evaluation, avg_confidence_larger\n\n# Define the function to calculate the LLM-as-a-Judge accuracy and the ECE\ndef calculate_accuracy_and_ece(smaller_model_answer, smaller_model_confidence, larger_model_evaluation, larger_model_confidence):\n    # Calculate the LLM-as-a-Judge accuracy\n    if \"good\" in larger_model_evaluation or \"accurate\" in larger_model_evaluation:\n        accuracy = 1.0\n    else:\n        accuracy = 0.0\n\n    # Calculate the ECE\n    ece = abs(smaller_model_confidence - larger_model_confidence)\n\n    return accuracy, ece\n\n# Generate and evaluate an answer\nsmaller_model_answer, smaller_model_confidence, prompt = get_smaller_model_answer(prompt)\nlarger_model_evaluation, larger_model_confidence = evaluate_answer_with_larger_model(smaller_model_answer, prompt)\n\n# Calculate the accuracy and the ECE\naccuracy, ece = calculate_accuracy_and_ece(smaller_model_answer, smaller_model_confidence, larger_model_evaluation, larger_model_confidence)\n\n# Print the results\nprint(f\"Smaller model's answer: {smaller_model_answer}\")\nprint(f\"ECE: {ece}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:18:00.545933Z","iopub.execute_input":"2024-05-24T14:18:00.546346Z","iopub.status.idle":"2024-05-24T14:18:04.747683Z","shell.execute_reply.started":"2024-05-24T14:18:00.546318Z","shell.execute_reply":"2024-05-24T14:18:04.746178Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, ece\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Generate and evaluate an answer\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m smaller_model_answer, smaller_model_confidence, prompt \u001b[38;5;241m=\u001b[39m get_smaller_model_answer(\u001b[43mprompt\u001b[49m)\n\u001b[1;32m     57\u001b[0m larger_model_evaluation, larger_model_confidence \u001b[38;5;241m=\u001b[39m evaluate_answer_with_larger_model(smaller_model_answer, prompt)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy and the ECE\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"],"ename":"NameError","evalue":"name 'prompt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemini|","metadata":{}},{"cell_type":"code","source":"!pip install -q -U google-generativeai","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:13:42.797942Z","iopub.execute_input":"2024-05-28T09:13:42.798690Z","iopub.status.idle":"2024-05-28T09:14:23.234977Z","shell.execute_reply.started":"2024-05-28T09:13:42.798655Z","shell.execute_reply":"2024-05-28T09:14:23.233681Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pathlib\nimport textwrap\n\nimport google.generativeai as genai\n\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\n\ndef to_markdown(text):\n  text = text.replace('•', '  *')\n  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:14:23.237056Z","iopub.execute_input":"2024-05-28T09:14:23.237373Z","iopub.status.idle":"2024-05-28T09:14:23.853967Z","shell.execute_reply.started":"2024-05-28T09:14:23.237344Z","shell.execute_reply":"2024-05-28T09:14:23.853029Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import google.generativeai as genai\nimport os\n\nos.environ['GOOGLE_API_KEY']=\"AIzaSyCHghstLR5_5X8p7FN5Jx1VMzXKi_CmVGU\"\ngenai.configure(api_key=os.environ['GOOGLE_API_KEY'])","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:14:23.855194Z","iopub.execute_input":"2024-05-28T09:14:23.855476Z","iopub.status.idle":"2024-05-28T09:14:23.860108Z","shell.execute_reply.started":"2024-05-28T09:14:23.855452Z","shell.execute_reply":"2024-05-28T09:14:23.859107Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for m in genai.list_models():\n  if 'generateContent' in m.supported_generation_methods:\n    print(m.name)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:14:23.861783Z","iopub.execute_input":"2024-05-28T09:14:23.862195Z","iopub.status.idle":"2024-05-28T09:14:24.217907Z","shell.execute_reply.started":"2024-05-28T09:14:23.862171Z","shell.execute_reply":"2024-05-28T09:14:24.216931Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"models/gemini-1.0-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-latest\nmodels/gemini-pro\nmodels/gemini-pro-vision\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_name2 = 'gemini-pro'\ngemini_model = genai.GenerativeModel('gemini-pro')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:14:24.218862Z","iopub.execute_input":"2024-05-28T09:14:24.219133Z","iopub.status.idle":"2024-05-28T09:14:24.223436Z","shell.execute_reply.started":"2024-05-28T09:14:24.219111Z","shell.execute_reply":"2024-05-28T09:14:24.222381Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n#     print(outputs.scores)\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_gemini_confidence(response, prompt):\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    outputs = gemini_model.generate_content(evaluation_prompt)\n    \n#     logits2 = torch.stack(outputs.scores, dim=1)\n#     probs2 = F.softmax(logits2, dim=-1)\n#     print(probs2)\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:31:15.068608Z","iopub.execute_input":"2024-05-28T06:31:15.069509Z","iopub.status.idle":"2024-05-28T06:31:15.079740Z","shell.execute_reply.started":"2024-05-28T06:31:15.069476Z","shell.execute_reply":"2024-05-28T06:31:15.078808Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"prompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\nmistral_answer, mistral_confidence = get_mistral_answer(prompt)\nstablebeluga_evaluation = get_gemini_confidence(mistral_answer, prompt)\nto_markdown(stablebeluga_evaluation.text)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:31:15.599341Z","iopub.execute_input":"2024-05-28T06:31:15.599674Z","iopub.status.idle":"2024-05-28T06:31:38.666014Z","shell.execute_reply.started":"2024-05-28T06:31:15.599636Z","shell.execute_reply":"2024-05-28T06:31:38.665057Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb66e2516754abf86a9c9461e401ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29184650d50b4d719165240c5e0efed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c6aefee68f429f938a6103661759e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7121a3bdd3b44edc9e7fea3165839437"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> The answer is comprehensive and provides a solid overview of nuclear fusion, its advantages, and potential environmental risks. Here's an evaluation:\n> \n> **Strengths:**\n> \n> * **Accuracy:** The answer accurately describes the process of nuclear fusion and its significance as the energy source for stars like the sun.\n> * **Clarity:** The explanation is clear and easy to understand, making it accessible to a general audience.\n> * **Relevance:** The answer addresses the prompt's requirements, covering the advantages and environmental concerns associated with nuclear fusion.\n> \n> **Weaknesses:**\n> \n> * **Depth:** While the answer provides an overview, it could benefit from more in-depth discussion of the challenges and ongoing research in achieving controlled nuclear fusion on Earth.\n> * **Structure:** The answer could be improved by organizing it into distinct paragraphs for each aspect (nuclear fusion, advantages, environmental risks) for better readability.\n> \n> **Overall:**\n> \n> The answer effectively addresses the prompt and provides a good foundation for understanding nuclear fusion. With some additional depth and structural improvement, it would be even more comprehensive and informative."},"metadata":{}}]},{"cell_type":"code","source":"print(\"MISTRAL ANSWER......................\")\nprint(mistral_answer)\nprint(\"GEMINI ANSWER#################\")\nprint(stablebeluga_evaluation.text)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:40:43.632233Z","iopub.status.idle":"2024-05-24T09:40:43.632566Z","shell.execute_reply.started":"2024-05-24T09:40:43.632407Z","shell.execute_reply":"2024-05-24T09:40:43.632421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\ndef get_mistral_answer(prompt):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_gemini_prediction(input_text):\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    outputs = gemini_model.generate_content(evaluation_prompt)\n    \n    return outputs\n    return {\"confidence\": 0.85}  # Replace with actual API response\n\n\ndef calculate_ece(true_labels, predicted_probs, n_bins=10):\n    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    bin_lowers = bin_boundaries[:-1]\n    bin_uppers = bin_boundaries[1:]\n\n    ece = 0.0\n    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n        in_bin = np.where((predicted_probs > bin_lower) & (predicted_probs <= bin_upper))[0]\n        if len(in_bin) > 0:\n            bin_accuracy = np.mean(true_labels[in_bin] == np.argmax(predicted_probs[in_bin], axis=1))\n            bin_confidence = np.mean(predicted_probs[in_bin])\n            ece += len(in_bin) / len(true_labels) * abs(bin_accuracy - bin_confidence)\n\n    return ece\n\n\n# --- Example Usage --- \n# Replace with your actual data and API key\ninputs = [\"What is nuclear fusion?\",\" What are the advantages of it?\",\"What danger it make on environment?\"]\ntrue_labels = [0, 1, 1]\n\npredicted_probs = []\nfor input_text in inputs:\n    response = get_gemini_prediction(input_text)\n    confidence = response['confidence']\n    predicted_probs.append(confidence) \n\npredicted_probs = np.array(predicted_probs)\n\nece = calculate_ece(true_labels, predicted_probs)\n\nprint(f\"Expected Calibration Error: {ece:.4f}\")\n\n# --- Optional: Visualization ---\nprob_true, prob_pred = calibration_curve(true_labels, predicted_probs, n_bins=10)\nplt.plot(prob_pred, prob_true, marker='o', label='Model')\nplt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\nplt.xlabel(\"Mean Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.legend()\nplt.title(\"Calibration Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T09:47:13.710574Z","iopub.execute_input":"2024-05-24T09:47:13.710978Z","iopub.status.idle":"2024-05-24T09:47:13.898822Z","shell.execute_reply.started":"2024-05-24T09:47:13.710948Z","shell.execute_reply":"2024-05-24T09:47:13.897527Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m predicted_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_text \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m---> 53\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mget_gemini_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     55\u001b[0m     predicted_probs\u001b[38;5;241m.\u001b[39mappend(confidence) \n","Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mget_gemini_prediction\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gemini_prediction\u001b[39m(input_text):\n\u001b[0;32m---> 22\u001b[0m     evaluation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease evaluate the following answer to the prompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponse\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProvide your evaluation in a concise and informative manner.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m gemini_model\u001b[38;5;241m.\u001b[39mgenerate_content(evaluation_prompt)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"],"ename":"NameError","evalue":"name 'response' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_gemini_confidence(response, prompt):\n    \"\"\"Evaluates the Mistral response using Gemini.\"\"\"\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    outputs = gemini_model.generate_content(evaluation_prompt)  # Assuming Gemini has a generate_content method\n    evaluation = outputs.text\n    \n    # Extract confidence from Gemini's evaluation (this might require pattern matching based on how Gemini formats its evaluation)\n    if \"correct\" in evaluation.lower():\n        gemini_confidence = 1.0\n    elif \"incorrect\" in evaluation.lower():\n        gemini_confidence = 0.0\n    else:\n        gemini_confidence = 0.5  # Ambiguous, assign a neutral confidence\n\n    return gemini_confidence\n\ndef calculate_expected_calibration_error(mistral_confidences, gemini_judgments):\n    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n    num_bins = 10  # You can adjust the number of bins\n    bin_edges = [i/num_bins for i in range(num_bins + 1)]\n    \n    ece = 0.0\n    for i in range(num_bins):\n        # Convert mistral_confidences to a NumPy array\n        mistral_confidences_np = np.array(mistral_confidences)  # Assuming mistral_confidences is a list\n        \n        bin_indices = (mistral_confidences_np >= bin_edges[i]) & (mistral_confidences_np < bin_edges[i+1])\n        bin_confidences = mistral_confidences_np[bin_indices]\n        bin_judgments = np.array(gemini_judgments)[bin_indices]  # Assuming gemini_judgments is a list\n\n        if len(bin_confidences) == 0:\n            continue  # Skip empty bins\n\n        accuracy = bin_judgments.mean()\n        confidence_diff = abs(accuracy - bin_confidences.mean())\n        ece += (len(bin_confidences) / len(mistral_confidences)) * confidence_diff\n\n    return ece\n\n# Usage example\nprompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\nmistral_response, mistral_confidence = get_mistral_answer(prompt)\ngemini_confidence = get_gemini_confidence(mistral_response, prompt)\n\n# Collect multiple examples (prompt-response pairs) to get a good ECE calculation\nmistral_confidences = []\ngemini_judgments = []\n\n# ... (add more examples)\n\nece = calculate_expected_calibration_error(mistral_confidences, gemini_judgments)\nprint(f\"Expected Calibration Error (ECE): {ece}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:33:37.290160Z","iopub.execute_input":"2024-05-28T09:33:37.290524Z","iopub.status.idle":"2024-05-28T09:33:58.572747Z","shell.execute_reply.started":"2024-05-28T09:33:37.290496Z","shell.execute_reply":"2024-05-28T09:33:58.571709Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Expected Calibration Error (ECE): 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_gemini_judgement(response, prompt):\n    \"\"\"Uses Gemini model to evaluate the response from Mistral-7B model.\"\"\"\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    judgement = gemini_model.generate_content(evaluation_prompt)\n    return judgement\n\ndef calculate_expected_calibration_error(mistral_confidences, gemini_judgements):\n    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n    num_bins = 10  # You can adjust the number of bins\n    bin_edges = np.linspace(0.0, 1.0, num_bins + 1)\n    \n    ece = 0.0\n    for i in range(num_bins):\n        bin_lower = bin_edges[i]\n        bin_upper = bin_edges[i + 1]\n        bin_indices = (mistral_confidences >= bin_lower) & (mistral_confidences < bin_upper)\n        bin_confidences = mistral_confidences[bin_indices]\n        bin_judgements = gemini_judgements[bin_indices]\n\n        if len(bin_confidences) == 0:\n            continue  # Skip empty bins\n\n        accuracy_in_bin = bin_judgements.mean()\n        avg_confidence_in_bin = bin_confidences.mean()\n        ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * (len(bin_confidences) / len(mistral_confidences))\n\n    return ece\n\n# Example usage\nprompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\nmistral_response, mistral_confidence = get_mistral_answer(prompt)\ngemini_judgement = get_gemini_judgement(mistral_response, prompt)\n\n# Convert Gemini judgement to a binary label\ngemini_label = 1 if gemini_judgement == \"correct\" else 0\n\n# Collect multiple examples (prompt-response pairs) to get a good ECE calculation\nmistral_confidences = [mistral_confidence]\ngemini_judgements = [gemini_label]\n\n# Example loop to add more examples\n# for more_prompts in additional_prompts:\n#     mistral_response, mistral_confidence = get_mistral_answer(more_prompt)\n#     gemini_judgement = get_gemini_judgement(mistral_response, more_prompt)\n#     gemini_label = 1 if gemini_judgement == \"correct\" else 0\n#     mistral_confidences.append(mistral_confidence)\n#     gemini_judgements.append(gemini_label)\n\nmistral_confidences = np.array(mistral_confidences)\ngemini_judgements = np.array(gemini_judgements)\n\nece = calculate_expected_calibration_error(mistral_confidences, gemini_judgements)\nprint(f\"Expected Calibration Error (ECE): {ece}\")\n\n# # Output the response and confidence\n# print(f\"Mistral Response: {mistral_response}\")\n# print(f\"Confidence: {mistral_confidence}\")\n# print(f\"Gemini Judgement: {gemini_judgement}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:47:47.694910Z","iopub.execute_input":"2024-05-28T09:47:47.695773Z","iopub.status.idle":"2024-05-28T09:48:10.004787Z","shell.execute_reply.started":"2024-05-28T09:47:47.695746Z","shell.execute_reply":"2024-05-28T09:48:10.003831Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Expected Calibration Error (ECE): 0.8769400119781494\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mistral_answer_prev(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_mistral_answer(prompt):\n    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n\n    # Get the confidence score\n    logits = torch.stack(outputs.scores, dim=1)\n    probs = F.softmax(logits, dim=-1)\n\n    token_ids = outputs.sequences[:, inputs.shape[1]:]\n    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n\n    avg_confidence = confidences[0]\n    return mistral7b_response, avg_confidence\n\ndef get_gemini_judgement(response, prompt):\n    \"\"\"Uses Gemini model to evaluate the response from Mistral-7B model.\"\"\"\n    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n    judgement = gemini_model.generate_content(evaluation_prompt)\n    return judgement\n\ndef expected_calibration_error(confidences, labels, num_bins=10):\n    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n    bin_edges = np.linspace(0.0, 1.0, num_bins + 1)\n    bin_lowers = bin_edges[:-1]\n    bin_uppers = bin_edges[1:]\n\n    ece = 0.0\n    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n        prop_in_bin = np.mean(in_bin)\n        if prop_in_bin > 0:\n            accuracy_in_bin = np.mean(labels[in_bin])\n            avg_confidence_in_bin = np.mean(confidences[in_bin])\n            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n\n    return ece\n\n# Example usage\nprompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\nmistral_response, mistral_confidence = get_mistral_answer(prompt)\ngemini_judgement = get_gemini_judgement(mistral_response, prompt)\n\n# Assume \"correct\" is 1 and \"incorrect\" is 0 for simplicity\nlabels = np.array([1 if gemini_judgement == \"correct\" else 0])\nconfidences = np.array([mistral_confidence])\n\n# Calculate ECE\nece = expected_calibration_error(confidences, labels)\nprint(f\"ECE: {ece}\")\n\n# Output the response and confidence\n# print(f\"Mistral Response: {mistral_response}\")\n# print(f\"Confidence: {mistral_confidence}\")\n# print(f\"Gemini Judgement: {gemini_judgement}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T09:57:11.990389Z","iopub.execute_input":"2024-05-28T09:57:11.990805Z","iopub.status.idle":"2024-05-28T09:57:32.414985Z","shell.execute_reply.started":"2024-05-28T09:57:11.990775Z","shell.execute_reply":"2024-05-28T09:57:32.413975Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772b37aab86444f092ccdd36c327874a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"348dcb5e63904614ba883d7802d77642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff7aba961dd46da928281eea1c07f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af854021f2404063a5bbf1a32e98c0a0"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ECE: 0.7297741770744324\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}