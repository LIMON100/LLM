{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-12T10:06:23.144055Z","iopub.status.busy":"2024-06-12T10:06:23.143647Z","iopub.status.idle":"2024-06-12T10:06:48.325635Z","shell.execute_reply":"2024-06-12T10:06:48.323948Z","shell.execute_reply.started":"2024-06-12T10:06:23.144021Z"},"trusted":true},"outputs":[],"source":["!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:04:11.697570Z","iopub.status.busy":"2024-06-12T08:04:11.697230Z","iopub.status.idle":"2024-06-12T08:04:12.061513Z","shell.execute_reply":"2024-06-12T08:04:12.060397Z","shell.execute_reply.started":"2024-06-12T08:04:11.697536Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b134bd1087da46f3acf6c89859fc8000","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:04:41.612578Z","iopub.status.busy":"2024-06-12T08:04:41.611782Z","iopub.status.idle":"2024-06-12T08:05:24.491367Z","shell.execute_reply":"2024-06-12T08:05:24.489961Z","shell.execute_reply.started":"2024-06-12T08:04:41.612542Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:05:24.494319Z","iopub.status.busy":"2024-06-12T08:05:24.493688Z","iopub.status.idle":"2024-06-12T08:05:28.684400Z","shell.execute_reply":"2024-06-12T08:05:28.683151Z","shell.execute_reply.started":"2024-06-12T08:05:24.494272Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-12 08:05:25--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\n","Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n","Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 166184960 (158M) [application/x-tar]\n","Saving to: 'data.tar'\n","\n","data.tar            100%[===================>] 158.49M  55.0MB/s    in 2.9s    \n","\n","2024-06-12 08:05:28 (55.0 MB/s) - 'data.tar' saved [166184960/166184960]\n","\n"]}],"source":["!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:05:28.686170Z","iopub.status.busy":"2024-06-12T08:05:28.685859Z","iopub.status.idle":"2024-06-12T08:05:28.946146Z","shell.execute_reply":"2024-06-12T08:05:28.945195Z","shell.execute_reply.started":"2024-06-12T08:05:28.686139Z"},"trusted":true},"outputs":[],"source":["import tarfile\n","unzip_path = '.'\n","tar = tarfile.open('data.tar')\n","tar.extractall(path=unzip_path)\n","tar.close()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:05:28.950634Z","iopub.status.busy":"2024-06-12T08:05:28.949545Z","iopub.status.idle":"2024-06-12T08:06:27.693981Z","shell.execute_reply":"2024-06-12T08:06:27.692919Z","shell.execute_reply.started":"2024-06-12T08:05:28.950589Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting groq\n","  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n","Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: groq\n","Successfully installed groq-0.9.0\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\n","Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m882.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install groq\n","!pip install bert_score"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:27.695792Z","iopub.status.busy":"2024-06-12T08:06:27.695436Z","iopub.status.idle":"2024-06-12T08:06:50.762729Z","shell.execute_reply":"2024-06-12T08:06:50.761732Z","shell.execute_reply.started":"2024-06-12T08:06:27.695758Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-12 08:06:35.479374: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-12 08:06:35.479542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-12 08:06:35.633385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","import numpy as np\n","from sklearn.isotonic import IsotonicRegression\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from transformers import pipeline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pathlib\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","from bert_score import score\n","import os\n","from groq import Groq\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","import json\n","import re\n","import concurrent.futures\n","from tqdm import tqdm\n","from yaml import safe_load"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:50.764826Z","iopub.status.busy":"2024-06-12T08:06:50.764085Z","iopub.status.idle":"2024-06-12T08:06:50.965434Z","shell.execute_reply":"2024-06-12T08:06:50.964394Z","shell.execute_reply.started":"2024-06-12T08:06:50.764789Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:50.966886Z","iopub.status.busy":"2024-06-12T08:06:50.966589Z","iopub.status.idle":"2024-06-12T08:06:50.971961Z","shell.execute_reply":"2024-06-12T08:06:50.970830Z","shell.execute_reply.started":"2024-06-12T08:06:50.966859Z"},"trusted":true},"outputs":[],"source":["# # Define models and tokenizers\n","# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n","# tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.float16\n","# )\n","\n","# mistral_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:50.973689Z","iopub.status.busy":"2024-06-12T08:06:50.973310Z","iopub.status.idle":"2024-06-12T08:06:50.986257Z","shell.execute_reply":"2024-06-12T08:06:50.985046Z","shell.execute_reply.started":"2024-06-12T08:06:50.973654Z"},"trusted":true},"outputs":[],"source":["# model_name2 = 'stabilityai/StableBeluga-13B'\n","# tokenizer_beluga = AutoTokenizer.from_pretrained(model_name2)\n","# tokenizer_beluga.pad_token = tokenizer_beluga.eos_token\n","\n","# beluga_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name2,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:50.987841Z","iopub.status.busy":"2024-06-12T08:06:50.987543Z","iopub.status.idle":"2024-06-12T08:06:50.997920Z","shell.execute_reply":"2024-06-12T08:06:50.996964Z","shell.execute_reply.started":"2024-06-12T08:06:50.987815Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return ' '.join(filtered_tokens)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:51.001794Z","iopub.status.busy":"2024-06-12T08:06:51.001057Z","iopub.status.idle":"2024-06-12T08:06:51.014377Z","shell.execute_reply":"2024-06-12T08:06:51.013207Z","shell.execute_reply.started":"2024-06-12T08:06:51.001764Z"},"trusted":true},"outputs":[],"source":["# Utility functions\n","def load_questions(file_path):\n","    questions = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            questions.append(json.loads(line.strip()))\n","    return questions\n","\n","def load_model_answers(dir_path):\n","    model_answers = {}\n","    for file_name in os.listdir(dir_path):\n","        model_name = file_name.split('.')[0]\n","        model_answers[model_name] = {}\n","        with open(os.path.join(dir_path, file_name), 'r') as f:\n","            for line in f:\n","                answer = json.loads(line.strip())\n","                question_id = answer['question_id']\n","                model_answers[model_name][question_id] = answer\n","    return model_answers\n","\n","def get_endpoint(endpoint_config):\n","    # Implementation depends on the format of your API config file\n","    return endpoint_config\n","\n","def make_config(file_path):\n","    with open(file_path, 'r') as f:\n","        return safe_load(f)\n","\n","def chat_completion_groq(model, conv, temperature, max_tokens, api_key):\n","    client = Groq(api_key=api_key)\n","    chat_completion = client.chat.completions.create(\n","        messages=conv,\n","        model=model,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    return chat_completion.choices[0].message.content\n","\n","def get_score(judgment, pattern, pairwise=True):\n","    matches = pattern.findall(judgment)\n","    matches = [m for m in matches if m != \"\"]\n","    if len(set(matches)) == 0:\n","        return None, True\n","    elif len(set(matches)) == 1:\n","        if pairwise:\n","            return matches[0].strip(\"\\n\"), False\n","        return int(matches[0])\n","    else:\n","        return None, False"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:06:51.015743Z","iopub.status.busy":"2024-06-12T08:06:51.015426Z","iopub.status.idle":"2024-06-12T08:06:53.085057Z","shell.execute_reply":"2024-06-12T08:06:53.084051Z","shell.execute_reply.started":"2024-06-12T08:06:51.015715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["**What is the meaning of life?**\n","\n","Philosophers, theologians, and thinkers have debated the meaning of life for centuries. There's no one answer, as it's a highly subjective and complex question. Here are some perspectives:\n","\n","1. **Existentialism**: Life has no inherent meaning; we create our own purpose through our experiences, choices, and values.\n","2. **Religious**: Life's meaning is often linked to a higher power or divine purpose, with the ultimate goal of achieving spiritual enlightenment or salvation.\n","3. **Humanism**: Life's meaning is found in human dignity, compassion, and the pursuit of happiness, knowledge, and personal growth.\n","4. **Biological**: Life's meaning is to survive, reproduce, and perpetuate the species.\n","\n","Ultimately, the meaning of life is a personal and subjective inquiry that each individual must answer for themselves.\n","\n","**How modern AI is affecting real life:**\n","\n","Artificial Intelligence (AI) is transforming various aspects of our lives, including:\n","\n","1. **Workforce automation**: AI-powered robots and algorithms are replacing some jobs, changing the nature of work, and requiring new skills.\n","2. **Personalization and convenience**: AI-driven recommendations, chatbots, and virtual assistants (e.g., Siri, Alexa) are enhancing customer experiences and simplifying tasks.\n","3. **Healthcare advancements**: AI is improving diagnosis accuracy, streamlining medical research, and enabling personalized medicine.\n","4. **Data analysis and insights**: AI-powered analytics are helping businesses make data-driven decisions, optimize operations, and detect patterns.\n","\n","However, AI also raises concerns about:\n","\n","1. **Job displacement and inequality**: AI might exacerbate income disparities and unemployment.\n","2. **Bias and discrimination**: AI systems can perpetuate biases present in the data used to train them.\n","3. **Privacy and security**: AI can compromise personal data and create new vulnerabilities.\n","\n","**AI outcomes in the future:**\n","\n","1. **Increased automation**: AI will continue to automate tasks, potentially leading to significant job displacement.\n","2. **Improved healthcare**: AI-driven medical breakthroughs will enhance diagnosis, treatment, and patient outcomes.\n","3. **Enhanced decision-making**: AI will become an essential tool for businesses, governments, and individuals to make informed decisions.\n","4. **Risks and challenges**: The development of autonomous weapons, AI-driven disinformation, and job displacement will require careful consideration and regulation.\n","5. **Singularity and superintelligence**: The possibility of creating a superintelligent AI, which could surpass human intelligence and pose existential risks, is a topic of ongoing debate and research.\n","\n","In the future, AI will likely:\n","\n","1. **Augment human capabilities**: AI will enhance human abilities, freeing us to focus on creative, high-value tasks.\n","2. **Transform education**: AI-driven adaptive learning systems will revolutionize the way we learn.\n","3. **Shape urban planning and infrastructure**: AI will optimize urban development, transportation, and resource allocation.\n","\n","Ultimately, the future of AI will be shaped by the choices we make today, and it's crucial to prioritize responsible AI development, ethics, and governance to ensure AI benefits humanity as a whole.\n"]}],"source":["client = Groq(api_key=\"Your_Api_key\")\n","\n","chat_completion = client.chat.completions.create(\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n","        }\n","\n","    ],\n","    model=\"llama3-70b-8192\",\n",")\n","\n","print(chat_completion.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model, regex_pattern, temperature=0.8, max_tokens=300, pairwise=True):\n","        self.api_key = api_key\n","        self.judge_model = judge_model\n","        self.regex_pattern = re.compile(regex_pattern)\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","        self.pairwise = pairwise\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","    def get_mistral_answer(self, prompt, include_stopwords=True):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n","            do_sample=True, output_scores=True, return_dict_in_generate=True\n","        )\n","\n","        response_ids = outputs.sequences[0]\n","        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n","\n","        logits = outputs.scores  # Logits of the generated tokens\n","        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n","\n","        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n","        if include_stopwords:\n","            token_confidences = [prob.max().item() for prob in softmax_probs]\n","        else:\n","            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n","\n","        mean_confidence = np.mean(token_confidences)\n","\n","        return response_text, mean_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=\"llama3-70b-8192\",\n","        )\n","\n","        llama3_response = chat_completion.choices[0].message.content.strip()\n","        return llama3_response\n","\n","    def parse_evaluation(self, evaluation):\n","        if \"1 point\" in evaluation:\n","            return 1\n","        return 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n","        )\n","\n","        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate(self, prompts):\n","        self.prompts = prompts  # Ensure prompts are stored\n","        for prompt in prompts:\n","            response, confidence = self.get_mistral_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies\n","\n","    def plot_reliability_diagram(self, bins, bin_accuracies):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot((bins[:-1] + bins[1:]) / 2, bin_accuracies, marker='o', label='Accuracy per bin')\n","        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n","        plt.xlabel('Confidence')\n","        plt.ylabel('Accuracy')\n","        plt.title('Reliability Diagram')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\", regex_pattern=r\"(correct|incorrect)\")\n","prompts = [\"What is nuclear fusion?\", \"Tell me about the current AI situation in the world?\", \"How does a combustion engine work?\"]\n","pipeline.evaluate(prompts)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data, bins, bin_accuracies = pipeline.calculate_ece()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["few_shot_examples = \"\"\"\n","Q: What is nuclear fusion?\n","A: Nuclear fusion is a reaction in which two atomic nuclei combine to form a heavier nucleus, releasing energy in the process.\n","\n","Q: How does a combustion engine work?\n","A: A combustion engine works by burning fuel in a combustion chamber to produce mechanical energy that drives the engine.\n","\"\"\"\n","\n","def get_mistral_answer_few_shot(prompt):\n","    few_shot_prompt = few_shot_examples + f\"\\nQ: {prompt}\\nA:\"\n","    response, confidence = get_mistral_answer(few_shot_prompt)\n","    return response, confidence\n","\n","# Example usage:\n","few_shot_response, few_shot_confidence = get_mistral_answer_few_shot(\"Tell me about the current AI situation in the world.\")\n","print(f\"Few-shot response: {few_shot_response}\")\n","print(f\"Few-shot confidence: {few_shot_confidence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Class pipeline for mmlu dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:07:07.364696Z","iopub.status.busy":"2024-06-12T08:07:07.364003Z","iopub.status.idle":"2024-06-12T08:07:07.371627Z","shell.execute_reply":"2024-06-12T08:07:07.370485Z","shell.execute_reply.started":"2024-06-12T08:07:07.364657Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, model_name):\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","    def encode(self, text, max_length=1024):\n","        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n","\n","    def decode(self, tokens):\n","        return self.tokenizer.decode(tokens, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:07:10.112626Z","iopub.status.busy":"2024-06-12T08:07:10.112179Z","iopub.status.idle":"2024-06-12T08:07:10.159604Z","shell.execute_reply":"2024-06-12T08:07:10.158497Z","shell.execute_reply.started":"2024-06-12T08:07:10.112590Z"},"trusted":true},"outputs":[],"source":["  class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","\n","        self.tokenizer = Tokenizer(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def get_model_answer(self, prompt):\n","        \"\"\"Generates an answer from the specified model.\"\"\"\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_new_tokens=300,  # Limit the number of generated tokens\n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0])\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0])\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:07:11.876735Z","iopub.status.busy":"2024-06-12T08:07:11.876354Z","iopub.status.idle":"2024-06-12T08:07:11.880888Z","shell.execute_reply":"2024-06-12T08:07:11.879978Z","shell.execute_reply.started":"2024-06-12T08:07:11.876705Z"},"trusted":true},"outputs":[],"source":["# pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","# pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Chain of thought"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:07:13.297674Z","iopub.status.busy":"2024-06-12T08:07:13.297234Z","iopub.status.idle":"2024-06-12T08:07:13.345232Z","shell.execute_reply":"2024-06-12T08:07:13.344157Z","shell.execute_reply.started":"2024-06-12T08:07:13.297640Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from bert_score import score\n","\n","class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def generate_cot_prompt(self, question):\n","        # Chain of Thought prompt\n","        cot_prompt = f\"Q: {question}\\nA: Let's think through this step by step.\\n\"\n","        cot_prompt += \"First, consider the main aspects of the question. \"\n","        cot_prompt += \"Next, break down the problem into smaller parts. \"\n","        cot_prompt += \"Finally, synthesize the information to form a coherent answer.\"\n","        return cot_prompt\n","\n","    def get_model_answer(self, question):\n","        \"\"\"Generates an answer from the specified model using Chain of Thought prompting.\"\"\"\n","        prompt = self.generate_cot_prompt(question)\n","        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_new_tokens=300,  # Limit the number of generated tokens\n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-12T08:07:14.122464Z","iopub.status.busy":"2024-06-12T08:07:14.121386Z","iopub.status.idle":"2024-06-12T10:04:06.843947Z","shell.execute_reply":"2024-06-12T10:04:06.843025Z","shell.execute_reply.started":"2024-06-12T08:07:14.122417Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37fd59a85bf14c60808bc94248c46a04","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4591a5d806f849ddb0fea4211f593f3c","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4a2b0ed62414c94a1acb32360a47f51","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2e8e365f67c49ca8292a64974837524","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35e0919dc78b4424bd5cfd93a1d413be","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3266c3632efa42fda315aa8e3f5a355a","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc53bff2b1c24d55bfc07c89e87ae0ff","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a23c4737b48d4d8cba683837f7faeb93","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46f9d28aa9664b179ef1b063a6a23b74","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e899b72420442edae880f54184fe5bf","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7c8a72838df4d869e34b5ffcb297d4e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d7379209118434f9227749a73434bc2","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating prehistory in category history...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0202e6c68ad54309ab0fcc27a7035d39","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a637ea1a1ca04b64bd773cec79d3fb70","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"109863c868bf4c6dbc32911667ffe4a4","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3229d0ba00248e78c58282119c4f5c3","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43058e2ef3ad433d85b98984b4e28801","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6339928507804871, Recall: 0.646126389503479, F1: 0.6392560601234436\n","Expected Calibration Error (ECE): 0.44999999999999996\n","prehistory ECE: 0.44999999999999996\n","Evaluating elementary_mathematics in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6837377548217773, Recall: 0.7477951049804688, F1: 0.713681161403656\n","Expected Calibration Error (ECE): 0.050000000000000044\n","elementary_mathematics ECE: 0.050000000000000044\n","Evaluating college_physics in category physics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7158308625221252, Recall: 0.7305806279182434, F1: 0.7228883504867554\n","Expected Calibration Error (ECE): 0.44999999999999996\n","college_physics ECE: 0.44999999999999996\n","Evaluating professional_psychology in category psychology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.659892201423645, Recall: 0.6891579627990723, F1: 0.6739794015884399\n","Expected Calibration Error (ECE): 0.050000000000000044\n","professional_psychology ECE: 0.050000000000000044\n","Evaluating professional_accounting in category other...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6989593505859375, Recall: 0.7309845685958862, F1: 0.7142873406410217\n","Expected Calibration Error (ECE): 0.5\n","professional_accounting ECE: 0.5\n","Evaluating college_computer_science in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7804489731788635, Recall: 0.8155450224876404, F1: 0.797234296798706\n","Expected Calibration Error (ECE): 0.19999999999999996\n","college_computer_science ECE: 0.19999999999999996\n","Evaluating professional_medicine in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7515987157821655, Recall: 0.7363089323043823, F1: 0.7437739968299866\n","Expected Calibration Error (ECE): 0.050000000000000044\n","professional_medicine ECE: 0.050000000000000044\n","Evaluating moral_scenarios in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7018132209777832, Recall: 0.8154664039611816, F1: 0.7543392181396484\n","Expected Calibration Error (ECE): 0.19999999999999996\n","moral_scenarios ECE: 0.19999999999999996\n","Evaluating high_school_mathematics in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.714411735534668, Recall: 0.7361297011375427, F1: 0.7244102358818054\n","Expected Calibration Error (ECE): 0.7\n","high_school_mathematics ECE: 0.7\n","Evaluating clinical_knowledge in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6294011473655701, Recall: 0.6284540295600891, F1: 0.6286017894744873\n","Expected Calibration Error (ECE): 0.19999999999999996\n","clinical_knowledge ECE: 0.19999999999999996\n","Evaluating marketing in category business...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6170973181724548, Recall: 0.6716013550758362, F1: 0.6417735815048218\n","Expected Calibration Error (ECE): 0.050000000000000044\n","marketing ECE: 0.050000000000000044\n","Evaluating management in category business...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.5874842405319214, Recall: 0.6876441836357117, F1: 0.6325666904449463\n","Expected Calibration Error (ECE): 0.44999999999999996\n","management ECE: 0.44999999999999996\n","Evaluating high_school_chemistry in category chemistry...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6899381875991821, Recall: 0.7074669003486633, F1: 0.6985300779342651\n","Expected Calibration Error (ECE): 0.44999999999999996\n","high_school_chemistry ECE: 0.44999999999999996\n","Evaluating virology in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6208117008209229, Recall: 0.5941415429115295, F1: 0.606736421585083\n","Expected Calibration Error (ECE): 0.44999999999999996\n","virology ECE: 0.44999999999999996\n","Evaluating moral_disputes in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.5990657806396484, Recall: 0.6261063814163208, F1: 0.610586404800415\n","Expected Calibration Error (ECE): 0.050000000000000044\n","moral_disputes ECE: 0.050000000000000044\n","Evaluating us_foreign_policy in category politics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6590663194656372, Recall: 0.6653509140014648, F1: 0.6602483987808228\n","Expected Calibration Error (ECE): 0.050000000000000044\n","us_foreign_policy ECE: 0.050000000000000044\n","Evaluating college_medicine in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6443365812301636, Recall: 0.6429410576820374, F1: 0.6435619592666626\n","Expected Calibration Error (ECE): 0.19999999999999996\n","college_medicine ECE: 0.19999999999999996\n","Evaluating world_religions in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6396685838699341, Recall: 0.6598471403121948, F1: 0.6493556499481201\n","Expected Calibration Error (ECE): 0.07500000000000001\n","world_religions ECE: 0.07500000000000001\n","Evaluating public_relations in category politics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6100300550460815, Recall: 0.7041884660720825, F1: 0.6532495021820068\n","Expected Calibration Error (ECE): 0.19999999999999996\n","public_relations ECE: 0.19999999999999996\n","Evaluating college_biology in category biology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6936075687408447, Recall: 0.6982257962226868, F1: 0.6947707533836365\n","Expected Calibration Error (ECE): 0.44999999999999996\n","college_biology ECE: 0.44999999999999996\n","Evaluating abstract_algebra in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6769496202468872, Recall: 0.6974403858184814, F1: 0.6866719126701355\n","Expected Calibration Error (ECE): 0.25000000000000006\n","abstract_algebra ECE: 0.25000000000000006\n","Evaluating philosophy in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6627349853515625, Recall: 0.6728911995887756, F1: 0.6667096018791199\n","Expected Calibration Error (ECE): 0.19999999999999996\n","philosophy ECE: 0.19999999999999996\n","Evaluating security_studies in category politics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6657552719116211, Recall: 0.6864618062973022, F1: 0.67559814453125\n","Expected Calibration Error (ECE): 0.19999999999999996\n","security_studies ECE: 0.19999999999999996\n","Evaluating anatomy in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6795506477355957, Recall: 0.6944300532341003, F1: 0.685681939125061\n","Expected Calibration Error (ECE): 0.050000000000000044\n","anatomy ECE: 0.050000000000000044\n","Evaluating logical_fallacies in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6087924242019653, Recall: 0.623183012008667, F1: 0.6146827936172485\n","Expected Calibration Error (ECE): 0.24999999999999997\n","logical_fallacies ECE: 0.24999999999999997\n","Evaluating college_mathematics in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7361247539520264, Recall: 0.7744659185409546, F1: 0.7534113526344299\n","Expected Calibration Error (ECE): 0.44999999999999996\n","college_mathematics ECE: 0.44999999999999996\n","Evaluating global_facts in category other...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6144339442253113, Recall: 0.6477363109588623, F1: 0.6284106373786926\n","Expected Calibration Error (ECE): 0.050000000000000044\n","global_facts ECE: 0.050000000000000044\n","Evaluating professional_law in category law...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7822022438049316, Recall: 0.8072214722633362, F1: 0.7944397330284119\n","Expected Calibration Error (ECE): 0.44999999999999996\n","professional_law ECE: 0.44999999999999996\n","Evaluating high_school_computer_science in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.72240149974823, Recall: 0.7563115358352661, F1: 0.7377078533172607\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_computer_science ECE: 0.19999999999999996\n","Evaluating electrical_engineering in category engineering...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.704463005065918, Recall: 0.7110668420791626, F1: 0.7075126767158508\n","Expected Calibration Error (ECE): 0.44999999999999996\n","electrical_engineering ECE: 0.44999999999999996\n","Evaluating high_school_physics in category physics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6649301648139954, Recall: 0.7568923830986023, F1: 0.7069213390350342\n","Expected Calibration Error (ECE): 0.44999999999999996\n","high_school_physics ECE: 0.44999999999999996\n","Evaluating high_school_psychology in category psychology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6651532053947449, Recall: 0.6891831755638123, F1: 0.676491379737854\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_psychology ECE: 0.19999999999999996\n","Evaluating international_law in category law...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6141598224639893, Recall: 0.6216412782669067, F1: 0.6169857978820801\n","Expected Calibration Error (ECE): 0.24999999999999997\n","international_law ECE: 0.24999999999999997\n","Evaluating astronomy in category physics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6521729826927185, Recall: 0.6891274452209473, F1: 0.6696947813034058\n","Expected Calibration Error (ECE): 0.29999999999999993\n","astronomy ECE: 0.29999999999999993\n","Evaluating high_school_statistics in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7476105690002441, Recall: 0.7890998125076294, F1: 0.7674471139907837\n","Expected Calibration Error (ECE): 0.44999999999999996\n","high_school_statistics ECE: 0.44999999999999996\n","Evaluating econometrics in category economics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7505456209182739, Recall: 0.8024786710739136, F1: 0.7753179669380188\n","Expected Calibration Error (ECE): 0.7\n","econometrics ECE: 0.7\n","Evaluating high_school_biology in category biology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6344455480575562, Recall: 0.7458221912384033, F1: 0.6854252219200134\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_biology ECE: 0.19999999999999996\n","Evaluating human_aging in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6449713706970215, Recall: 0.6910526156425476, F1: 0.6668069362640381\n","Expected Calibration Error (ECE): 0.24999999999999997\n","human_aging ECE: 0.24999999999999997\n","Evaluating medical_genetics in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6394978761672974, Recall: 0.657630205154419, F1: 0.6479019522666931\n","Expected Calibration Error (ECE): 0.050000000000000044\n","medical_genetics ECE: 0.050000000000000044\n","Evaluating high_school_world_history in category history...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7807133793830872, Recall: 0.8330931663513184, F1: 0.8058438897132874\n","Expected Calibration Error (ECE): 0.07500000000000001\n","high_school_world_history ECE: 0.07500000000000001\n","Evaluating nutrition in category health...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.5790227651596069, Recall: 0.6216880083084106, F1: 0.5979691743850708\n","Expected Calibration Error (ECE): 0.19999999999999996\n","nutrition ECE: 0.19999999999999996\n","Evaluating miscellaneous in category other...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6453812718391418, Recall: 0.7123745679855347, F1: 0.6771827340126038\n","Expected Calibration Error (ECE): 0.050000000000000044\n","miscellaneous ECE: 0.050000000000000044\n","Evaluating conceptual_physics in category physics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6365315914154053, Recall: 0.6427524089813232, F1: 0.639305830001831\n","Expected Calibration Error (ECE): 0.5\n","conceptual_physics ECE: 0.5\n","Evaluating high_school_geography in category geography...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6304186582565308, Recall: 0.6660398244857788, F1: 0.6458275318145752\n","Expected Calibration Error (ECE): 0.09999999999999998\n","high_school_geography ECE: 0.09999999999999998\n","Evaluating business_ethics in category business...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6732578277587891, Recall: 0.7688603401184082, F1: 0.7152594327926636\n","Expected Calibration Error (ECE): 0.19999999999999996\n","business_ethics ECE: 0.19999999999999996\n","Evaluating sociology in category culture...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.629059910774231, Recall: 0.6954232454299927, F1: 0.6600255370140076\n","Expected Calibration Error (ECE): 0.19999999999999996\n","sociology ECE: 0.19999999999999996\n","Evaluating jurisprudence in category law...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6437543630599976, Recall: 0.6834267973899841, F1: 0.6619002819061279\n","Expected Calibration Error (ECE): 0.050000000000000044\n","jurisprudence ECE: 0.050000000000000044\n","Evaluating formal_logic in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7114101052284241, Recall: 0.7435455918312073, F1: 0.7261021733283997\n","Expected Calibration Error (ECE): 0.19999999999999996\n","formal_logic ECE: 0.19999999999999996\n","Evaluating college_chemistry in category chemistry...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7091479301452637, Recall: 0.72735196352005, F1: 0.7179147601127625\n","Expected Calibration Error (ECE): 0.44999999999999996\n","college_chemistry ECE: 0.44999999999999996\n","Evaluating high_school_us_history in category history...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.8610353469848633, Recall: 0.914620041847229, F1: 0.886654794216156\n","Expected Calibration Error (ECE): 0.050000000000000044\n","high_school_us_history ECE: 0.050000000000000044\n","Evaluating high_school_government_and_politics in category politics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6286543011665344, Recall: 0.6596214175224304, F1: 0.643152117729187\n","Expected Calibration Error (ECE): 0.050000000000000044\n","high_school_government_and_politics ECE: 0.050000000000000044\n","Evaluating high_school_macroeconomics in category economics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.655210018157959, Recall: 0.644071102142334, F1: 0.6490761041641235\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_macroeconomics ECE: 0.19999999999999996\n","Evaluating machine_learning in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6514697670936584, Recall: 0.6659669280052185, F1: 0.6585547924041748\n","Expected Calibration Error (ECE): 0.44999999999999996\n","machine_learning ECE: 0.44999999999999996\n","Evaluating human_sexuality in category culture...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6180141568183899, Recall: 0.6692608594894409, F1: 0.6419561505317688\n","Expected Calibration Error (ECE): 0.050000000000000044\n","human_sexuality ECE: 0.050000000000000044\n","Evaluating high_school_microeconomics in category economics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6687872409820557, Recall: 0.6676023006439209, F1: 0.667985200881958\n","Expected Calibration Error (ECE): 0.050000000000000044\n","high_school_microeconomics ECE: 0.050000000000000044\n","Evaluating computer_security in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6233068704605103, Recall: 0.6450125575065613, F1: 0.6291664838790894\n","Expected Calibration Error (ECE): 0.050000000000000044\n","computer_security ECE: 0.050000000000000044\n","Evaluating high_school_european_history in category history...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.911251962184906, Recall: 0.9311766028404236, F1: 0.9210892915725708\n","Expected Calibration Error (ECE): 0.050000000000000044\n","high_school_european_history ECE: 0.050000000000000044\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["   "]},{"cell_type":"markdown","metadata":{},"source":["## Few-shot prompting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
