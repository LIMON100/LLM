{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-06T11:38:54.041017Z","iopub.status.busy":"2024-06-06T11:38:54.040740Z","iopub.status.idle":"2024-06-06T11:39:14.252974Z","shell.execute_reply":"2024-06-06T11:39:14.251729Z","shell.execute_reply.started":"2024-06-06T11:38:54.040994Z"},"trusted":true},"outputs":[],"source":["!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:39:14.255709Z","iopub.status.busy":"2024-06-06T11:39:14.255311Z","iopub.status.idle":"2024-06-06T11:39:14.559891Z","shell.execute_reply":"2024-06-06T11:39:14.558922Z","shell.execute_reply.started":"2024-06-06T11:39:14.255657Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19e22fe465214ed6bee539819cf9a905","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:39:34.604330Z","iopub.status.busy":"2024-06-06T11:39:34.603459Z","iopub.status.idle":"2024-06-06T11:40:12.584026Z","shell.execute_reply":"2024-06-06T11:40:12.582794Z","shell.execute_reply.started":"2024-06-06T11:39:34.604291Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:40:12.586639Z","iopub.status.busy":"2024-06-06T11:40:12.586326Z","iopub.status.idle":"2024-06-06T11:40:16.875538Z","shell.execute_reply":"2024-06-06T11:40:16.874680Z","shell.execute_reply.started":"2024-06-06T11:40:12.586610Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-06 11:40:13--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\n","Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n","Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 166184960 (158M) [application/x-tar]\n","Saving to: 'data.tar'\n","\n","data.tar            100%[===================>] 158.49M  55.9MB/s    in 2.8s    \n","\n","2024-06-06 11:40:16 (55.9 MB/s) - 'data.tar' saved [166184960/166184960]\n","\n"]}],"source":["!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:40:16.877131Z","iopub.status.busy":"2024-06-06T11:40:16.876865Z","iopub.status.idle":"2024-06-06T11:40:17.124496Z","shell.execute_reply":"2024-06-06T11:40:17.123295Z","shell.execute_reply.started":"2024-06-06T11:40:16.877105Z"},"trusted":true},"outputs":[],"source":["import tarfile\n","unzip_path = '.'\n","tar = tarfile.open('data.tar')\n","tar.extractall(path=unzip_path)\n","tar.close()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:40:17.128628Z","iopub.status.busy":"2024-06-06T11:40:17.127941Z","iopub.status.idle":"2024-06-06T11:41:07.945370Z","shell.execute_reply":"2024-06-06T11:41:07.944199Z","shell.execute_reply.started":"2024-06-06T11:40:17.128585Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting groq\n","  Downloading groq-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n","Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: groq\n","Successfully installed groq-0.8.0\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\n","Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m864.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m--:--\u001b[0m\n","\u001b[?25hInstalling collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install groq\n","!pip install bert_score"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:07.947284Z","iopub.status.busy":"2024-06-06T11:41:07.946965Z","iopub.status.idle":"2024-06-06T11:41:28.859825Z","shell.execute_reply":"2024-06-06T11:41:28.859008Z","shell.execute_reply.started":"2024-06-06T11:41:07.947254Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-06 11:41:15.136296: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-06 11:41:15.136400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-06 11:41:15.300479: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","import numpy as np\n","from sklearn.isotonic import IsotonicRegression\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from transformers import pipeline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pathlib\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","from bert_score import score\n","import os\n","from groq import Groq\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","import json\n","import re\n","import concurrent.futures\n","from tqdm import tqdm\n","from yaml import safe_load"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:28.861475Z","iopub.status.busy":"2024-06-06T11:41:28.860900Z","iopub.status.idle":"2024-06-06T11:41:29.039229Z","shell.execute_reply":"2024-06-06T11:41:29.038223Z","shell.execute_reply.started":"2024-06-06T11:41:28.861449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:29.041107Z","iopub.status.busy":"2024-06-06T11:41:29.040799Z","iopub.status.idle":"2024-06-06T11:41:29.045747Z","shell.execute_reply":"2024-06-06T11:41:29.044645Z","shell.execute_reply.started":"2024-06-06T11:41:29.041080Z"},"trusted":true},"outputs":[],"source":["# # Define models and tokenizers\n","# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n","# tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.float16\n","# )\n","\n","# mistral_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:29.047388Z","iopub.status.busy":"2024-06-06T11:41:29.047008Z","iopub.status.idle":"2024-06-06T11:41:29.061059Z","shell.execute_reply":"2024-06-06T11:41:29.060189Z","shell.execute_reply.started":"2024-06-06T11:41:29.047349Z"},"trusted":true},"outputs":[],"source":["# model_name2 = 'stabilityai/StableBeluga-13B'\n","# tokenizer_beluga = AutoTokenizer.from_pretrained(model_name2)\n","# tokenizer_beluga.pad_token = tokenizer_beluga.eos_token\n","\n","# beluga_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name2,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:29.063972Z","iopub.status.busy":"2024-06-06T11:41:29.062200Z","iopub.status.idle":"2024-06-06T11:41:29.074157Z","shell.execute_reply":"2024-06-06T11:41:29.073246Z","shell.execute_reply.started":"2024-06-06T11:41:29.063944Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return ' '.join(filtered_tokens)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:29.077872Z","iopub.status.busy":"2024-06-06T11:41:29.077422Z","iopub.status.idle":"2024-06-06T11:41:29.089761Z","shell.execute_reply":"2024-06-06T11:41:29.088752Z","shell.execute_reply.started":"2024-06-06T11:41:29.077848Z"},"trusted":true},"outputs":[],"source":["# Utility functions\n","def load_questions(file_path):\n","    questions = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            questions.append(json.loads(line.strip()))\n","    return questions\n","\n","def load_model_answers(dir_path):\n","    model_answers = {}\n","    for file_name in os.listdir(dir_path):\n","        model_name = file_name.split('.')[0]\n","        model_answers[model_name] = {}\n","        with open(os.path.join(dir_path, file_name), 'r') as f:\n","            for line in f:\n","                answer = json.loads(line.strip())\n","                question_id = answer['question_id']\n","                model_answers[model_name][question_id] = answer\n","    return model_answers\n","\n","def get_endpoint(endpoint_config):\n","    # Implementation depends on the format of your API config file\n","    return endpoint_config\n","\n","def make_config(file_path):\n","    with open(file_path, 'r') as f:\n","        return safe_load(f)\n","\n","def chat_completion_groq(model, conv, temperature, max_tokens, api_key):\n","    client = Groq(api_key=api_key)\n","    chat_completion = client.chat.completions.create(\n","        messages=conv,\n","        model=model,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    return chat_completion.choices[0].message.content\n","\n","def get_score(judgment, pattern, pairwise=True):\n","    matches = pattern.findall(judgment)\n","    matches = [m for m in matches if m != \"\"]\n","    if len(set(matches)) == 0:\n","        return None, True\n","    elif len(set(matches)) == 1:\n","        if pairwise:\n","            return matches[0].strip(\"\\n\"), False\n","        return int(matches[0])\n","    else:\n","        return None, False"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:29.091173Z","iopub.status.busy":"2024-06-06T11:41:29.090841Z","iopub.status.idle":"2024-06-06T11:41:31.728016Z","shell.execute_reply":"2024-06-06T11:41:31.727141Z","shell.execute_reply.started":"2024-06-06T11:41:29.091140Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["**What is the meaning of life?**\n","\n","A question that has puzzled philosophers, theologians, and thinkers for centuries! The meaning of life is a complex and subjective concept that has been debated and explored in various ways. Here are some possible perspectives:\n","\n","1. **Existentialism**: Life has no inherent meaning; we create our own purpose and meaning through our experiences, choices, and relationships.\n","2. **Religious or spiritual**: Life's meaning is derived from a higher power, divine plan, or spiritual pursuit, providing a sense of purpose and direction.\n","3. **Humanism**: Life's meaning is found in human connections, personal growth, and the pursuit of happiness, freedom, and fulfillment.\n","4. **Biological**: Life's meaning is simply to survive, reproduce, and ensure the continuation of the species.\n","5. **Philosophical**: Life's meaning is a quest for knowledge, wisdom, and self-awareness, often involving the pursuit of truth, beauty, and goodness.\n","\n","Ultimately, the meaning of life is a deeply personal and subjective question, and each individual's answer may vary.\n","\n","**How modern AI is affecting real life:**\n","\n","Artificial Intelligence (AI) has already started transforming various aspects of our lives, with both positive and negative consequences:\n","\n","**Positive impacts:**\n","\n","1. **Automation**: AI is increasing efficiency, reducing costs, and freeing humans from repetitive tasks in industries like manufacturing, healthcare, and finance.\n","2. **Personalization**: AI-driven recommendation systems are improving customer experiences in e-commerce, entertainment, and education.\n","3. **Healthcare**: AI is aiding diagnosis, predicting patient outcomes, and streamlining clinical workflows.\n","4. **Transportation**: AI is enhancing autonomous vehicles, traffic management, and route optimization.\n","\n","**Negative impacts:**\n","\n","1. **Job displacement**: AI automation may lead to significant job losses, especially in industries with repetitive tasks.\n","2. **Biased decision-making**: AI systems can perpetuate human biases, leading to unfair outcomes in areas like lending, hiring, and criminal justice.\n","3. **Privacy concerns**: AI's reliance on vast amounts of personal data raises concerns about surveillance, data protection, and individual privacy.\n","4. **Dependence on technology**: Over-reliance on AI may lead to a loss of critical thinking skills, emotional intelligence, and human connection.\n","\n","**Future AI outcomes:**\n","\n","As AI continues to evolve, we can expect:\n","\n","1. **Increased autonomy**: AI systems will become more autonomous, making decisions with minimal human intervention.\n","2. **Widespread adoption**: AI will be integrated into various industries, revolutionizing sectors like education, energy, and agriculture.\n","3. **Human-AI collaboration**: AI will augment human capabilities, enabling more efficient and effective collaboration.\n","4. **AI-driven governance**: AI may play a significant role in policy-making, law enforcement, and social governance.\n","\n","However, we must also be cautious about the potential risks and challenges associated with AI, such as:\n","\n","1. **AI-driven inequality**: The benefits of AI may not be evenly distributed, exacerbating existing social and economic inequalities.\n","2. **AI safety and security**: The increasing reliance on AI will create new vulnerabilities, vulnerable to cyber threats and potential misuse.\n","3. **Lack of transparency and accountability**: AI decision-making processes may be opaque, making it difficult to identify and rectify biases and errors.\n","\n","Ultimately, the future of AI will be shaped by our collective efforts to develop and deploy AI responsibly, ensuring that its benefits are shared by all, while minimizing its risks and negative consequences.\n"]}],"source":["client = Groq(api_key=\"YOUR_API\")\n","\n","chat_completion = client.chat.completions.create(\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n","        }\n","\n","    ],\n","    model=\"llama3-70b-8192\",\n",")\n","\n","print(chat_completion.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:12.364018Z","iopub.status.busy":"2024-06-05T09:38:12.363622Z","iopub.status.idle":"2024-06-05T09:38:12.391914Z","shell.execute_reply":"2024-06-05T09:38:12.391081Z","shell.execute_reply.started":"2024-06-05T09:38:12.363988Z"},"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model, regex_pattern, temperature=0.8, max_tokens=300, pairwise=True):\n","        self.api_key = api_key\n","        self.judge_model = judge_model\n","        self.regex_pattern = re.compile(regex_pattern)\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","        self.pairwise = pairwise\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","    def get_mistral_answer(self, prompt, include_stopwords=True):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n","            do_sample=True, output_scores=True, return_dict_in_generate=True\n","        )\n","\n","        response_ids = outputs.sequences[0]\n","        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n","\n","        logits = outputs.scores  # Logits of the generated tokens\n","        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n","\n","        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n","        if include_stopwords:\n","            token_confidences = [prob.max().item() for prob in softmax_probs]\n","        else:\n","            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n","\n","        mean_confidence = np.mean(token_confidences)\n","\n","        return response_text, mean_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=\"llama3-70b-8192\",\n","        )\n","\n","        llama3_response = chat_completion.choices[0].message.content.strip()\n","        return llama3_response\n","\n","    def parse_evaluation(self, evaluation):\n","        if \"1 point\" in evaluation:\n","            return 1\n","        return 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n","        )\n","\n","        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate(self, prompts):\n","        self.prompts = prompts  # Ensure prompts are stored\n","        for prompt in prompts:\n","            response, confidence = self.get_mistral_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies\n","\n","    def plot_reliability_diagram(self, bins, bin_accuracies):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot((bins[:-1] + bins[1:]) / 2, bin_accuracies, marker='o', label='Accuracy per bin')\n","        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n","        plt.xlabel('Confidence')\n","        plt.ylabel('Accuracy')\n","        plt.title('Reliability Diagram')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:13.025882Z","iopub.status.busy":"2024-06-05T09:38:13.025298Z","iopub.status.idle":"2024-06-05T09:40:08.802455Z","shell.execute_reply":"2024-06-05T09:40:08.801451Z","shell.execute_reply.started":"2024-06-05T09:38:13.025852Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a215bc76f664495b445209290af45c4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"948c2410474e47c5bc7f1ef6fe11cf73","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"735945e2f09f49eb8cd9d79aa6f4e619","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98b40942285743e39e9b3f246c6a9263","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0accba5fde9b43bb988a8b83d6b1f13e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6630645990371704, Recall: 0.6872342228889465, F1: 0.6748167872428894\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\", regex_pattern=r\"(correct|incorrect)\")\n","prompts = [\"What is nuclear fusion?\", \"Tell me about the current AI situation in the world?\", \"How does a combustion engine work?\"]\n","pipeline.evaluate(prompts)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:58:26.596665Z","iopub.status.busy":"2024-06-05T09:58:26.595910Z","iopub.status.idle":"2024-06-05T09:58:26.607206Z","shell.execute_reply":"2024-06-05T09:58:26.606213Z","shell.execute_reply.started":"2024-06-05T09:58:26.596631Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected Calibration Error (ECE): 0.050000000000000044\n"]}],"source":["data, bins, bin_accuracies = pipeline.calculate_ece()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:40:27.233246Z","iopub.status.busy":"2024-06-05T09:40:27.232603Z","iopub.status.idle":"2024-06-05T09:40:27.237806Z","shell.execute_reply":"2024-06-05T09:40:27.236833Z","shell.execute_reply.started":"2024-06-05T09:40:27.233215Z"},"trusted":true},"outputs":[],"source":["few_shot_examples = \"\"\"\n","Q: What is nuclear fusion?\n","A: Nuclear fusion is a reaction in which two atomic nuclei combine to form a heavier nucleus, releasing energy in the process.\n","\n","Q: How does a combustion engine work?\n","A: A combustion engine works by burning fuel in a combustion chamber to produce mechanical energy that drives the engine.\n","\"\"\"\n","\n","def get_mistral_answer_few_shot(prompt):\n","    few_shot_prompt = few_shot_examples + f\"\\nQ: {prompt}\\nA:\"\n","    response, confidence = get_mistral_answer(few_shot_prompt)\n","    return response, confidence\n","\n","# Example usage:\n","few_shot_response, few_shot_confidence = get_mistral_answer_few_shot(\"Tell me about the current AI situation in the world.\")\n","print(f\"Few-shot response: {few_shot_response}\")\n","print(f\"Few-shot confidence: {few_shot_confidence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Class pipeline for mmlu dataset"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:50.671386Z","iopub.status.busy":"2024-06-06T11:41:50.671011Z","iopub.status.idle":"2024-06-06T11:41:50.678208Z","shell.execute_reply":"2024-06-06T11:41:50.677145Z","shell.execute_reply.started":"2024-06-06T11:41:50.671356Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, model_name):\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","    def encode(self, text, max_length=1024):\n","        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n","\n","    def decode(self, tokens):\n","        return self.tokenizer.decode(tokens, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:51.027820Z","iopub.status.busy":"2024-06-06T11:41:51.027445Z","iopub.status.idle":"2024-06-06T11:41:51.067201Z","shell.execute_reply":"2024-06-06T11:41:51.066080Z","shell.execute_reply.started":"2024-06-06T11:41:51.027788Z"},"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","\n","        self.tokenizer = Tokenizer(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def get_model_answer(self, prompt):\n","        \"\"\"Generates an answer from the specified model.\"\"\"\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_length=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0])\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0])\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T11:41:54.129263Z","iopub.status.busy":"2024-06-06T11:41:54.128880Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af29fbb27b549c59393ccad1a986e4f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a059176ab64b4488b95e95535c45344a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a9ef5b9817a44b4a84726774f44d9aa","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0da77aa897646a0a27efba0b835dcc8","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09c9686105cc4d0fad5d9163e3e56a6e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e09ebdf0785440dda7704109efa6cc4d","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"597c80c9a7ef470abe0fbbc2eca78dfd","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3c19e8b80d9437099154ef04f46afad","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"188d1c60956a4430a86ef04fa5531e37","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53728df0909f4a739350c0c89de32fce","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"839011d44fe64e47a175b736c67b5b1a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7227051c16db40b39c4409914b0c81a1","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating high_school_psychology in category psychology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c574ac60dc654b73a35e853a4f0a840f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e287811800e4ab19c98a791d2927cf6","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dee75edc7754675b305f098eeed2694","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"051699cf9cbf45d6b0745cdcca42f3e5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ae1d748027243c7ba0fc7f4fe63a325","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7571039199829102, Recall: 0.7485461235046387, F1: 0.7515882849693298\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_psychology ECE: 0.19999999999999996\n","Evaluating computer_security in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6872113943099976, Recall: 0.6256282329559326, F1: 0.653347909450531\n","Expected Calibration Error (ECE): 0.07500000000000001\n","computer_security ECE: 0.07500000000000001\n","Evaluating moral_scenarios in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
