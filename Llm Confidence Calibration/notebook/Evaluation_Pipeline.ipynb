{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-05T11:05:24.497843Z","iopub.execute_input":"2024-06-05T11:05:24.498654Z","iopub.status.idle":"2024-06-05T11:05:47.887601Z","shell.execute_reply.started":"2024-06-05T11:05:24.498618Z","shell.execute_reply":"2024-06-05T11:05:47.886086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T11:05:47.889917Z","iopub.execute_input":"2024-06-05T11:05:47.890309Z","iopub.status.idle":"2024-06-05T11:05:48.308209Z","shell.execute_reply.started":"2024-06-05T11:05:47.890270Z","shell.execute_reply":"2024-06-05T11:05:48.307154Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c55bd69080e4ee884f42944cd9f61e4"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade transformers -qq\n!pip install accelerate\n!pip install -q -U google-generativeai","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:33:05.374758Z","iopub.execute_input":"2024-06-05T09:33:05.375161Z","iopub.status.idle":"2024-06-05T09:33:44.194784Z","shell.execute_reply.started":"2024-06-05T09:33:05.375129Z","shell.execute_reply":"2024-06-05T09:33:44.193476Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:33:44.197242Z","iopub.execute_input":"2024-06-05T09:33:44.198087Z","iopub.status.idle":"2024-06-05T09:33:48.046593Z","shell.execute_reply.started":"2024-06-05T09:33:44.198016Z","shell.execute_reply":"2024-06-05T09:33:48.045706Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-06-05 09:33:45--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\nResolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\nConnecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 166184960 (158M) [application/x-tar]\nSaving to: 'data.tar'\n\ndata.tar            100%[===================>] 158.49M  59.5MB/s    in 2.7s    \n\n2024-06-05 09:33:47 (59.5 MB/s) - 'data.tar' saved [166184960/166184960]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import tarfile\nunzip_path = '.'\ntar = tarfile.open('data.tar')\ntar.extractall(path=unzip_path)\ntar.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:33:48.047910Z","iopub.execute_input":"2024-06-05T09:33:48.048233Z","iopub.status.idle":"2024-06-05T09:33:48.297373Z","shell.execute_reply.started":"2024-06-05T09:33:48.048202Z","shell.execute_reply":"2024-06-05T09:33:48.296609Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers -qq\n!pip install accelerate\n!pip install groq\n!pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:33:48.299805Z","iopub.execute_input":"2024-06-05T09:33:48.300133Z","iopub.status.idle":"2024-06-05T09:34:40.316908Z","shell.execute_reply.started":"2024-06-05T09:33:48.300108Z","shell.execute_reply":"2024-06-05T09:34:40.315707Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting groq\n  Downloading groq-0.8.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\nDownloading groq-0.8.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: groq\nSuccessfully installed groq-0.8.0\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.isotonic import IsotonicRegression\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\nfrom transformers import BitsAndBytesConfig\nimport torch.nn.functional as F\nfrom transformers import pipeline\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pathlib\nimport textwrap\nimport google.generativeai as genai\nfrom IPython.display import display\nfrom IPython.display import Markdown\nimport google.generativeai as genai\nfrom bert_score import score\nimport os\nfrom groq import Groq\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport json\nimport re\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom yaml import safe_load","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:34:40.318531Z","iopub.execute_input":"2024-06-05T09:34:40.318896Z","iopub.status.idle":"2024-06-05T09:35:00.740776Z","shell.execute_reply.started":"2024-06-05T09:34:40.318857Z","shell.execute_reply":"2024-06-05T09:35:00.739995Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-06-05 09:34:47.278646: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 09:34:47.278769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 09:34:47.441874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:35:00.741908Z","iopub.execute_input":"2024-06-05T09:35:00.742470Z","iopub.status.idle":"2024-06-05T09:35:00.910424Z","shell.execute_reply.started":"2024-06-05T09:35:00.742444Z","shell.execute_reply":"2024-06-05T09:35:00.909539Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define models and tokenizers\nmodel_name = 'mistralai/Mistral-7B-Instruct-v0.2'\ntokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\ntokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmistral_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:35:00.911658Z","iopub.execute_input":"2024-06-05T09:35:00.912005Z","iopub.status.idle":"2024-06-05T09:36:45.463029Z","shell.execute_reply.started":"2024-06-05T09:35:00.911973Z","shell.execute_reply":"2024-06-05T09:36:45.462096Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9bfeb5ecfe4f02b03323b127130e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a457366db08c4a30a7c02ba38fe3fb69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e703fbcd56041c5bd256836e00984bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b786d682bbb74fbd8fc0e29c789d8aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06fa731d287b4ff29c86e930a983ccd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d80319f6164f88a15f3e394e959eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b144d1fdc804adea2b2e920a34d6234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"971e7882d17c42cd94dccb8d820e9cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd6fa6519a684e6a82993a7e702e6933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443da628f47c49a7a46973ace18daa8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5564399e1f40a0a4075a0fdb66d313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb979a43eb54f6bb96074c597545fd6"}},"metadata":{}}]},{"cell_type":"code","source":"# model_name2 = 'stabilityai/StableBeluga-13B'\n# tokenizer_beluga = AutoTokenizer.from_pretrained(model_name2)\n# tokenizer_beluga.pad_token = tokenizer_beluga.eos_token\n\n# beluga_model = AutoModelForCausalLM.from_pretrained(\n#     model_name2,\n#     torch_dtype=torch.float16,\n#     quantization_config=bnb_config,\n#     low_cpu_mem_usage=True,\n#     device_map=\"auto\",\n# )","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:36:45.464789Z","iopub.execute_input":"2024-06-05T09:36:45.465293Z","iopub.status.idle":"2024-06-05T09:36:45.470113Z","shell.execute_reply.started":"2024-06-05T09:36:45.465259Z","shell.execute_reply":"2024-06-05T09:36:45.469165Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n    return ' '.join(filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:36:45.471675Z","iopub.execute_input":"2024-06-05T09:36:45.471972Z","iopub.status.idle":"2024-06-05T09:36:45.480944Z","shell.execute_reply.started":"2024-06-05T09:36:45.471922Z","shell.execute_reply":"2024-06-05T09:36:45.480004Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Utility functions\ndef load_questions(file_path):\n    questions = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            questions.append(json.loads(line.strip()))\n    return questions\n\ndef load_model_answers(dir_path):\n    model_answers = {}\n    for file_name in os.listdir(dir_path):\n        model_name = file_name.split('.')[0]\n        model_answers[model_name] = {}\n        with open(os.path.join(dir_path, file_name), 'r') as f:\n            for line in f:\n                answer = json.loads(line.strip())\n                question_id = answer['question_id']\n                model_answers[model_name][question_id] = answer\n    return model_answers\n\ndef get_endpoint(endpoint_config):\n    # Implementation depends on the format of your API config file\n    return endpoint_config\n\ndef make_config(file_path):\n    with open(file_path, 'r') as f:\n        return safe_load(f)\n\ndef chat_completion_groq(model, conv, temperature, max_tokens, api_key):\n    client = Groq(api_key=api_key)\n    chat_completion = client.chat.completions.create(\n        messages=conv,\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    return chat_completion.choices[0].message.content\n\ndef get_score(judgment, pattern, pairwise=True):\n    matches = pattern.findall(judgment)\n    matches = [m for m in matches if m != \"\"]\n    if len(set(matches)) == 0:\n        return None, True\n    elif len(set(matches)) == 1:\n        if pairwise:\n            return matches[0].strip(\"\\n\"), False\n        return int(matches[0])\n    else:\n        return None, False","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:36:45.483928Z","iopub.execute_input":"2024-06-05T09:36:45.484272Z","iopub.status.idle":"2024-06-05T09:36:45.497345Z","shell.execute_reply.started":"2024-06-05T09:36:45.484248Z","shell.execute_reply":"2024-06-05T09:36:45.496406Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"client = Groq(api_key=\"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\")\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n        }\n\n    ],\n    model=\"llama3-70b-8192\",\n)\n\nprint(chat_completion.choices[0].message.content)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:38:07.945081Z","iopub.execute_input":"2024-06-05T09:38:07.945782Z","iopub.status.idle":"2024-06-05T09:38:10.547813Z","shell.execute_reply.started":"2024-06-05T09:38:07.945752Z","shell.execute_reply":"2024-06-05T09:38:10.546851Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"What a profound and complex set of questions! Let's dive in and explore each aspect:\n\n**The Meaning of Life:**\n\nPhilosophers, theologians, and scientists have debated the meaning of life for centuries. There is no one definitive answer, as it's a deeply personal and subjective inquiry. Here are a few perspectives:\n\n1. **Biological purpose:** From a biological standpoint, the meaning of life can be seen as survival and propagation of one's genes.\n2. **Evolutionary purpose:** In the context of evolution, the meaning of life could be to adapt, evolve, and ensure the survival of the species.\n3. **Philosophical perspectives:** Philosophers like Aristotle, Kant, and Nietzsche offered their own takes on the meaning of life, ranging from seeking happiness, living a virtuous life, to embracing life's absurdity and creating one's own meaning.\n4. **Existentialism:** This philosophical movement emphasizes individual freedom and creativity, suggesting that each person must create their own purpose and meaning in life.\n5. **Spiritual and religious beliefs:** Many faiths and spiritual traditions offer their own answers to the question of life's meaning, often tied to a higher power, afterlife, or spiritual realm.\n\n**Modern AI's Impact on Real Life:**\n\nArtificial Intelligence (AI) is transforming various aspects of our lives, with both positive and negative consequences:\n\n**Positive impacts:**\n\n1. **Efficiency and automation:** AI is streamlining processes, increasing productivity, and freeing up human resources for more strategic and creative tasks.\n2. **Healthcare and medicine:** AI is improving diagnosis accuracy, facilitating medical research, and enhancing patient care.\n3. **Personalization and convenience:** AI-powered systems are tailoring experiences, such as personalized recommendations and virtual assistants, to make our lives more convenient.\n\n**Negative impacts:**\n\n1. **Job displacement:** AI automation is replacing human workers in certain industries, sparking concerns about job security and the future of work.\n2. **Bias and discrimination:** AI systems can perpetuate and amplify existing biases, leading to unfair outcomes and discriminatory practices.\n3. **Privacy and security:** AI's reliance on vast amounts of data raises concerns about data privacy, security, and potential misuse.\n\n**Future AI Outcomes:**\n\nAs AI continues to evolve, we can expect:\n\n1. **Increased adoption:** AI will become more pervasive across industries, transforming the way we live and work.\n2. **Advancements in AI algorithms:** Researchers will continue to improve AI's capabilities, enabling more accurate decision-making and human-AI collaboration.\n3. **Rise of Explainable AI (XAI):** As AI becomes more ubiquitous, there will be a growing need for transparent and interpretable AI systems that can explain their decisions.\n4. **Job market shifts:** While AI may replace certain jobs, it will also create new opportunities in fields like AI development, deployment, and ethics.\n5. **AI-driven societal changes:** AI will continue to influence how we interact with each other, with potential implications for education, healthcare, and social structures.\n\nThe future of AI is uncertain, and it's crucial that we, as a society, engage in ethical discussions and responsible development of AI to ensure its benefits are equitably distributed and its risks are mitigated.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"class EvaluationPipeline:\n    def __init__(self, api_key, judge_model, regex_pattern, temperature=0.8, max_tokens=300, pairwise=True):\n        self.api_key = api_key\n        self.judge_model = judge_model\n        self.regex_pattern = re.compile(regex_pattern)\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.pairwise = pairwise\n        self.prompts = []\n        self.responses = []\n        self.references = []\n        self.confidences = []\n        self.accuracies = []\n\n    def get_mistral_answer(self, prompt, include_stopwords=True):\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n            do_sample=True, output_scores=True, return_dict_in_generate=True\n        )\n\n        response_ids = outputs.sequences[0]\n        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n\n        logits = outputs.scores  # Logits of the generated tokens\n        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n\n        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n        if include_stopwords:\n            token_confidences = [prob.max().item() for prob in softmax_probs]\n        else:\n            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n\n        mean_confidence = np.mean(token_confidences)\n\n        return response_text, mean_confidence\n\n    def judge_answer(self, response, prompt):\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Review the user’s question and the corresponding response using the binary scoring system described below.\n                        - 0 points: The response is incorrect or does not address the user’s question.\n                        - 1 point: The response is correct and addresses the user’s question.\n\n                        User: {prompt}\n                        Response: {response}\n                        \"\"\"\n                }\n            ],\n            model=\"llama3-70b-8192\",\n        )\n\n        llama3_response = chat_completion.choices[0].message.content.strip()\n        return llama3_response\n\n    def parse_evaluation(self, evaluation):\n        if \"1 point\" in evaluation:\n            return 1\n        return 0\n\n    def generate_reference_text(self, prompt):\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n        )\n\n        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n        return reference_text\n\n    def calculate_bertscore(self, references, candidates):\n        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n        return P.mean().item(), R.mean().item(), F1.mean().item()\n\n    def evaluate(self, prompts):\n        self.prompts = prompts  # Ensure prompts are stored\n        for prompt in prompts:\n            response, confidence = self.get_mistral_answer(prompt)\n            self.responses.append(response)\n            self.confidences.append(confidence)\n            reference_text = self.generate_reference_text(prompt)\n            self.references.append(reference_text)\n            judgement = self.judge_answer(response, prompt)\n            accuracy = self.parse_evaluation(judgement)\n            self.accuracies.append(accuracy)\n\n        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n\n    def calculate_ece(self):\n        # Ensure all lists have the same length\n        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n            data = pd.DataFrame({\n                'prompt': self.prompts,\n                'response': self.responses,\n                'confidence': self.confidences,\n                'rating': self.accuracies\n            })\n        else:\n            raise ValueError(\"All arrays must be of the same length\")\n\n        # Normalize confidence scores\n        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n\n        # Bin the normalized confidence scores\n        bins = np.linspace(0, 1, 11)\n        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n\n        # Calculate accuracy for each bin\n        bin_accuracies = data.groupby('bin')['rating'].mean()\n        bin_proportions = data['bin'].value_counts(normalize=True)\n\n        # Drop bins with NaN values\n        valid_bins = bin_accuracies.dropna().index\n        bin_accuracies = bin_accuracies[valid_bins]\n        bin_proportions = bin_proportions[valid_bins]\n        bin_confidences = (bins[:-1] + bins[1:]) / 2\n        bin_confidences = bin_confidences[valid_bins]\n\n        # Ensure lengths match\n        bin_confidences = bin_confidences[:len(bin_accuracies)]\n\n        # Compute ECE\n        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n        print(f\"Expected Calibration Error (ECE): {ece}\")\n\n        return data, bins, bin_accuracies\n\n    def plot_reliability_diagram(self, bins, bin_accuracies):\n        plt.figure(figsize=(10, 6))\n        plt.plot((bins[:-1] + bins[1:]) / 2, bin_accuracies, marker='o', label='Accuracy per bin')\n        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n        plt.xlabel('Confidence')\n        plt.ylabel('Accuracy')\n        plt.title('Reliability Diagram')\n        plt.legend()\n        plt.grid(True)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:38:12.363622Z","iopub.execute_input":"2024-06-05T09:38:12.364018Z","iopub.status.idle":"2024-06-05T09:38:12.391914Z","shell.execute_reply.started":"2024-06-05T09:38:12.363988Z","shell.execute_reply":"2024-06-05T09:38:12.391081Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\", regex_pattern=r\"(correct|incorrect)\")\nprompts = [\"What is nuclear fusion?\", \"Tell me about the current AI situation in the world?\", \"How does a combustion engine work?\"]\npipeline.evaluate(prompts)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:38:13.025298Z","iopub.execute_input":"2024-06-05T09:38:13.025882Z","iopub.status.idle":"2024-06-05T09:40:08.802455Z","shell.execute_reply.started":"2024-06-05T09:38:13.025852Z","shell.execute_reply":"2024-06-05T09:40:08.801451Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a215bc76f664495b445209290af45c4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948c2410474e47c5bc7f1ef6fe11cf73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"735945e2f09f49eb8cd9d79aa6f4e619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b40942285743e39e9b3f246c6a9263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0accba5fde9b43bb988a8b83d6b1f13e"}},"metadata":{}},{"name":"stdout","text":"BERTScore - Precision: 0.6630645990371704, Recall: 0.6872342228889465, F1: 0.6748167872428894\n","output_type":"stream"}]},{"cell_type":"code","source":"data, bins, bin_accuracies = pipeline.calculate_ece()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:58:26.595910Z","iopub.execute_input":"2024-06-05T09:58:26.596665Z","iopub.status.idle":"2024-06-05T09:58:26.607206Z","shell.execute_reply.started":"2024-06-05T09:58:26.596631Z","shell.execute_reply":"2024-06-05T09:58:26.606213Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Expected Calibration Error (ECE): 0.050000000000000044\n","output_type":"stream"}]},{"cell_type":"code","source":"few_shot_examples = \"\"\"\nQ: What is nuclear fusion?\nA: Nuclear fusion is a reaction in which two atomic nuclei combine to form a heavier nucleus, releasing energy in the process.\n\nQ: How does a combustion engine work?\nA: A combustion engine works by burning fuel in a combustion chamber to produce mechanical energy that drives the engine.\n\"\"\"\n\ndef get_mistral_answer_few_shot(prompt):\n    few_shot_prompt = few_shot_examples + f\"\\nQ: {prompt}\\nA:\"\n    response, confidence = get_mistral_answer(few_shot_prompt)\n    return response, confidence\n\n# Example usage:\nfew_shot_response, few_shot_confidence = get_mistral_answer_few_shot(\"Tell me about the current AI situation in the world.\")\nprint(f\"Few-shot response: {few_shot_response}\")\nprint(f\"Few-shot confidence: {few_shot_confidence}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T09:40:27.232603Z","iopub.execute_input":"2024-06-05T09:40:27.233246Z","iopub.status.idle":"2024-06-05T09:40:27.237806Z","shell.execute_reply.started":"2024-06-05T09:40:27.233215Z","shell.execute_reply":"2024-06-05T09:40:27.236833Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class pipeline for mmlu dataset","metadata":{}},{"cell_type":"code","source":"import re\n\nclass EvaluationPipeline:\n    def __init__(self, api_key, judge_model, temperature=0.8, max_tokens=300, pairwise=True):\n        self.api_key = api_key\n        self.judge_model = judge_model\n#         self.regex_pattern = re.compile(regex_pattern)\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.pairwise = pairwise\n\n    def get_mistral_answer(self, prompt, include_stopwords=True):\n        tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n        tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n            do_sample=True, output_scores=True, return_dict_in_generate=True\n        )\n\n        response_ids = outputs.sequences[0]\n        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n\n        logits = outputs.scores  # Logits of the generated tokens\n        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n\n        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n        if include_stopwords:\n            token_confidences = [prob.max().item() for prob in softmax_probs]\n        else:\n            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n\n        mean_confidence = np.mean(token_confidences)\n\n        return response_text, mean_confidence\n\n    def judge_answer(self, response, prompt):\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Review the user’s question and the corresponding response using the binary scoring system described below.\n                        - 0 points: The response is incorrect or does not address the user’s question.\n                        - 1 point: The response is correct and addresses the user’s question.\n\n                        User: {prompt}\n                        Response: {response}\n                        \"\"\"\n                }\n            ],\n            model=\"llama3-70b-8192\",\n        )\n\n        llama3_response = chat_completion.choices[0].message.content.strip()\n        return llama3_response\n\n    def parse_evaluation(self, evaluation):\n        if \"1 point\" in evaluation:\n            return 1\n        return 0\n\n    def generate_reference_text(self, prompt):\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n        )\n\n        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n        return reference_text\n\n    def calculate_bertscore(self, references, candidates):\n        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n        return P.mean().item(), R.mean().item(), F1.mean().item()\n\n\n    def evaluate_from_csv(self, csv_file_path):\n        # Clear data before processing each file\n        self.prompts = []\n        self.responses = []\n        self.references = []\n        self.confidences = []\n        self.accuracies = []\n\n        df = pd.read_csv(csv_file_path)\n        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n\n        for prompt in self.prompts:\n            response, confidence = self.get_mistral_answer(prompt)\n            self.responses.append(response)\n            self.confidences.append(confidence)\n            reference_text = self.generate_reference_text(prompt)\n            self.references.append(reference_text)\n            judgement = self.judge_answer(response, prompt)\n            accuracy = self.parse_evaluation(judgement)\n            self.accuracies.append(accuracy)\n\n        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n\n        data, bins, bin_accuracies = self.calculate_ece()\n        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n        return data, bins, bin_accuracies\n\n    def evaluate_folder(self, folder_path):\n        results = {}  # Store ECE values for each class\n        for filename in os.listdir(folder_path):\n            if filename.endswith(\".csv\"):\n                filepath = os.path.join(folder_path, filename)\n                class_name = filename[:-4]  # Extract class name from filename\n                print(f\"Evaluating {class_name}...\")\n                data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n\n                # Align shapes and calculate ECE\n                bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n                valid_bins = bin_accuracies.dropna().index  # Bins with data\n                bin_accuracies = bin_accuracies[valid_bins]\n                bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins] # Get proportions for valid bins\n                bin_confidences = bin_confidences[valid_bins] # Select confidences for valid bins\n\n                ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n\n                results[class_name] = ece\n                print(f\"{class_name} ECE: {ece}\")\n\n        # Save results to CSV\n        results_df = pd.DataFrame({'Class': results.keys(), 'ECE': results.values()})\n        results_df.to_csv(\"results.csv\", index=False)\n\n\n    def calculate_ece(self):\n        # Ensure all lists have the same length\n        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n            data = pd.DataFrame({\n                'prompt': self.prompts,\n                'response': self.responses,\n                'confidence': self.confidences,\n                'rating': self.accuracies\n            })\n        else:\n            raise ValueError(\"All arrays must be of the same length\")\n\n        # Normalize confidence scores\n        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n\n        # Bin the normalized confidence scores\n        bins = np.linspace(0, 1, 11)\n        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n\n        # Calculate accuracy for each bin\n        bin_accuracies = data.groupby('bin')['rating'].mean()\n        bin_proportions = data['bin'].value_counts(normalize=True)\n\n        # Drop bins with NaN values\n        valid_bins = bin_accuracies.dropna().index\n        bin_accuracies = bin_accuracies[valid_bins]\n        bin_proportions = bin_proportions[valid_bins]\n        bin_confidences = (bins[:-1] + bins[1:]) / 2\n        bin_confidences = bin_confidences[valid_bins]\n\n        # Ensure lengths match\n        bin_confidences = bin_confidences[:len(bin_accuracies)]\n\n        # Compute ECE\n        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n        print(f\"Expected Calibration Error (ECE): {ece}\")\n\n        return data, bins, bin_accuracies","metadata":{"execution":{"iopub.status.busy":"2024-06-05T10:08:23.969167Z","iopub.execute_input":"2024-06-05T10:08:23.969690Z","iopub.status.idle":"2024-06-05T10:08:24.001236Z","shell.execute_reply.started":"2024-06-05T10:08:23.969658Z","shell.execute_reply":"2024-06-05T10:08:24.000235Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\")\npipeline.evaluate_folder(\"/kaggle/working/data/dev\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T10:10:57.021697Z","iopub.execute_input":"2024-06-05T10:10:57.022115Z","iopub.status.idle":"2024-06-05T10:10:57.350023Z","shell.execute_reply.started":"2024-06-05T10:10:57.022059Z","shell.execute_reply":"2024-06-05T10:10:57.348345Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluationPipeline\u001b[49m(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, judge_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mevaluate_folder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/data/dev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'EvaluationPipeline' is not defined"],"ename":"NameError","evalue":"name 'EvaluationPipeline' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CLASSWISE","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom bert_score import score\n\nclass EvaluationPipeline:\n    def __init__(self, api_key, judge_model, temperature=0.8, max_tokens=300, pairwise=True):\n        self.api_key = api_key\n        self.judge_model = judge_model\n        # self.regex_pattern = re.compile(regex_pattern)\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.pairwise = pairwise\n\n        self.categories = {\n            \"abstract_algebra\": [\"math\"],\n            \"anatomy\": [\"health\"],\n            \"astronomy\": [\"physics\"],\n            \"business_ethics\": [\"business\"],\n            \"clinical_knowledge\": [\"health\"],\n            \"college_biology\": [\"biology\"],\n            \"college_chemistry\": [\"chemistry\"],\n            \"college_computer_science\": [\"computer science\"],\n            \"college_mathematics\": [\"math\"],\n            \"college_medicine\": [\"health\"],\n            \"college_physics\": [\"physics\"],\n            \"computer_security\": [\"computer science\"],\n            \"conceptual_physics\": [\"physics\"],\n            \"econometrics\": [\"economics\"],\n            \"electrical_engineering\": [\"engineering\"],\n            \"elementary_mathematics\": [\"math\"],\n            \"formal_logic\": [\"philosophy\"],\n            \"global_facts\": [\"other\"],\n            \"high_school_biology\": [\"biology\"],\n            \"high_school_chemistry\": [\"chemistry\"],\n            \"high_school_computer_science\": [\"computer science\"],\n            \"high_school_european_history\": [\"history\"],\n            \"high_school_geography\": [\"geography\"],\n            \"high_school_government_and_politics\": [\"politics\"],\n            \"high_school_macroeconomics\": [\"economics\"],\n            \"high_school_mathematics\": [\"math\"],\n            \"high_school_microeconomics\": [\"economics\"],\n            \"high_school_physics\": [\"physics\"],\n            \"high_school_psychology\": [\"psychology\"],\n            \"high_school_statistics\": [\"math\"],\n            \"high_school_us_history\": [\"history\"],\n            \"high_school_world_history\": [\"history\"],\n            \"human_aging\": [\"health\"],\n            \"human_sexuality\": [\"culture\"],\n            \"international_law\": [\"law\"],\n            \"jurisprudence\": [\"law\"],\n            \"logical_fallacies\": [\"philosophy\"],\n            \"machine_learning\": [\"computer science\"],\n            \"management\": [\"business\"],\n            \"marketing\": [\"business\"],\n            \"medical_genetics\": [\"health\"],\n            \"miscellaneous\": [\"other\"],\n            \"moral_disputes\": [\"philosophy\"],\n            \"moral_scenarios\": [\"philosophy\"],\n            \"nutrition\": [\"health\"],\n            \"philosophy\": [\"philosophy\"],\n            \"prehistory\": [\"history\"],\n            \"professional_accounting\": [\"other\"],\n            \"professional_law\": [\"law\"],\n            \"professional_medicine\": [\"health\"],\n            \"professional_psychology\": [\"psychology\"],\n            \"public_relations\": [\"politics\"],\n            \"security_studies\": [\"politics\"],\n            \"sociology\": [\"culture\"],\n            \"us_foreign_policy\": [\"politics\"],\n            \"virology\": [\"health\"],\n            \"world_religions\": [\"philosophy\"],\n        }\n    \n    def get_mistral_answer(self, prompt, include_stopwords=True):\n        tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n        tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n            do_sample=True, output_scores=True, return_dict_in_generate=True\n        )\n\n        response_ids = outputs.sequences[0]\n        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n\n        logits = outputs.scores  # Logits of the generated tokens\n        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n\n        tokens = tokenizer_mistral.convert_ids_to_tokens\n        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n        if include_stopwords:\n            token_confidences = [prob.max().item() for prob in softmax_probs]\n        else:\n            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n\n        mean_confidence = np.mean(token_confidences)\n\n        return response_text, mean_confidence\n\n    def judge_answer(self, response, prompt):\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Review the user’s question and the corresponding response using the binary scoring system described below.\n                        - 0 points: The response is incorrect or does not address the user’s question.\n                        - 1 point: The response is correct and addresses the user’s question.\n\n                        User: {prompt}\n                        Response: {response}\n                        \"\"\"\n                }\n            ],\n            model=self.judge_model,\n        )\n\n        llama3_response = chat_completion.choices[0].message.content.strip()\n        return llama3_response\n\n    def parse_evaluation(self, evaluation):\n        if \"1 point\" in evaluation:\n            return 1\n        return 0\n\n    def generate_reference_text(self, prompt):\n        inputs = tokenizer_mistral.encode_plus(\n            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n        ).to(mistral_model.device)\n\n        outputs = mistral_model.generate(\n            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n        )\n\n        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n        return reference_text\n\n    def calculate_bertscore(self, references, candidates):\n        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n        return P.mean().item(), R.mean().item(), F1.mean().item()\n\n    def evaluate_from_csv(self, csv_file_path):\n        # Clear data before processing each file\n        self.prompts = []\n        self.responses = []\n        self.references = []\n        self.confidences = []\n        self.accuracies = []\n\n        df = pd.read_csv(csv_file_path)\n        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n\n        for prompt in self.prompts:\n            response, confidence = self.get_mistral_answer(prompt)\n            self.responses.append(response)\n            self.confidences.append(confidence)\n            reference_text = self.generate_reference_text(prompt)\n            self.references.append(reference_text)\n            judgement = self.judge_answer(response, prompt)\n            accuracy = self.parse_evaluation(judgement)\n            self.accuracies.append(accuracy)\n\n        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n\n        data, bins, bin_accuracies = self.calculate_ece()\n        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n        return data, bins, bin_accuracies\n\n    def evaluate_folder(self, folder_path):\n        results = {}  # Store ECE values for each class\n        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n\n        for filename in os.listdir(folder_path):\n            if filename.endswith(\".csv\"):\n                filepath = os.path.join(folder_path, filename)\n                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n                \n                category_name = None\n                for key, value in self.categories.items():\n                    if key == class_name:\n                        category_name = value[0]\n                        break\n\n                if category_name:\n                    print(f\"Evaluating {class_name} in category {category_name}...\")\n                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n\n                    # Align shapes and calculate ECE\n                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n                    bin_accuracies = bin_accuracies[valid_bins]\n                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n\n                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n                    category_results[category_name].append(ece)\n                    print(f\"{class_name} ECE: {ece}\")\n\n        # Calculate average ECE for each category and save to CSV\n        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n        average_ece_df.to_csv(\"category_results.csv\", index=False)\n\n    def calculate_ece(self):\n        # Ensure all lists have the same length\n        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n            data = pd.DataFrame({\n                'prompt': self.prompts,\n                'response': self.responses,\n                'confidence': self.confidences,\n                'rating': self.accuracies\n            })\n        else:\n            raise ValueError(\"All arrays must be of the same length\")\n\n        # Normalize confidence scores\n        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n\n        # Bin the normalized confidence scores\n        bins = np.linspace(0, 1, 11)\n        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n\n        # Calculate accuracy for each bin\n        bin_accuracies = data.groupby('bin')['rating'].mean()\n        bin_proportions = data['bin'].value_counts(normalize=True)\n\n        # Drop bins with NaN values\n        valid_bins = bin_accuracies.dropna().index\n        bin_accuracies = bin_accuracies[valid_bins]\n        bin_proportions = bin_proportions[valid_bins]\n        bin_confidences = (bins[:-1] + bins[1:]) / 2\n        bin_confidences = bin_confidences[valid_bins]\n\n        # Ensure lengths match\n        bin_confidences = bin_confidences[:len(bin_accuracies)]\n\n        # Compute ECE\n        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n        print(f\"Expected Calibration Error (ECE): {ece}\")\n\n        return data, bins, bin_accuracies","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\")\npipeline.evaluate_folder(\"/content/drive/MyDrive/fellowshipai/Confidence_Calibration/demo_data/dev\")","metadata":{},"execution_count":null,"outputs":[]}]}