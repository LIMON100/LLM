{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-11T11:49:32.854048Z","iopub.status.busy":"2024-06-11T11:49:32.853731Z","iopub.status.idle":"2024-06-11T11:49:56.459203Z","shell.execute_reply":"2024-06-11T11:49:56.457967Z","shell.execute_reply.started":"2024-06-11T11:49:32.854023Z"},"trusted":true},"outputs":[],"source":["!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:54:26.731016Z","iopub.status.busy":"2024-06-11T11:54:26.730351Z","iopub.status.idle":"2024-06-11T11:54:26.754777Z","shell.execute_reply":"2024-06-11T11:54:26.753678Z","shell.execute_reply.started":"2024-06-11T11:54:26.730981Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0a5175c789240898e415ff2364e4c13","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:54:33.515856Z","iopub.status.busy":"2024-06-11T11:54:33.515146Z","iopub.status.idle":"2024-06-11T11:55:12.818571Z","shell.execute_reply":"2024-06-11T11:55:12.817044Z","shell.execute_reply.started":"2024-06-11T11:54:33.515824Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:55:12.821532Z","iopub.status.busy":"2024-06-11T11:55:12.821146Z","iopub.status.idle":"2024-06-11T11:55:13.826436Z","shell.execute_reply":"2024-06-11T11:55:13.825096Z","shell.execute_reply.started":"2024-06-11T11:55:12.821499Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'data.tar' already there; not retrieving.\n","\n"]}],"source":["!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:55:13.828352Z","iopub.status.busy":"2024-06-11T11:55:13.827987Z","iopub.status.idle":"2024-06-11T11:55:14.215924Z","shell.execute_reply":"2024-06-11T11:55:14.214478Z","shell.execute_reply.started":"2024-06-11T11:55:13.828322Z"},"trusted":true},"outputs":[],"source":["import tarfile\n","unzip_path = '.'\n","tar = tarfile.open('data.tar')\n","tar.extractall(path=unzip_path)\n","tar.close()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:55:14.218854Z","iopub.status.busy":"2024-06-11T11:55:14.218562Z","iopub.status.idle":"2024-06-11T11:56:06.555961Z","shell.execute_reply":"2024-06-11T11:56:06.554804Z","shell.execute_reply.started":"2024-06-11T11:55:14.218831Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: groq in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n","Requirement already satisfied: bert_score in /opt/conda/lib/python3.10/site-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\n","Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install groq\n","!pip install bert_score"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:06.557880Z","iopub.status.busy":"2024-06-11T11:56:06.557578Z","iopub.status.idle":"2024-06-11T11:56:06.566742Z","shell.execute_reply":"2024-06-11T11:56:06.565608Z","shell.execute_reply.started":"2024-06-11T11:56:06.557853Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","from sklearn.isotonic import IsotonicRegression\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from transformers import pipeline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pathlib\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","from bert_score import score\n","import os\n","from groq import Groq\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","import json\n","import re\n","import concurrent.futures\n","from tqdm import tqdm\n","from yaml import safe_load"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:06.568197Z","iopub.status.busy":"2024-06-11T11:56:06.567930Z","iopub.status.idle":"2024-06-11T11:56:06.581174Z","shell.execute_reply":"2024-06-11T11:56:06.580276Z","shell.execute_reply.started":"2024-06-11T11:56:06.568166Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:11.049804Z","iopub.status.busy":"2024-06-11T11:56:11.049079Z","iopub.status.idle":"2024-06-11T11:56:11.054214Z","shell.execute_reply":"2024-06-11T11:56:11.053297Z","shell.execute_reply.started":"2024-06-11T11:56:11.049770Z"},"trusted":true},"outputs":[],"source":["# # Define models and tokenizers\n","# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n","# tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.float16\n","# )\n","\n","# mistral_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:12.136069Z","iopub.status.busy":"2024-06-11T11:56:12.135172Z","iopub.status.idle":"2024-06-11T11:56:12.140132Z","shell.execute_reply":"2024-06-11T11:56:12.139061Z","shell.execute_reply.started":"2024-06-11T11:56:12.136037Z"},"trusted":true},"outputs":[],"source":["# model_name2 = 'stabilityai/StableBeluga-13B'\n","# tokenizer_beluga = AutoTokenizer.from_pretrained(model_name2)\n","# tokenizer_beluga.pad_token = tokenizer_beluga.eos_token\n","\n","# beluga_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name2,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:14.137598Z","iopub.status.busy":"2024-06-11T11:56:14.137197Z","iopub.status.idle":"2024-06-11T11:56:14.143379Z","shell.execute_reply":"2024-06-11T11:56:14.142297Z","shell.execute_reply.started":"2024-06-11T11:56:14.137570Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return ' '.join(filtered_tokens)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:14.457951Z","iopub.status.busy":"2024-06-11T11:56:14.457519Z","iopub.status.idle":"2024-06-11T11:56:14.471636Z","shell.execute_reply":"2024-06-11T11:56:14.470513Z","shell.execute_reply.started":"2024-06-11T11:56:14.457918Z"},"trusted":true},"outputs":[],"source":["# Utility functions\n","def load_questions(file_path):\n","    questions = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            questions.append(json.loads(line.strip()))\n","    return questions\n","\n","def load_model_answers(dir_path):\n","    model_answers = {}\n","    for file_name in os.listdir(dir_path):\n","        model_name = file_name.split('.')[0]\n","        model_answers[model_name] = {}\n","        with open(os.path.join(dir_path, file_name), 'r') as f:\n","            for line in f:\n","                answer = json.loads(line.strip())\n","                question_id = answer['question_id']\n","                model_answers[model_name][question_id] = answer\n","    return model_answers\n","\n","def get_endpoint(endpoint_config):\n","    # Implementation depends on the format of your API config file\n","    return endpoint_config\n","\n","def make_config(file_path):\n","    with open(file_path, 'r') as f:\n","        return safe_load(f)\n","\n","def chat_completion_groq(model, conv, temperature, max_tokens, api_key):\n","    client = Groq(api_key=api_key)\n","    chat_completion = client.chat.completions.create(\n","        messages=conv,\n","        model=model,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    return chat_completion.choices[0].message.content\n","\n","def get_score(judgment, pattern, pairwise=True):\n","    matches = pattern.findall(judgment)\n","    matches = [m for m in matches if m != \"\"]\n","    if len(set(matches)) == 0:\n","        return None, True\n","    elif len(set(matches)) == 1:\n","        if pairwise:\n","            return matches[0].strip(\"\\n\"), False\n","        return int(matches[0])\n","    else:\n","        return None, False"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:15.293026Z","iopub.status.busy":"2024-06-11T11:56:15.292362Z","iopub.status.idle":"2024-06-11T11:56:17.428304Z","shell.execute_reply":"2024-06-11T11:56:17.427400Z","shell.execute_reply.started":"2024-06-11T11:56:15.292992Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["What a profound and complex question! The meaning of life is a topic of ongoing debate and inquiry across various disciplines, including philosophy, theology, science, and psychology. While there's no one definitive answer, here's a brief exploration:\n","\n","**The Meaning of Life:**\n","\n","The meaning of life is a deeply personal and subjective question that has puzzled humans for centuries. Some possible answers include:\n","\n","1. **Pursuit of Happiness**: The pursuit of happiness, fulfillment, and self-actualization is a common goal for many individuals.\n","2. **Purpose and Significance**: Having a sense of purpose, making a positive impact, and leaving a lasting legacy can give life meaning.\n","3. **Love, Connection, and Relationships**: Building strong relationships, experiencing love, and fostering connections with others can bring a sense of fulfillment.\n","4. **Personal Growth and Self-Discovery**: Continuously learning, growing, and self-improving can give life meaning and direction.\n","5. **Transcendence and Spirituality**: For many, spirituality, faith, or a connection to something greater than oneself provides a sense of purpose and meaning.\n","\n","Now, let's address the impact of modern AI on real life and potential future outcomes:\n","\n","**How Modern AI is Affecting Real Life:**\n","\n","1. **Automation and Job Displacement**: AI has already started replacing certain jobs, transforming industries, and changing the workforce.\n","2. **Enhanced Efficiency and Productivity**: AI is streamlining processes, improving accuracy, and increasing productivity in various sectors, such as healthcare, finance, and customer service.\n","3. **Personalized Experiences**: AI-powered algorithms are enabling personalized recommendations, targeted advertising, and tailored customer interactions.\n","4. **Healthcare and Medicine**: AI is being used to analyze medical data, diagnose diseases, and develop personalized treatment plans.\n","5. **Cybersecurity**: AI-powered systems are helping to detect and prevent cyber threats, protecting sensitive information and systems.\n","\n","**Potential Future Outcomes of AI:**\n","\n","1. **Singularity and Superintelligence**: The potential creation of superintelligent AI could significantly impact human existence, with some experts predicting a utopian future and others warning of existential risks.\n","2. **Job Market Shifts**: As AI continues to automate jobs, there may be a significant shift in the job market, with a greater focus on creative, social, and emotional intelligence.\n","3. **AI-Driven Entrepreneurship**: AI could enable new business models, products, and services, leading to innovative entrepreneurship and economic growth.\n","4. **Improved Quality of Life**: AI can help solve complex problems in areas like healthcare, education, and environmental sustainability, leading to an improved quality of life.\n","5. **Ethical Concerns and Bias**: As AI becomes more pervasive, there is a growing need to address ethical concerns, such as bias, accountability, and transparency in AI decision-making processes.\n","\n","In conclusion, the meaning of life is a deeply personal and complex question that may not have a single, definitive answer. Meanwhile, modern AI is transforming various aspects of our lives, with potential outcomes ranging from significant benefits to potential risks and challenges. As AI continues to evolve, it's essential to address the ethical considerations and ensure that AI is developed and used in ways that benefit humanity as a whole.\n"]}],"source":["client = Groq(api_key=\"Your_API_KEY\")\n","\n","chat_completion = client.chat.completions.create(\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n","        }\n","\n","    ],\n","    model=\"llama3-70b-8192\",\n",")\n","\n","print(chat_completion.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:12.364018Z","iopub.status.busy":"2024-06-05T09:38:12.363622Z","iopub.status.idle":"2024-06-05T09:38:12.391914Z","shell.execute_reply":"2024-06-05T09:38:12.391081Z","shell.execute_reply.started":"2024-06-05T09:38:12.363988Z"},"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model, regex_pattern, temperature=0.8, max_tokens=300, pairwise=True):\n","        self.api_key = api_key\n","        self.judge_model = judge_model\n","        self.regex_pattern = re.compile(regex_pattern)\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","        self.pairwise = pairwise\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","    def get_mistral_answer(self, prompt, include_stopwords=True):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n","            do_sample=True, output_scores=True, return_dict_in_generate=True\n","        )\n","\n","        response_ids = outputs.sequences[0]\n","        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n","\n","        logits = outputs.scores  # Logits of the generated tokens\n","        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n","\n","        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n","        if include_stopwords:\n","            token_confidences = [prob.max().item() for prob in softmax_probs]\n","        else:\n","            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n","\n","        mean_confidence = np.mean(token_confidences)\n","\n","        return response_text, mean_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=\"llama3-70b-8192\",\n","        )\n","\n","        llama3_response = chat_completion.choices[0].message.content.strip()\n","        return llama3_response\n","\n","    def parse_evaluation(self, evaluation):\n","        if \"1 point\" in evaluation:\n","            return 1\n","        return 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n","        )\n","\n","        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate(self, prompts):\n","        self.prompts = prompts  # Ensure prompts are stored\n","        for prompt in prompts:\n","            response, confidence = self.get_mistral_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies\n","\n","    def plot_reliability_diagram(self, bins, bin_accuracies):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot((bins[:-1] + bins[1:]) / 2, bin_accuracies, marker='o', label='Accuracy per bin')\n","        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n","        plt.xlabel('Confidence')\n","        plt.ylabel('Accuracy')\n","        plt.title('Reliability Diagram')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:13.025882Z","iopub.status.busy":"2024-06-05T09:38:13.025298Z","iopub.status.idle":"2024-06-05T09:40:08.802455Z","shell.execute_reply":"2024-06-05T09:40:08.801451Z","shell.execute_reply.started":"2024-06-05T09:38:13.025852Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a215bc76f664495b445209290af45c4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"948c2410474e47c5bc7f1ef6fe11cf73","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"735945e2f09f49eb8cd9d79aa6f4e619","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98b40942285743e39e9b3f246c6a9263","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0accba5fde9b43bb988a8b83d6b1f13e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6630645990371704, Recall: 0.6872342228889465, F1: 0.6748167872428894\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\", regex_pattern=r\"(correct|incorrect)\")\n","prompts = [\"What is nuclear fusion?\", \"Tell me about the current AI situation in the world?\", \"How does a combustion engine work?\"]\n","pipeline.evaluate(prompts)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:58:26.596665Z","iopub.status.busy":"2024-06-05T09:58:26.595910Z","iopub.status.idle":"2024-06-05T09:58:26.607206Z","shell.execute_reply":"2024-06-05T09:58:26.606213Z","shell.execute_reply.started":"2024-06-05T09:58:26.596631Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected Calibration Error (ECE): 0.050000000000000044\n"]}],"source":["data, bins, bin_accuracies = pipeline.calculate_ece()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:40:27.233246Z","iopub.status.busy":"2024-06-05T09:40:27.232603Z","iopub.status.idle":"2024-06-05T09:40:27.237806Z","shell.execute_reply":"2024-06-05T09:40:27.236833Z","shell.execute_reply.started":"2024-06-05T09:40:27.233215Z"},"trusted":true},"outputs":[],"source":["few_shot_examples = \"\"\"\n","Q: What is nuclear fusion?\n","A: Nuclear fusion is a reaction in which two atomic nuclei combine to form a heavier nucleus, releasing energy in the process.\n","\n","Q: How does a combustion engine work?\n","A: A combustion engine works by burning fuel in a combustion chamber to produce mechanical energy that drives the engine.\n","\"\"\"\n","\n","def get_mistral_answer_few_shot(prompt):\n","    few_shot_prompt = few_shot_examples + f\"\\nQ: {prompt}\\nA:\"\n","    response, confidence = get_mistral_answer(few_shot_prompt)\n","    return response, confidence\n","\n","# Example usage:\n","few_shot_response, few_shot_confidence = get_mistral_answer_few_shot(\"Tell me about the current AI situation in the world.\")\n","print(f\"Few-shot response: {few_shot_response}\")\n","print(f\"Few-shot confidence: {few_shot_confidence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Class pipeline for mmlu dataset"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:24.497065Z","iopub.status.busy":"2024-06-11T11:56:24.496686Z","iopub.status.idle":"2024-06-11T11:56:24.503988Z","shell.execute_reply":"2024-06-11T11:56:24.502932Z","shell.execute_reply.started":"2024-06-11T11:56:24.497034Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, model_name):\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","    def encode(self, text, max_length=1024):\n","        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n","\n","    def decode(self, tokens):\n","        return self.tokenizer.decode(tokens, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:24.810539Z","iopub.status.busy":"2024-06-11T11:56:24.810169Z","iopub.status.idle":"2024-06-11T11:56:24.853636Z","shell.execute_reply":"2024-06-11T11:56:24.852887Z","shell.execute_reply.started":"2024-06-11T11:56:24.810512Z"},"trusted":true},"outputs":[],"source":["  class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","\n","        self.tokenizer = Tokenizer(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def get_model_answer(self, prompt):\n","        \"\"\"Generates an answer from the specified model.\"\"\"\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_new_tokens=300,  # Limit the number of generated tokens\n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0])\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0])\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:26.427218Z","iopub.status.busy":"2024-06-11T11:56:26.426546Z","iopub.status.idle":"2024-06-11T11:56:26.431688Z","shell.execute_reply":"2024-06-11T11:56:26.430689Z","shell.execute_reply.started":"2024-06-11T11:56:26.427186Z"},"trusted":true},"outputs":[],"source":["# pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","# pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Chain of thought"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:29.196973Z","iopub.status.busy":"2024-06-11T11:56:29.196603Z","iopub.status.idle":"2024-06-11T11:56:29.242839Z","shell.execute_reply":"2024-06-11T11:56:29.241694Z","shell.execute_reply.started":"2024-06-11T11:56:29.196944Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from bert_score import score\n","\n","class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def generate_cot_prompt(self, question):\n","        # Chain of Thought prompt\n","        cot_prompt = f\"Q: {question}\\nA: Let's think through this step by step.\\n\"\n","        cot_prompt += \"First, consider the main aspects of the question. \"\n","        cot_prompt += \"Next, break down the problem into smaller parts. \"\n","        cot_prompt += \"Finally, synthesize the information to form a coherent answer.\"\n","        return cot_prompt\n","\n","    def get_model_answer(self, question):\n","        \"\"\"Generates an answer from the specified model using Chain of Thought prompting.\"\"\"\n","        prompt = self.generate_cot_prompt(question)\n","        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_new_tokens=300,  # Limit the number of generated tokens\n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-11T11:56:31.630734Z","iopub.status.busy":"2024-06-11T11:56:31.630353Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e365e3615b76476e9105d7be4032ab20","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38573ae90a104dcbb6bb438163cf5a15","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54a568333a204fb4aada6dda2e18cbab","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e923083125641e38494ee96c2788154","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2acf0b708d6d4843bba31a9fe41da632","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"781528e1c8a94f49aaf13c198fa3e1bb","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6628078e7e124e4c87c2403aadf70f67","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00e6106291d14290b4a3c829cb9aac02","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"332cf1fcb8084d7ea4b5a75e79c69604","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32cf80961b30471b9d9ccbf6fb4483c6","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73391b35ea0d41de998db51b460eff01","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a42a2e396564a0a91e437a278caeb60","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating econometrics in category economics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e025059ec544cb3a8337770bc2f2353","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"273c5fa3517541c7a11bbe50e5e1e32e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9862bbbb87d047f581d286d99d8872c9","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59820746aca342b1be48541fed26f0a6","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b481cf26d6a43d6a996a4bb6e190ba9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.736057460308075, Recall: 0.7952789068222046, F1: 0.7643265128135681\n","Expected Calibration Error (ECE): 0.7\n","econometrics ECE: 0.7\n","Evaluating abstract_algebra in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6584957838058472, Recall: 0.6841974258422852, F1: 0.6709675788879395\n","Expected Calibration Error (ECE): 0.25000000000000006\n","abstract_algebra ECE: 0.25000000000000006\n","Evaluating moral_disputes in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6124948263168335, Recall: 0.6275460720062256, F1: 0.6185678243637085\n","Expected Calibration Error (ECE): 0.050000000000000044\n","moral_disputes ECE: 0.050000000000000044\n","Evaluating management in category business...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6000545024871826, Recall: 0.7123739719390869, F1: 0.6510104537010193\n","Expected Calibration Error (ECE): 0.44999999999999996\n","management ECE: 0.44999999999999996\n","Evaluating jurisprudence in category law...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6167377829551697, Recall: 0.6750869750976562, F1: 0.6434708833694458\n","Expected Calibration Error (ECE): 0.050000000000000044\n","jurisprudence ECE: 0.050000000000000044\n","Evaluating professional_psychology in category psychology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
