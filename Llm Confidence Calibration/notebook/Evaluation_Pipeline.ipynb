{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-10T10:33:38.117573Z","iopub.status.busy":"2024-06-10T10:33:38.117224Z","iopub.status.idle":"2024-06-10T10:34:01.736024Z","shell.execute_reply":"2024-06-10T10:34:01.734720Z","shell.execute_reply.started":"2024-06-10T10:33:38.117543Z"},"trusted":true},"outputs":[],"source":["!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:34:01.738503Z","iopub.status.busy":"2024-06-10T10:34:01.738153Z","iopub.status.idle":"2024-06-10T10:34:02.124906Z","shell.execute_reply":"2024-06-10T10:34:02.123908Z","shell.execute_reply.started":"2024-06-10T10:34:01.738466Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d02dedab58d445de89064e174ba35a66","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:35:11.335532Z","iopub.status.busy":"2024-06-10T10:35:11.335154Z","iopub.status.idle":"2024-06-10T10:35:49.969795Z","shell.execute_reply":"2024-06-10T10:35:49.968581Z","shell.execute_reply.started":"2024-06-10T10:35:11.335500Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:35:49.971958Z","iopub.status.busy":"2024-06-10T10:35:49.971622Z","iopub.status.idle":"2024-06-10T10:35:54.520422Z","shell.execute_reply":"2024-06-10T10:35:54.519312Z","shell.execute_reply.started":"2024-06-10T10:35:49.971925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-10 10:35:50--  https://people.eecs.berkeley.edu/~hendrycks/data.tar\n","Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n","Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 166184960 (158M) [application/x-tar]\n","Saving to: 'data.tar'\n","\n","data.tar            100%[===================>] 158.49M  54.2MB/s    in 2.9s    \n","\n","2024-06-10 10:35:54 (54.2 MB/s) - 'data.tar' saved [166184960/166184960]\n","\n"]}],"source":["!wget -nc https://people.eecs.berkeley.edu/~hendrycks/data.tar"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:35:54.522248Z","iopub.status.busy":"2024-06-10T10:35:54.521878Z","iopub.status.idle":"2024-06-10T10:35:54.766058Z","shell.execute_reply":"2024-06-10T10:35:54.764898Z","shell.execute_reply.started":"2024-06-10T10:35:54.522210Z"},"trusted":true},"outputs":[],"source":["import tarfile\n","unzip_path = '.'\n","tar = tarfile.open('data.tar')\n","tar.extractall(path=unzip_path)\n","tar.close()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:35:54.768915Z","iopub.status.busy":"2024-06-10T10:35:54.768583Z","iopub.status.idle":"2024-06-10T10:36:47.682927Z","shell.execute_reply":"2024-06-10T10:36:47.681571Z","shell.execute_reply.started":"2024-06-10T10:35:54.768875Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting groq\n","  Downloading groq-0.8.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.2.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.5.3)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.14.6)\n","Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hInstalling collected packages: groq\n","Successfully installed groq-0.8.0\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\n","Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.41.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.3.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m914.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install groq\n","!pip install bert_score"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:36:47.685169Z","iopub.status.busy":"2024-06-10T10:36:47.684744Z","iopub.status.idle":"2024-06-10T10:37:15.006298Z","shell.execute_reply":"2024-06-10T10:37:15.005213Z","shell.execute_reply.started":"2024-06-10T10:36:47.685126Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-10 10:36:55.057715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-10 10:36:55.057858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-10 10:36:55.340623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","import numpy as np\n","from sklearn.isotonic import IsotonicRegression\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from transformers import pipeline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pathlib\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","from bert_score import score\n","import os\n","from groq import Groq\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","import json\n","import re\n","import concurrent.futures\n","from tqdm import tqdm\n","from yaml import safe_load"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.008395Z","iopub.status.busy":"2024-06-10T10:37:15.007778Z","iopub.status.idle":"2024-06-10T10:37:15.329854Z","shell.execute_reply":"2024-06-10T10:37:15.328766Z","shell.execute_reply.started":"2024-06-10T10:37:15.008365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.331749Z","iopub.status.busy":"2024-06-10T10:37:15.331281Z","iopub.status.idle":"2024-06-10T10:37:15.336748Z","shell.execute_reply":"2024-06-10T10:37:15.335806Z","shell.execute_reply.started":"2024-06-10T10:37:15.331714Z"},"trusted":true},"outputs":[],"source":["# # Define models and tokenizers\n","# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)\n","# tokenizer_mistral.pad_token = tokenizer_mistral.eos_token\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.float16\n","# )\n","\n","# mistral_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.338381Z","iopub.status.busy":"2024-06-10T10:37:15.337989Z","iopub.status.idle":"2024-06-10T10:37:15.355357Z","shell.execute_reply":"2024-06-10T10:37:15.354548Z","shell.execute_reply.started":"2024-06-10T10:37:15.338353Z"},"trusted":true},"outputs":[],"source":["# model_name2 = 'stabilityai/StableBeluga-13B'\n","# tokenizer_beluga = AutoTokenizer.from_pretrained(model_name2)\n","# tokenizer_beluga.pad_token = tokenizer_beluga.eos_token\n","\n","# beluga_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name2,\n","#     torch_dtype=torch.float16,\n","#     quantization_config=bnb_config,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","# )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.356760Z","iopub.status.busy":"2024-06-10T10:37:15.356476Z","iopub.status.idle":"2024-06-10T10:37:15.368665Z","shell.execute_reply":"2024-06-10T10:37:15.367920Z","shell.execute_reply.started":"2024-06-10T10:37:15.356735Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return ' '.join(filtered_tokens)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.371617Z","iopub.status.busy":"2024-06-10T10:37:15.371329Z","iopub.status.idle":"2024-06-10T10:37:15.383942Z","shell.execute_reply":"2024-06-10T10:37:15.383202Z","shell.execute_reply.started":"2024-06-10T10:37:15.371591Z"},"trusted":true},"outputs":[],"source":["# Utility functions\n","def load_questions(file_path):\n","    questions = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            questions.append(json.loads(line.strip()))\n","    return questions\n","\n","def load_model_answers(dir_path):\n","    model_answers = {}\n","    for file_name in os.listdir(dir_path):\n","        model_name = file_name.split('.')[0]\n","        model_answers[model_name] = {}\n","        with open(os.path.join(dir_path, file_name), 'r') as f:\n","            for line in f:\n","                answer = json.loads(line.strip())\n","                question_id = answer['question_id']\n","                model_answers[model_name][question_id] = answer\n","    return model_answers\n","\n","def get_endpoint(endpoint_config):\n","    # Implementation depends on the format of your API config file\n","    return endpoint_config\n","\n","def make_config(file_path):\n","    with open(file_path, 'r') as f:\n","        return safe_load(f)\n","\n","def chat_completion_groq(model, conv, temperature, max_tokens, api_key):\n","    client = Groq(api_key=api_key)\n","    chat_completion = client.chat.completions.create(\n","        messages=conv,\n","        model=model,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    return chat_completion.choices[0].message.content\n","\n","def get_score(judgment, pattern, pairwise=True):\n","    matches = pattern.findall(judgment)\n","    matches = [m for m in matches if m != \"\"]\n","    if len(set(matches)) == 0:\n","        return None, True\n","    elif len(set(matches)) == 1:\n","        if pairwise:\n","            return matches[0].strip(\"\\n\"), False\n","        return int(matches[0])\n","    else:\n","        return None, False"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:15.385319Z","iopub.status.busy":"2024-06-10T10:37:15.385041Z","iopub.status.idle":"2024-06-10T10:37:17.835269Z","shell.execute_reply":"2024-06-10T10:37:17.834364Z","shell.execute_reply.started":"2024-06-10T10:37:15.385295Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["What a profound and timely question!\n","\n","**What is the meaning of life?**\n","\n","The meaning of life is a philosophical question that has puzzled humans for centuries. There is no one definitive answer, as it's a subjective and personal inquiry. However, here are some perspectives:\n","\n","* **Existentialism**: Life has no inherent meaning, and it's up to each individual to create their own purpose and meaning.\n","* **Religion**: Many religions believe that the purpose of life is to follow a set of beliefs, moral codes, and principles to achieve spiritual enlightenment or salvation.\n","* **Humanism**: Life's meaning lies in human experiences, relationships, and the pursuit of happiness, knowledge, and personal growth.\n","* **Scientific perspective**: Some scientists suggest that life's purpose might be to propagate the species, ensuring the survival of humanity.\n","* **Philosophical perspective**: The meaning of life might be to seek knowledge, understand the universe, and find one's place within it.\n","\n","Ultimately, the meaning of life is a personal and subjective quest that each individual must undertake.\n","\n","**How modern AI is affecting real life**\n","\n","Artificial Intelligence (AI) has become an integral part of modern life, transforming various aspects of society. Here are some examples:\n","\n","* **Workforce and economy**: AI is automating jobs, displacing some workers, but also creating new opportunities in fields like data science, machine learning, and AI development.\n","* **Healthcare**: AI-powered diagnosis systems, medical imaging analysis, and personalized medicine are improving healthcare outcomes.\n","* **Customer service**: Chatbots and virtual assistants are changing the way we interact with businesses and access information.\n","* **Transportation**: Autonomous vehicles are being developed to enhance road safety and efficiency.\n","* **Education**: AI-powered adaptive learning systems are personalizing education, making it more effective and efficient.\n","\n","**Future AI outcomes**\n","\n","As AI continues to advance, we can expect:\n","\n","* **Increased automation**: AI will replace more jobs, especially those that involve repetitive tasks or can be easily automated.\n","* **Improved efficiency**: AI will optimize processes, leading to increased productivity and efficiency in various industries.\n","* **Enhanced decision-making**: AI will provide better insights, enabling humans to make more informed decisions.\n","* **New job creation**: AI will create new job opportunities in fields like AI development, deployment, and maintenance.\n","* **Ethical considerations**: The need for ethical considerations, such as bias, privacy, and transparency, will become increasingly important as AI becomes more pervasive.\n","* **Potential risks**: The development of autonomous weapons, job displacement, and increased surveillance are potential risks associated with AI.\n","\n","In the future, we can expect AI to continue transforming industries and aspects of our lives. It's essential to ensure that AI is developed and used responsibly, with consideration for its potential impact on society.\n","\n","I hope this provides a comprehensive overview of the meaning of life and the impact of AI on our lives!\n"]}],"source":["client = Groq(api_key=\"YOUR_API_KEY\")\n","\n","chat_completion = client.chat.completions.create(\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"What is the meaning of life? How modern Ai is affeting the real life and what will be the Ai outcomes in future?\",\n","        }\n","\n","    ],\n","    model=\"llama3-70b-8192\",\n",")\n","\n","print(chat_completion.choices[0].message.content)"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:12.364018Z","iopub.status.busy":"2024-06-05T09:38:12.363622Z","iopub.status.idle":"2024-06-05T09:38:12.391914Z","shell.execute_reply":"2024-06-05T09:38:12.391081Z","shell.execute_reply.started":"2024-06-05T09:38:12.363988Z"},"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model, regex_pattern, temperature=0.8, max_tokens=300, pairwise=True):\n","        self.api_key = api_key\n","        self.judge_model = judge_model\n","        self.regex_pattern = re.compile(regex_pattern)\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","        self.pairwise = pairwise\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","    def get_mistral_answer(self, prompt, include_stopwords=True):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8,\n","            do_sample=True, output_scores=True, return_dict_in_generate=True\n","        )\n","\n","        response_ids = outputs.sequences[0]\n","        response_text = tokenizer_mistral.decode(response_ids, skip_special_tokens=True)\n","\n","        logits = outputs.scores  # Logits of the generated tokens\n","        softmax_probs = torch.softmax(torch.stack(logits), dim=-1)\n","\n","        tokens = tokenizer_mistral.convert_ids_to_tokens(response_ids)\n","        if include_stopwords:\n","            token_confidences = [prob.max().item() for prob in softmax_probs]\n","        else:\n","            token_confidences = [prob.max().item() for prob, token in zip(softmax_probs, tokens) if token.lower() not in stop_words]\n","\n","        mean_confidence = np.mean(token_confidences)\n","\n","        return response_text, mean_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=\"llama3-70b-8192\",\n","        )\n","\n","        llama3_response = chat_completion.choices[0].message.content.strip()\n","        return llama3_response\n","\n","    def parse_evaluation(self, evaluation):\n","        if \"1 point\" in evaluation:\n","            return 1\n","        return 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = tokenizer_mistral.encode_plus(\n","            prompt, return_tensors='pt', max_length=1024, truncation=True, padding='max_length'\n","        ).to(mistral_model.device)\n","\n","        outputs = mistral_model.generate(\n","            inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","            max_new_tokens=300, num_return_sequences=1, temperature=0.8, do_sample=True\n","        )\n","\n","        reference_text = tokenizer_mistral.decode(outputs[0], skip_special_tokens=True)\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate(self, prompts):\n","        self.prompts = prompts  # Ensure prompts are stored\n","        for prompt in prompts:\n","            response, confidence = self.get_mistral_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies\n","\n","    def plot_reliability_diagram(self, bins, bin_accuracies):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot((bins[:-1] + bins[1:]) / 2, bin_accuracies, marker='o', label='Accuracy per bin')\n","        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n","        plt.xlabel('Confidence')\n","        plt.ylabel('Accuracy')\n","        plt.title('Reliability Diagram')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:38:13.025882Z","iopub.status.busy":"2024-06-05T09:38:13.025298Z","iopub.status.idle":"2024-06-05T09:40:08.802455Z","shell.execute_reply":"2024-06-05T09:40:08.801451Z","shell.execute_reply.started":"2024-06-05T09:38:13.025852Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a215bc76f664495b445209290af45c4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"948c2410474e47c5bc7f1ef6fe11cf73","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"735945e2f09f49eb8cd9d79aa6f4e619","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98b40942285743e39e9b3f246c6a9263","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0accba5fde9b43bb988a8b83d6b1f13e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.6630645990371704, Recall: 0.6872342228889465, F1: 0.6748167872428894\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model=\"llama3-70b-8192\", regex_pattern=r\"(correct|incorrect)\")\n","prompts = [\"What is nuclear fusion?\", \"Tell me about the current AI situation in the world?\", \"How does a combustion engine work?\"]\n","pipeline.evaluate(prompts)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:58:26.596665Z","iopub.status.busy":"2024-06-05T09:58:26.595910Z","iopub.status.idle":"2024-06-05T09:58:26.607206Z","shell.execute_reply":"2024-06-05T09:58:26.606213Z","shell.execute_reply.started":"2024-06-05T09:58:26.596631Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected Calibration Error (ECE): 0.050000000000000044\n"]}],"source":["data, bins, bin_accuracies = pipeline.calculate_ece()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T09:40:27.233246Z","iopub.status.busy":"2024-06-05T09:40:27.232603Z","iopub.status.idle":"2024-06-05T09:40:27.237806Z","shell.execute_reply":"2024-06-05T09:40:27.236833Z","shell.execute_reply.started":"2024-06-05T09:40:27.233215Z"},"trusted":true},"outputs":[],"source":["few_shot_examples = \"\"\"\n","Q: What is nuclear fusion?\n","A: Nuclear fusion is a reaction in which two atomic nuclei combine to form a heavier nucleus, releasing energy in the process.\n","\n","Q: How does a combustion engine work?\n","A: A combustion engine works by burning fuel in a combustion chamber to produce mechanical energy that drives the engine.\n","\"\"\"\n","\n","def get_mistral_answer_few_shot(prompt):\n","    few_shot_prompt = few_shot_examples + f\"\\nQ: {prompt}\\nA:\"\n","    response, confidence = get_mistral_answer(few_shot_prompt)\n","    return response, confidence\n","\n","# Example usage:\n","few_shot_response, few_shot_confidence = get_mistral_answer_few_shot(\"Tell me about the current AI situation in the world.\")\n","print(f\"Few-shot response: {few_shot_response}\")\n","print(f\"Few-shot confidence: {few_shot_confidence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Class pipeline for mmlu dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:37:17.836673Z","iopub.status.busy":"2024-06-10T10:37:17.836397Z","iopub.status.idle":"2024-06-10T10:37:17.842738Z","shell.execute_reply":"2024-06-10T10:37:17.841812Z","shell.execute_reply.started":"2024-06-10T10:37:17.836649Z"},"trusted":true},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, model_name):\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","    def encode(self, text, max_length=1024):\n","        return self.tokenizer.encode_plus(text, return_tensors='pt', max_length=max_length, truncation=True)\n","\n","    def decode(self, tokens):\n","        return self.tokenizer.decode(tokens, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T12:03:08.407213Z","iopub.status.busy":"2024-06-06T12:03:08.406815Z","iopub.status.idle":"2024-06-06T12:03:08.455995Z","shell.execute_reply":"2024-06-06T12:03:08.454988Z","shell.execute_reply.started":"2024-06-06T12:03:08.407177Z"},"trusted":true},"outputs":[],"source":["class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","\n","        self.tokenizer = Tokenizer(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def get_model_answer(self, prompt):\n","        \"\"\"Generates an answer from the specified model.\"\"\"\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_length=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0])\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0])\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T12:03:08.870257Z","iopub.status.busy":"2024-06-06T12:03:08.869547Z","iopub.status.idle":"2024-06-06T12:08:31.227430Z","shell.execute_reply":"2024-06-06T12:08:31.225763Z","shell.execute_reply.started":"2024-06-06T12:03:08.870224Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb65b3aab51c405ba7ab4cdf963567b7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating high_school_psychology in category psychology...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7022345662117004, Recall: 0.7619578838348389, F1: 0.7282421588897705\n","Expected Calibration Error (ECE): 0.19999999999999996\n","high_school_psychology ECE: 0.19999999999999996\n","Evaluating computer_security in category computer science...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7066546082496643, Recall: 0.643768846988678, F1: 0.6691666841506958\n","Expected Calibration Error (ECE): 0.42500000000000004\n","computer_security ECE: 0.42500000000000004\n","Evaluating moral_scenarios in category philosophy...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.801176905632019, Recall: 0.8034803867340088, F1: 0.7996551990509033\n","Expected Calibration Error (ECE): 0.050000000000000044\n","moral_scenarios ECE: 0.050000000000000044\n","Evaluating high_school_us_history in category history...\n"]},{"ename":"ValueError","evalue":"Input length of input_ids is 455, but `max_length` is set to 300. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m EvaluationPipeline(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, judge_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m, smaller_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/data/dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[36], line 194\u001b[0m, in \u001b[0;36mEvaluationPipeline.evaluate_folder\u001b[0;34m(self, folder_path)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m category_name:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in category \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m     data, bins, bin_accuracies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# Align shapes and calculate ECE\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     bin_confidences \u001b[38;5;241m=\u001b[39m (bins[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m bins[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Confidences for each bin\u001b[39;00m\n","Cell \u001b[0;32mIn[36], line 161\u001b[0m, in \u001b[0;36mEvaluationPipeline.evaluate_from_csv\u001b[0;34m(self, csv_file_path)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Take only the first column (questions)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n\u001b[0;32m--> 161\u001b[0m     response, confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfidences\u001b[38;5;241m.\u001b[39mappend(confidence)\n","Cell \u001b[0;32mIn[36], line 88\u001b[0m, in \u001b[0;36mEvaluationPipeline.get_model_answer\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates an answer from the specified model.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 88\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Get the confidence score\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model does not support `cache_implementation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`. Please check the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue: https://github.com/huggingface/transformers/issues/28981\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1645\u001b[0m             )\n\u001b[1;32m   1646\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_static_cache(batch_size, generation_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1648\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1176\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1175\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1186\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 455, but `max_length` is set to 300. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:38:13.364573Z","iopub.status.busy":"2024-06-10T10:38:13.364195Z","iopub.status.idle":"2024-06-10T10:38:13.408342Z","shell.execute_reply":"2024-06-10T10:38:13.407517Z","shell.execute_reply.started":"2024-06-10T10:38:13.364543Z"},"trusted":true},"outputs":[],"source":["  class EvaluationPipeline:\n","    def __init__(self, api_key, judge_model_name, smaller_model_name, temperature=0.8, max_tokens=300):\n","        self.api_key = api_key\n","        self.judge_model_name = judge_model_name\n","        self.temperature = temperature\n","        self.max_tokens = max_tokens\n","\n","\n","        self.tokenizer = Tokenizer(smaller_model_name)\n","        \n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16\n","        )\n","\n","        self.main_model = AutoModelForCausalLM.from_pretrained(\n","            smaller_model_name,\n","            torch_dtype=torch.float16,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True,\n","            device_map=\"auto\",\n","        )\n","\n","        self.categories = {\n","            \"abstract_algebra\": [\"math\"],\n","            \"anatomy\": [\"health\"],\n","            \"astronomy\": [\"physics\"],\n","            \"business_ethics\": [\"business\"],\n","            \"clinical_knowledge\": [\"health\"],\n","            \"college_biology\": [\"biology\"],\n","            \"college_chemistry\": [\"chemistry\"],\n","            \"college_computer_science\": [\"computer science\"],\n","            \"college_mathematics\": [\"math\"],\n","            \"college_medicine\": [\"health\"],\n","            \"college_physics\": [\"physics\"],\n","            \"computer_security\": [\"computer science\"],\n","            \"conceptual_physics\": [\"physics\"],\n","            \"econometrics\": [\"economics\"],\n","            \"electrical_engineering\": [\"engineering\"],\n","            \"elementary_mathematics\": [\"math\"],\n","            \"formal_logic\": [\"philosophy\"],\n","            \"global_facts\": [\"other\"],\n","            \"high_school_biology\": [\"biology\"],\n","            \"high_school_chemistry\": [\"chemistry\"],\n","            \"high_school_computer_science\": [\"computer science\"],\n","            \"high_school_european_history\": [\"history\"],\n","            \"high_school_geography\": [\"geography\"],\n","            \"high_school_government_and_politics\": [\"politics\"],\n","            \"high_school_macroeconomics\": [\"economics\"],\n","            \"high_school_mathematics\": [\"math\"],\n","            \"high_school_microeconomics\": [\"economics\"],\n","            \"high_school_physics\": [\"physics\"],\n","            \"high_school_psychology\": [\"psychology\"],\n","            \"high_school_statistics\": [\"math\"],\n","            \"high_school_us_history\": [\"history\"],\n","            \"high_school_world_history\": [\"history\"],\n","            \"human_aging\": [\"health\"],\n","            \"human_sexuality\": [\"culture\"],\n","            \"international_law\": [\"law\"],\n","            \"jurisprudence\": [\"law\"],\n","            \"logical_fallacies\": [\"philosophy\"],\n","            \"machine_learning\": [\"computer science\"],\n","            \"management\": [\"business\"],\n","            \"marketing\": [\"business\"],\n","            \"medical_genetics\": [\"health\"],\n","            \"miscellaneous\": [\"other\"],\n","            \"moral_disputes\": [\"philosophy\"],\n","            \"moral_scenarios\": [\"philosophy\"],\n","            \"nutrition\": [\"health\"],\n","            \"philosophy\": [\"philosophy\"],\n","            \"prehistory\": [\"history\"],\n","            \"professional_accounting\": [\"other\"],\n","            \"professional_law\": [\"law\"],\n","            \"professional_medicine\": [\"health\"],\n","            \"professional_psychology\": [\"psychology\"],\n","            \"public_relations\": [\"politics\"],\n","            \"security_studies\": [\"politics\"],\n","            \"sociology\": [\"culture\"],\n","            \"us_foreign_policy\": [\"politics\"],\n","            \"virology\": [\"health\"],\n","            \"world_religions\": [\"philosophy\"],\n","        }\n","\n","    def get_model_answer(self, prompt):\n","        \"\"\"Generates an answer from the specified model.\"\"\"\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            max_new_tokens=300,  # Limit the number of generated tokens\n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            output_scores=True, \n","            return_dict_in_generate=True\n","        )\n","        response = self.tokenizer.decode(outputs.sequences[0])\n","\n","        # Get the confidence score\n","        logits = torch.stack(outputs.scores, dim=1)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        token_ids = outputs.sequences[:, inputs['input_ids'].shape[1]:]\n","        confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","        avg_confidence = confidences[0]\n","        return response, avg_confidence\n","\n","    def judge_answer(self, response, prompt):\n","        chat_completion = client.chat.completions.create(\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","                        Review the user’s question and the corresponding response using the binary scoring system described below.\n","                        - 0 points: The response is incorrect or does not address the user’s question.\n","                        - 1 point: The response is correct and addresses the user’s question.\n","\n","                        User: {prompt}\n","                        Response: {response}\n","                        \"\"\"\n","                }\n","            ],\n","            model=self.judge_model_name,\n","        )\n","\n","        judge_response = chat_completion.choices[0].message.content.strip()\n","        return judge_response\n","\n","    def parse_evaluation(self, evaluation):\n","        return 1 if \"1 point\" in evaluation else 0\n","\n","    def generate_reference_text(self, prompt):\n","        inputs = self.tokenizer.encode(prompt).to(self.main_model.device)\n","        outputs = self.main_model.generate(\n","            inputs['input_ids'], \n","            attention_mask=inputs['attention_mask'],\n","            max_new_tokens=self.max_tokens, \n","            num_return_sequences=1, \n","            temperature=self.temperature, \n","            do_sample=True\n","        )\n","        reference_text = self.tokenizer.decode(outputs[0])\n","        return reference_text\n","\n","    def calculate_bertscore(self, references, candidates):\n","        P, R, F1 = score(candidates, references, lang=\"en\", model_type=\"bert-base-uncased\")\n","        return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","    def evaluate_from_csv(self, csv_file_path):\n","        # Clear data before processing each file\n","        self.prompts = []\n","        self.responses = []\n","        self.references = []\n","        self.confidences = []\n","        self.accuracies = []\n","\n","        df = pd.read_csv(csv_file_path)\n","        self.prompts = df.iloc[:, 0].tolist()  # Take only the first column (questions)\n","\n","        for prompt in self.prompts:\n","            response, confidence = self.get_model_answer(prompt)\n","            self.responses.append(response)\n","            self.confidences.append(confidence)\n","            reference_text = self.generate_reference_text(prompt)\n","            self.references.append(reference_text)\n","            judgement = self.judge_answer(response, prompt)\n","            accuracy = self.parse_evaluation(judgement)\n","            self.accuracies.append(accuracy)\n","\n","        precision, recall, f1 = self.calculate_bertscore(self.references, self.responses)\n","        print(f\"BERTScore - Precision: {precision}, Recall: {recall}, F1: {f1}\")\n","\n","        data, bins, bin_accuracies = self.calculate_ece()\n","        data.to_csv(\"results.csv\", index=False)  # Save results to CSV\n","        return data, bins, bin_accuracies\n","\n","    def evaluate_folder(self, folder_path):\n","        results = {}  # Store ECE values for each class\n","        category_results = {category: [] for category in set(cat for sublist in self.categories.values() for cat in sublist)}\n","\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith(\".csv\"):\n","                filepath = os.path.join(folder_path, filename)\n","                class_name = filename[:-8]  # Extract class name from filename without \"_dev.csv\"\n","                \n","                category_name = None\n","                for key, value in self.categories.items():\n","                    if key == class_name:\n","                        category_name = value[0]\n","                        break\n","\n","                if category_name:\n","                    print(f\"Evaluating {class_name} in category {category_name}...\")\n","                    data, bins, bin_accuracies = self.evaluate_from_csv(filepath)\n","\n","                    # Align shapes and calculate ECE\n","                    bin_confidences = (bins[:-1] + bins[1:]) / 2  # Confidences for each bin\n","                    valid_bins = bin_accuracies.dropna().index  # Bins with data\n","                    bin_accuracies = bin_accuracies[valid_bins]\n","                    bin_proportions = data['bin'].value_counts(normalize=True)[valid_bins]  # Get proportions for valid bins\n","                    bin_confidences = bin_confidences[valid_bins]  # Select confidences for valid bins\n","\n","                    ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","                    category_results[category_name].append(ece)\n","                    print(f\"{class_name} ECE: {ece}\")\n","\n","        # Calculate average ECE for each category and save to CSV\n","        average_ece_results = {category: np.mean(ece_list) for category, ece_list in category_results.items()}\n","        average_ece_df = pd.DataFrame({'Category': average_ece_results.keys(), 'Average ECE': average_ece_results.values()})\n","        average_ece_df.to_csv(\"category_results.csv\", index=False)\n","\n","    def calculate_ece(self):\n","        # Ensure all lists have the same length\n","        if len(self.prompts) == len(self.responses) == len(self.confidences) == len(self.accuracies):\n","            data = pd.DataFrame({\n","                'prompt': self.prompts,\n","                'response': self.responses,\n","                'confidence': self.confidences,\n","                'rating': self.accuracies\n","            })\n","        else:\n","            raise ValueError(\"All arrays must be of the same length\")\n","\n","        # Normalize confidence scores\n","        data['confidence_normalized'] = data['confidence'] / data['confidence'].max()\n","\n","        # Bin the normalized confidence scores\n","        bins = np.linspace(0, 1, 11)\n","        data['bin'] = pd.cut(data['confidence_normalized'], bins=bins, labels=False, include_lowest=True)\n","\n","        # Calculate accuracy for each bin\n","        bin_accuracies = data.groupby('bin')['rating'].mean()\n","        bin_proportions = data['bin'].value_counts(normalize=True)\n","\n","        # Drop bins with NaN values\n","        valid_bins = bin_accuracies.dropna().index\n","        bin_accuracies = bin_accuracies[valid_bins]\n","        bin_proportions = bin_proportions[valid_bins]\n","        bin_confidences = (bins[:-1] + bins[1:]) / 2\n","        bin_confidences = bin_confidences[valid_bins]\n","\n","        # Ensure lengths match\n","        bin_confidences = bin_confidences[:len(bin_accuracies)]\n","\n","        # Compute ECE\n","        ece = np.sum(np.abs(bin_accuracies - bin_confidences) * bin_proportions)\n","        print(f\"Expected Calibration Error (ECE): {ece}\")\n","\n","        return data, bins, bin_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T10:38:16.042219Z","iopub.status.busy":"2024-06-10T10:38:16.041841Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d23c439e64804cacbdee91337ad532a0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67a47637127b4080af5bbee983e77e31","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"390fa28915b548dabc5e018b4c980193","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d07be9fcdba4c63ad5d11ec3238aed9","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4eb21207c944c2abf801b384f224232","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a0b390dcf1e48d9b1322a6279ad8b12","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3a7f97e632b4c6093acdd353aa2ba41","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c187b693499144b08a64b9f8d3ff9cdb","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ad112aed16b4a2dbb811af2bc01f414","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21de3c39561b4cebad92fd5376ad4849","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74dfdc7bb2cf4e3288cd3843dcee3a32","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"586d3808d68141b1915365bfa572932f","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating security_studies in category politics...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9733da5b94064d33aba5f1a8d9368784","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4492b848de4e4d1ca465d4cb91984272","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dc3e19432394c7dbc04dfca0a8d2afd","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a12bbc95b41e42daaae6f7f19444e86e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5600e5e31c184000af49c9da1135203c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7350423336029053, Recall: 0.7138944268226624, F1: 0.7236862778663635\n","Expected Calibration Error (ECE): 0.050000000000000044\n","security_studies ECE: 0.050000000000000044\n","Evaluating abstract_algebra in category math...\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["BERTScore - Precision: 0.7636005878448486, Recall: 0.7139451503753662, F1: 0.7374914884567261\n","Expected Calibration Error (ECE): 0.19999999999999996\n","abstract_algebra ECE: 0.19999999999999996\n","Evaluating college_physics in category physics...\n"]}],"source":["pipeline = EvaluationPipeline(api_key=\"Your_api_key\", judge_model_name=\"llama3-70b-8192\", smaller_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\")\n","pipeline.evaluate_folder(\"/kaggle/working/data/dev\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
