{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-28T13:39:49.245585Z","iopub.status.busy":"2024-05-28T13:39:49.245321Z","iopub.status.idle":"2024-05-28T13:40:23.057045Z","shell.execute_reply":"2024-05-28T13:40:23.055895Z","shell.execute_reply.started":"2024-05-28T13:39:49.245544Z"},"trusted":true},"outputs":[],"source":["!pip install transformers bitsandbytes sentencepiece accelerate guidance --upgrade -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:40:23.059066Z","iopub.status.busy":"2024-05-28T13:40:23.058757Z","iopub.status.idle":"2024-05-28T13:40:23.317458Z","shell.execute_reply":"2024-05-28T13:40:23.316641Z","shell.execute_reply.started":"2024-05-28T13:40:23.059037Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"468745c4e3084d489a76a10b8fa01d63","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:52:01.852011Z","iopub.status.busy":"2024-05-28T13:52:01.851221Z","iopub.status.idle":"2024-05-28T13:52:38.342255Z","shell.execute_reply":"2024-05-28T13:52:38.340838Z","shell.execute_reply.started":"2024-05-28T13:52:01.851979Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade transformers -qq\n","!pip install accelerate\n","!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:41:59.767300Z","iopub.status.busy":"2024-05-28T13:41:59.766914Z","iopub.status.idle":"2024-05-28T13:42:15.783263Z","shell.execute_reply":"2024-05-28T13:42:15.782501Z","shell.execute_reply.started":"2024-05-28T13:41:59.767267Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-28 13:42:05.287876: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-28 13:42:05.287997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-28 13:42:05.411907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","import numpy as np\n","from sklearn.isotonic import IsotonicRegression\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n","from transformers import BitsAndBytesConfig\n","import torch.nn.functional as F\n","from transformers import pipeline\n","import pandas as pd"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:11:53.912678Z","iopub.status.busy":"2024-05-28T14:11:53.912302Z","iopub.status.idle":"2024-05-28T14:11:54.543055Z","shell.execute_reply":"2024-05-28T14:11:54.542205Z","shell.execute_reply.started":"2024-05-28T14:11:53.912649Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pathlib\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","import os\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","\n","def to_markdown(text):\n","  text = text.replace('•', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n","\n","\n","os.environ['GOOGLE_API_KEY']=\"AIzaSyCHghstLR5_5X8p7FN5Jx1VMzXKi_CmVGU\"\n","genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:42:15.784725Z","iopub.status.busy":"2024-05-28T13:42:15.784235Z","iopub.status.idle":"2024-05-28T13:44:27.362138Z","shell.execute_reply":"2024-05-28T13:44:27.361214Z","shell.execute_reply.started":"2024-05-28T13:42:15.784699Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3a0fd8e31114b2aa0847fc04e3b482a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7984770f6831473cbd5620c9ba1939d7","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46903d06703945fc99d9a8cb270dd3d7","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32151619d074445d95c8f93a64a3b11f","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"680ade71da334365bb216c621a2e9776","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca4238bcc463434b830c73400f2eaf65","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7040bafd4d4414f805d2d3e26d81d42","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4dc3fe2c6fd43aa87584d3251de572d","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","mistral_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    torch_dtype=torch.float16,\n","    quantization_config=bnb_config,\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:44:27.365209Z","iopub.status.busy":"2024-05-28T13:44:27.364205Z","iopub.status.idle":"2024-05-28T13:50:20.807323Z","shell.execute_reply":"2024-05-28T13:50:20.805639Z","shell.execute_reply.started":"2024-05-28T13:44:27.365174Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ce0710db2ea4dd99b49214b3f21fa4c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c46a54da57ea42f2804019f49612eae0","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e97fc6a545b542f4998718c01a2b0e40","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47a01956721e42d69db697c49183b624","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"516c24b1381944b7be68df4cccae59bd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57ec4c6a66414b30a69c4781e6501716","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28c806ea346547d3ace1df4fd7818d7f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1e0fc4dba6f412f80f4d8dd815c12d3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name2 = 'stabilityai/StableBeluga-13B'\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","beluga_model = AutoModelForCausalLM.from_pretrained(\n","    model_name2,\n","    torch_dtype=torch.float16,\n","    quantization_config=bnb_config,\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T13:52:45.254080Z","iopub.status.busy":"2024-05-28T13:52:45.253683Z","iopub.status.idle":"2024-05-28T13:52:45.258501Z","shell.execute_reply":"2024-05-28T13:52:45.257553Z","shell.execute_reply.started":"2024-05-28T13:52:45.254052Z"},"trusted":true},"outputs":[],"source":["gemini_model = genai.GenerativeModel('gemini-pro')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:11:57.853627Z","iopub.status.busy":"2024-05-28T14:11:57.853263Z","iopub.status.idle":"2024-05-28T14:11:57.859045Z","shell.execute_reply":"2024-05-28T14:11:57.858166Z","shell.execute_reply.started":"2024-05-28T14:11:57.853595Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    tokens = word_tokenize(text.lower())\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n","    return ' '.join(filtered_tokens)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:12:41.420067Z","iopub.status.busy":"2024-05-28T14:12:41.419657Z","iopub.status.idle":"2024-05-28T14:13:00.346723Z","shell.execute_reply":"2024-05-28T14:13:00.345676Z","shell.execute_reply.started":"2024-05-28T14:12:41.420036Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Expected Calibration Error (ECE): 0.8769400119781494\n"]}],"source":["def get_mistral_answer(prompt):\n","    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n","    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n","    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","\n","    # Get the confidence score\n","    logits = torch.stack(outputs.scores, dim=1)\n","    probs = F.softmax(logits, dim=-1)\n","\n","    token_ids = outputs.sequences[:, inputs.shape[1]:]\n","    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","    avg_confidence = confidences[0]\n","    return mistral7b_response, avg_confidence\n","\n","def get_gemini_judgement(response, prompt):\n","    \"\"\"Uses Gemini model to evaluate the response from Mistral-7B model.\"\"\"\n","    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.If the response is related with the prompt then provide correct otherwise provide incorrect\"\n","    judgement = gemini_model.generate_content(evaluation_prompt)\n","    return judgement\n","\n","def calculate_expected_calibration_error(mistral_confidences, gemini_judgements):\n","    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n","    num_bins = 10 \n","    bin_edges = np.linspace(0.0, 1.0, num_bins + 1)\n","    \n","    ece = 0.0\n","    for i in range(num_bins):\n","        bin_lower = bin_edges[i]\n","        bin_upper = bin_edges[i + 1]\n","        bin_indices = (mistral_confidences >= bin_lower) & (mistral_confidences < bin_upper)\n","        bin_confidences = mistral_confidences[bin_indices]\n","        bin_judgements = gemini_judgements[bin_indices]\n","\n","        if len(bin_confidences) == 0:\n","            continue  # Skip empty bins\n","\n","        accuracy_in_bin = bin_judgements.mean()\n","        avg_confidence_in_bin = bin_confidences.mean()\n","        ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * (len(bin_confidences) / len(mistral_confidences))\n","\n","    return ece\n","\n","# Example usage\n","prompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\n","mistral_response, mistral_confidence = get_mistral_answer(prompt)\n","mistral_preprocess_response = preprocess_text(mistral_response)\n","gemini_judgement = get_gemini_judgement(mistral_preprocess_response, prompt)\n","\n","# Convert Gemini judgement to a binary label\n","gemini_label = 1 if gemini_judgement == \"correct\" else 0\n","\n","# Collect multiple examples (prompt-response pairs) to get a good ECE calculation\n","mistral_confidences = [mistral_confidence]\n","gemini_judgements = [gemini_label]\n","\n","mistral_confidences = np.array(mistral_confidences)\n","gemini_judgements = np.array(gemini_judgements)\n","\n","ece = calculate_expected_calibration_error(mistral_confidences, gemini_judgements)\n","print(f\"Expected Calibration Error (ECE): {ece}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### StableBeluga13B "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:13:05.841749Z","iopub.status.busy":"2024-05-28T14:13:05.841391Z","iopub.status.idle":"2024-05-28T14:13:24.749724Z","shell.execute_reply":"2024-05-28T14:13:24.748710Z","shell.execute_reply.started":"2024-05-28T14:13:05.841721Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Expected Calibration Error (ECE): 0.7297741770744324\n"]}],"source":["def get_mistral_answer(prompt):\n","    \"\"\"Generates an answer from the Mistral-7B model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name2)\n","    inputs = tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True).to(mistral_model.device)\n","    outputs = mistral_model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.8, output_scores=True, return_dict_in_generate=True)\n","    mistral7b_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","\n","    # Get the confidence score\n","    logits = torch.stack(outputs.scores, dim=1)\n","    probs = F.softmax(logits, dim=-1)\n","\n","    token_ids = outputs.sequences[:, inputs.shape[1]:]\n","    confidences = probs.gather(2, token_ids.unsqueeze(-1)).squeeze(-1).mean(dim=1).detach().cpu().numpy()\n","\n","    avg_confidence = confidences[0]\n","    return mistral7b_response, avg_confidence\n","\n","def get_gemini_judgement(response, prompt):\n","    \"\"\"Uses Gemini model to evaluate the response from Mistral-7B model.\"\"\"\n","    evaluation_prompt = f\"Please evaluate the following answer to the prompt:\\n\\nPrompt: {prompt}\\n\\nAnswer: {response}\\n\\nProvide your evaluation in a concise and informative manner.\"\n","    judgement = gemini_model.generate_content(evaluation_prompt)\n","    return judgement\n","\n","def calculate_expected_calibration_error(mistral_confidences, gemini_judgements):\n","    \"\"\"Calculates the Expected Calibration Error (ECE).\"\"\"\n","    num_bins = 10 \n","    bin_edges = np.linspace(0.0, 1.0, num_bins + 1)\n","    \n","    ece = 0.0\n","    for i in range(num_bins):\n","        bin_lower = bin_edges[i]\n","        bin_upper = bin_edges[i + 1]\n","        bin_indices = (mistral_confidences >= bin_lower) & (mistral_confidences < bin_upper)\n","        bin_confidences = mistral_confidences[bin_indices]\n","        bin_judgements = gemini_judgements[bin_indices]\n","\n","        if len(bin_confidences) == 0:\n","            continue  # Skip empty bins\n","\n","        accuracy_in_bin = bin_judgements.mean()\n","        avg_confidence_in_bin = bin_confidences.mean()\n","        ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * (len(bin_confidences) / len(mistral_confidences))\n","\n","    return ece\n","\n","# Example usage\n","prompt = \"What is nuclear fusion? What are the advantages of it? What danger it make on environment?\"\n","mistral_response, mistral_confidence = get_mistral_answer(prompt)\n","mistral_preprocess_response = preprocess_text(mistral_response)\n","gemini_judgement = get_gemini_judgement(mistral_preprocess_response, prompt)\n","\n","# Convert Gemini judgement to a binary label\n","gemini_label = 1 if gemini_judgement == \"correct\" else 0\n","\n","mistral_confidences = [mistral_confidence]\n","gemini_judgements = [gemini_label]\n","\n","mistral_confidences = np.array(mistral_confidences)\n","gemini_judgements = np.array(gemini_judgements)\n","\n","ece = calculate_expected_calibration_error(mistral_confidences, gemini_judgements)\n","print(f\"Expected Calibration Error (ECE): {ece}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
